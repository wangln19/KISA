Epoch 2478: train_loss 0.022010563 val_loss 0.025472075
ttest: -28.64313405642393 pValue 3.247998661993646e-132
Epoch 2479: train_loss 0.022009607 val_loss 0.025472227
ttest: -28.617437233495973 pValue 4.868383432574427e-132
Epoch 2480: train_loss 0.022008652 val_loss 0.025472384
ttest: -28.591869527554778 pValue 7.282077042398295e-132
Epoch 2481: train_loss 0.0220077 val_loss 0.025472546
ttest: -28.566341505498137 pValue 1.088526172428666e-131
Epoch 2482: train_loss 0.022006743 val_loss 0.025472702
ttest: -28.540849393488482 pValue 1.6261527349847537e-131
Epoch 2483: train_loss 0.022005789 val_loss 0.025472863
ttest: -28.51545190903072 pValue 2.425611031302861e-131
Epoch 2484: train_loss 0.022004835 val_loss 0.025473014
ttest: -28.490074685628528 pValue 3.616818500650056e-131
Epoch 2485: train_loss 0.022003878 val_loss 0.025473181
ttest: -28.464807647721308 pValue 5.383481572893778e-131
Epoch 2486: train_loss 0.022002926 val_loss 0.025473336
ttest: -28.439578760544883 pValue 8.007982392078192e-131
Epoch 2487: train_loss 0.022001969 val_loss 0.025473488
ttest: -28.414409653948333 pValue 1.1900314704719583e-130
Epoch 2488: train_loss 0.022001015 val_loss 0.025473658
ttest: -28.389331827017152 pValue 1.7658506889982134e-130
Epoch 2489: train_loss 0.02200006 val_loss 0.025473809
ttest: -28.364291449510397 pValue 2.6186504077892258e-130
Epoch 2490: train_loss 0.021999108 val_loss 0.025473975
ttest: -28.339344412597555 pValue 3.877458637233792e-130
Epoch 2491: train_loss 0.021998154 val_loss 0.025474127
ttest: -28.314435245134796 pValue 5.737753573434391e-130
Epoch 2492: train_loss 0.021997202 val_loss 0.025474288
ttest: -28.289600982395847 pValue 8.480249847334322e-130
Epoch 2493: train_loss 0.021996247 val_loss 0.025474451
ttest: -28.264821388259538 pValue 1.2522344990630586e-129
Epoch 2494: train_loss 0.021995293 val_loss 0.02547461
ttest: -28.240096631885176 pValue 1.847445936717847e-129
Epoch 2495: train_loss 0.02199434 val_loss 0.025474772
ttest: -28.21544899620722 pValue 2.7221664882869774e-129
Epoch 2496: train_loss 0.021993386 val_loss 0.025474934
ttest: -28.190874689973622 pValue 4.006271702405273e-129
Epoch 2497: train_loss 0.021992432 val_loss 0.025475085
ttest: -28.16637682412223 pValue 5.888811348833776e-129
Epoch 2498: train_loss 0.021991476 val_loss 0.02547525
ttest: -28.14189721756613 pValue 8.653137727454243e-129
Epoch 2499: train_loss 0.021990526 val_loss 0.025475413
ttest: -28.117490190799785 pValue 1.270010856481335e-128
Epoch 2500: train_loss 0.02198957 val_loss 0.025475571
ttest: -28.09313782413059 pValue 1.862307564057185e-128
Epoch 2501: train_loss 0.021988617 val_loss 0.025475739
ttest: -28.06888248742875 pValue 2.726568187088508e-128
Epoch 2502: train_loss 0.021987662 val_loss 0.025475895
ttest: -28.044662599654117 pValue 3.989537781039075e-128
Epoch 2503: train_loss 0.021986712 val_loss 0.025476059
ttest: -28.02053609856664 pValue 5.828740067184818e-128
Epoch 2504: train_loss 0.02198576 val_loss 0.025476214
ttest: -27.99644819443252 pValue 8.510332828111452e-128
Epoch 2505: train_loss 0.021984806 val_loss 0.025476374
ttest: -27.97243436367502 pValue 1.241069429627351e-127
Epoch 2506: train_loss 0.021983854 val_loss 0.02547654
ttest: -27.94847782963357 pValue 1.808163709880177e-127
Epoch 2507: train_loss 0.021982903 val_loss 0.025476696
ttest: -27.924572855448393 pValue 2.6321503053787724e-127
Epoch 2508: train_loss 0.021981945 val_loss 0.025476864
ttest: -27.900744029250397 pValue 3.826900438111199e-127
Epoch 2509: train_loss 0.021980995 val_loss 0.02547703
ttest: -27.876971486354318 pValue 5.55882112433515e-127
Epoch 2510: train_loss 0.02198004 val_loss 0.025477193
ttest: -27.853256827477214 pValue 8.066893964992502e-127
Epoch 2511: train_loss 0.021979092 val_loss 0.02547736
ttest: -27.829634757575015 pValue 1.1689111552655641e-126
Epoch 2512: train_loss 0.02197814 val_loss 0.025477521
ttest: -27.806050661527205 pValue 1.6927021223448263e-126
Epoch 2513: train_loss 0.021977186 val_loss 0.025477683
ttest: -27.782540758901035 pValue 2.4482544558880897e-126
Epoch 2514: train_loss 0.021976234 val_loss 0.025477847
ttest: -27.759106805974955 pValue 3.536694863406608e-126
Epoch 2515: train_loss 0.021975284 val_loss 0.025478011
ttest: -27.735709514206505 pValue 5.10588988267126e-126
Epoch 2516: train_loss 0.021974329 val_loss 0.02547817
ttest: -27.712386904594364 pValue 7.362392883191354e-126
Epoch 2517: train_loss 0.021973379 val_loss 0.025478343
ttest: -27.68909941446437 pValue 1.0609863852050258e-125
Epoch 2518: train_loss 0.021972425 val_loss 0.0254785
ttest: -27.665886811453813 pValue 1.527119260304869e-125
Epoch 2519: train_loss 0.021971475 val_loss 0.025478663
ttest: -27.642773240958302 pValue 2.1945424879254014e-125
Epoch 2520: train_loss 0.021970522 val_loss 0.025478829
ttest: -27.619717436700142 pValue 3.1506785216336424e-125
Epoch 2521: train_loss 0.021969572 val_loss 0.025478994
ttest: -27.59665193117384 pValue 4.5238971719861726e-125
Epoch 2522: train_loss 0.021968622 val_loss 0.025479158
ttest: -27.573727834593385 pValue 6.480981275866393e-125
Epoch 2523: train_loss 0.021967668 val_loss 0.025479324
ttest: -27.55083749496409 pValue 9.279431818684644e-125
Epoch 2524: train_loss 0.021966718 val_loss 0.025479492
ttest: -27.528024571475743 pValue 1.3269584789697968e-124
Epoch 2525: train_loss 0.021965766 val_loss 0.025479658
ttest: -27.505248512091732 pValue 1.8963770908551045e-124
Epoch 2526: train_loss 0.021964813 val_loss 0.02547982
ttest: -27.48250589324638 pValue 2.708611678267736e-124
Epoch 2527: train_loss 0.021963863 val_loss 0.02547999
ttest: -27.459883419219658 pValue 3.861298338861695e-124
Epoch 2528: train_loss 0.021962913 val_loss 0.025480153
ttest: -27.437317516577682 pValue 5.49942535318155e-124
Epoch 2529: train_loss 0.021961963 val_loss 0.025480315
ttest: -27.414826322225103 pValue 7.823033999429351e-124
Epoch 2530: train_loss 0.021961013 val_loss 0.025480477
ttest: -27.392373568237876 pValue 1.112125719199151e-123
Epoch 2531: train_loss 0.021960063 val_loss 0.025480658
ttest: -27.369998077429823 pValue 1.579025552116069e-123
Epoch 2532: train_loss 0.02195911 val_loss 0.025480822
ttest: -27.347656684470756 pValue 2.240653701046581e-123
Epoch 2533: train_loss 0.02195816 val_loss 0.025480988
ttest: -27.325391745859598 pValue 3.175576125254662e-123
Epoch 2534: train_loss 0.02195721 val_loss 0.025481157
ttest: -27.303184987514197 pValue 4.496316286235688e-123
Epoch 2535: train_loss 0.021956258 val_loss 0.025481323
ttest: -27.28103466942516 pValue 6.360475532342332e-123
Epoch 2536: train_loss 0.02195531 val_loss 0.025481485
ttest: -27.258962663886336 pValue 8.986116279054941e-123
Epoch 2537: train_loss 0.021954358 val_loss 0.025481647
ttest: -27.236948819947525 pValue 1.2683562596228697e-122
Epoch 2538: train_loss 0.021953408 val_loss 0.025481828
ttest: -27.215012554518168 pValue 1.7879908652417803e-122
Epoch 2539: train_loss 0.021952458 val_loss 0.02548199
ttest: -27.193111408094644 pValue 2.5190269344314876e-122
Epoch 2540: train_loss 0.021951512 val_loss 0.025482161
ttest: -27.17129129671493 pValue 3.544310477337475e-122
Epoch 2541: train_loss 0.021950562 val_loss 0.02548232
ttest: -27.149524751481877 pValue 4.982517871869789e-122
Epoch 2542: train_loss 0.02194961 val_loss 0.025482494
ttest: -27.127841101319827 pValue 6.99495462981174e-122
Epoch 2543: train_loss 0.02194866 val_loss 0.025482658
ttest: -27.10623602875778 pValue 9.807747795131933e-122
Epoch 2544: train_loss 0.021947714 val_loss 0.025482832
ttest: -27.08459788761946 pValue 1.3758162095070593e-121
Epoch 2545: train_loss 0.021946762 val_loss 0.025482997
ttest: -27.063108250210313 pValue 1.925417952858329e-121
Epoch 2546: train_loss 0.02194581 val_loss 0.025483165
ttest: -27.04165269749259 pValue 2.693024382684477e-121
Epoch 2547: train_loss 0.021944864 val_loss 0.025483336
ttest: -27.020280002721655 pValue 3.7616211462015224e-121
Epoch 2548: train_loss 0.021943912 val_loss 0.025483502
ttest: -26.99896100536671 pValue 5.249614238026104e-121
Epoch 2549: train_loss 0.021942964 val_loss 0.025483673
ttest: -26.977680916298905 pValue 7.321460454731756e-121
Epoch 2550: train_loss 0.021942014 val_loss 0.025483841
ttest: -26.956479758502226 pValue 1.0197985478259322e-120
Epoch 2551: train_loss 0.021941068 val_loss 0.025484005
ttest: -26.935338252276523 pValue 1.4190847797331952e-120
Epoch 2552: train_loss 0.021940118 val_loss 0.025484188
ttest: -26.914277477573407 pValue 1.9721347149234156e-120
Epoch 2553: train_loss 0.021939166 val_loss 0.025484353
ttest: -26.89327425624477 pValue 2.7381450982634394e-120
Epoch 2554: train_loss 0.021938218 val_loss 0.025484517
ttest: -26.8723079528445 pValue 3.799338791603066e-120
Epoch 2555: train_loss 0.02193727 val_loss 0.025484696
ttest: -26.85142391699821 pValue 5.264821532109316e-120
Epoch 2556: train_loss 0.021936322 val_loss 0.02548487
ttest: -26.83059776299189 pValue 7.288679295672714e-120
Epoch 2557: train_loss 0.021935374 val_loss 0.025485035
ttest: -26.809831195613157 pValue 1.0080732295067307e-119
Epoch 2558: train_loss 0.02193443 val_loss 0.025485208
ttest: -26.789123014752903 pValue 1.39290520934068e-119
Epoch 2559: train_loss 0.021933476 val_loss 0.025485374
ttest: -26.768499601090003 pValue 1.922022611273798e-119
Epoch 2560: train_loss 0.02193253 val_loss 0.025485545
ttest: -26.747958011869553 pValue 2.6486400382486354e-119
Epoch 2561: train_loss 0.021931581 val_loss 0.025485715
ttest: -26.727406822854377 pValue 3.650351534824781e-119
Epoch 2562: train_loss 0.021930633 val_loss 0.025485888
ttest: -26.70698660987962 pValue 5.020429905513565e-119
Epoch 2563: train_loss 0.021929683 val_loss 0.025486058
ttest: -26.686603829584083 pValue 6.900423291176554e-119
Epoch 2564: train_loss 0.021928737 val_loss 0.025486227
ttest: -26.6662830815988 pValue 9.474853293181574e-119
Epoch 2565: train_loss 0.021927789 val_loss 0.025486393
ttest: -26.645971685335297 pValue 1.3007329242531587e-118
Epoch 2566: train_loss 0.021926843 val_loss 0.025486572
ttest: -26.625769920886974 pValue 1.782556584271058e-118
Epoch 2567: train_loss 0.021925895 val_loss 0.025486737
ttest: -26.60568006849986 pValue 2.438500349865844e-118
Epoch 2568: train_loss 0.021924945 val_loss 0.025486914
ttest: -26.585602597613796 pValue 3.335038351632975e-118
Epoch 2569: train_loss 0.021923997 val_loss 0.025487088
ttest: -26.565584649215154 pValue 4.556780399055922e-118
Epoch 2570: train_loss 0.02192305 val_loss 0.02548725
ttest: -26.545652566353812 pValue 6.217508085515871e-118
Epoch 2571: train_loss 0.021922106 val_loss 0.025487427
ttest: -26.52578158481914 pValue 8.475071324788218e-118
Epoch 2572: train_loss 0.021921158 val_loss 0.025487602
ttest: -26.505925021325762 pValue 1.1549286972956905e-117
Epoch 2573: train_loss 0.021920208 val_loss 0.025487762
ttest: -26.486202007661543 pValue 1.5705273272074528e-117
Epoch 2574: train_loss 0.021919264 val_loss 0.025487944
ttest: -26.46651855203116 pValue 2.1342761567553354e-117
Epoch 2575: train_loss 0.02191832 val_loss 0.025488107
ttest: -26.446896917438096 pValue 2.8974764352584066e-117
Epoch 2576: train_loss 0.02191737 val_loss 0.025488283
ttest: -26.42728648149339 pValue 3.932746262990159e-117
Epoch 2577: train_loss 0.021916425 val_loss 0.025488455
ttest: -26.407792607909723 pValue 5.3280207072537804e-117
Epoch 2578: train_loss 0.021915475 val_loss 0.025488634
ttest: -26.388336306486238 pValue 7.213803411917043e-117
Epoch 2579: train_loss 0.021914529 val_loss 0.025488798
ttest: -26.368942277603914 pValue 9.757175167658666e-117
Epoch 2580: train_loss 0.021913582 val_loss 0.025488973
ttest: -26.34961121109216 pValue 1.318380379449254e-116
Epoch 2581: train_loss 0.021912634 val_loss 0.025489148
ttest: -26.330398305235338 pValue 1.7780381892114778e-116
Epoch 2582: train_loss 0.021911694 val_loss 0.025489325
ttest: -26.311172063939186 pValue 2.398359483994299e-116
Epoch 2583: train_loss 0.021910746 val_loss 0.025489496
ttest: -26.29206222361579 pValue 3.229113668200032e-116
Epoch 2584: train_loss 0.0219098 val_loss 0.025489666
ttest: -26.272987936506986 pValue 4.345050580664872e-116
Epoch 2585: train_loss 0.021908853 val_loss 0.02548985
ttest: -26.254033134554565 pValue 5.835548169142855e-116
Epoch 2586: train_loss 0.021907907 val_loss 0.025490023
ttest: -26.23506784361612 pValue 7.838305352781168e-116
Epoch 2587: train_loss 0.02190696 val_loss 0.025490195
ttest: -26.216191741505146 pValue 1.051339371564594e-115
Epoch 2588: train_loss 0.021906016 val_loss 0.02549037
ttest: -26.19737954770871 pValue 1.4086880243013873e-115
Epoch 2589: train_loss 0.021905072 val_loss 0.025490548
ttest: -26.178609174757334 pValue 1.8861973294283611e-115
Epoch 2590: train_loss 0.021904122 val_loss 0.025490716
ttest: -26.159959084576244 pValue 2.5207515689160127e-115
Epoch 2591: train_loss 0.021903178 val_loss 0.025490891
ttest: -26.141320958876722 pValue 3.368023901474923e-115
Epoch 2592: train_loss 0.021902235 val_loss 0.025491064
ttest: -26.122777084952705 pValue 4.493315723535726e-115
Epoch 2593: train_loss 0.021901289 val_loss 0.025491243
ttest: -26.10424837317405 pValue 5.992932346306777e-115
Epoch 2594: train_loss 0.021900343 val_loss 0.025491415
ttest: -26.085814098303082 pValue 7.981002121890834e-115
Epoch 2595: train_loss 0.021899402 val_loss 0.025491593
ttest: -26.067474005454496 pValue 1.0612628690662066e-114
Epoch 2596: train_loss 0.021898454 val_loss 0.025491765
ttest: -26.04917633626256 pValue 1.4102153461284285e-114
Epoch 2597: train_loss 0.021897512 val_loss 0.025491942
ttest: -26.03092149437894 pValue 1.8725875545762897e-114
Epoch 2598: train_loss 0.021896563 val_loss 0.02549212
ttest: -26.012788928860186 pValue 2.48174499299644e-114
Epoch 2599: train_loss 0.021895621 val_loss 0.025492294
ttest: -25.99464220345188 pValue 3.289660001942891e-114
Epoch 2600: train_loss 0.021894677 val_loss 0.025492474
ttest: -25.976594437687194 pValue 4.353722248032684e-114
Epoch 2601: train_loss 0.02189373 val_loss 0.025492644
ttest: -25.958614025751906 pValue 5.75571898886394e-114
Epoch 2602: train_loss 0.021892786 val_loss 0.025492815
ttest: -25.94067788421474 pValue 7.603671485641308e-114
Epoch 2603: train_loss 0.021891844 val_loss 0.025492998
ttest: -25.922836593814132 pValue 1.002977244846343e-113
Epoch 2604: train_loss 0.021890895 val_loss 0.025493173
ttest: -25.90501160937293 pValue 1.3226118123530492e-113
Epoch 2605: train_loss 0.021889955 val_loss 0.025493346
ttest: -25.88728671421461 pValue 1.7413361801601722e-113
Epoch 2606: train_loss 0.021889007 val_loss 0.02549353
ttest: -25.86957755323888 pValue 2.2919775993717523e-113
Epoch 2607: train_loss 0.021888068 val_loss 0.025493706
ttest: -25.852023408976954 pValue 3.0093810897535515e-113
Epoch 2608: train_loss 0.021887122 val_loss 0.025493875
ttest: -25.834432382382438 pValue 3.9534496161941233e-113
Epoch 2609: train_loss 0.02188618 val_loss 0.025494054
ttest: -25.816968467604827 pValue 5.183255450522231e-113
Epoch 2610: train_loss 0.021885237 val_loss 0.02549423
ttest: -25.79955036477321 pValue 6.790538710977184e-113
Epoch 2611: train_loss 0.02188429 val_loss 0.0254944
ttest: -25.78220698352814 pValue 8.885593287361094e-113
Epoch 2612: train_loss 0.021883348 val_loss 0.025494583
ttest: -25.76493485986515 pValue 1.1613755519593724e-112
Epoch 2613: train_loss 0.021882406 val_loss 0.025494758
ttest: -25.74768127254726 pValue 1.5174627607760094e-112
Epoch 2614: train_loss 0.021881461 val_loss 0.025494935
ttest: -25.730558936629258 pValue 1.9786262627699211e-112
Epoch 2615: train_loss 0.02188052 val_loss 0.025495108
ttest: -25.71342566641117 pValue 2.5802817485441934e-112
Epoch 2616: train_loss 0.021879576 val_loss 0.025495285
ttest: -25.69642660867009 pValue 3.357772409323322e-112
Epoch 2617: train_loss 0.021878634 val_loss 0.025495458
ttest: -25.67941708675247 pValue 4.370085373962164e-112
Epoch 2618: train_loss 0.02187769 val_loss 0.025495632
ttest: -25.662568346689234 pValue 5.673237120967385e-112
Epoch 2619: train_loss 0.021876747 val_loss 0.025495814
ttest: -25.645711238326356 pValue 7.365674512033796e-112
Epoch 2620: train_loss 0.021875806 val_loss 0.025495999
ttest: -25.628900145269885 pValue 9.55583820907421e-112
Epoch 2621: train_loss 0.021874864 val_loss 0.025496168
ttest: -25.612196158821643 pValue 1.2376247337904588e-111
Epoch 2622: train_loss 0.021873921 val_loss 0.02549635
ttest: -25.595569020481527 pValue 1.6009463381102898e-111
Epoch 2623: train_loss 0.021872979 val_loss 0.025496518
ttest: -25.578961081224048 pValue 2.070236255937315e-111
Epoch 2624: train_loss 0.021872034 val_loss 0.025496705
ttest: -25.56246384475544 pValue 2.6724110459047162e-111
Epoch 2625: train_loss 0.021871094 val_loss 0.025496876
ttest: -25.545984949961998 pValue 3.4486400113968185e-111
Epoch 2626: train_loss 0.02187015 val_loss 0.025497053
ttest: -25.52958557606723 pValue 4.444700804997387e-111
Epoch 2627: train_loss 0.021869209 val_loss 0.025497235
ttest: -25.513299056178592 pValue 5.718253980216889e-111
Epoch 2628: train_loss 0.021868266 val_loss 0.025497418
ttest: -25.497001425094954 pValue 7.35772808485811e-111
Epoch 2629: train_loss 0.021867327 val_loss 0.025497593
ttest: -25.480785617136892 pValue 9.454944790796404e-111
Epoch 2630: train_loss 0.021866385 val_loss 0.025497764
ttest: -25.464652309648226 pValue 1.2134023943067586e-110
Epoch 2631: train_loss 0.02186544 val_loss 0.025497941
ttest: -25.448631439400145 pValue 1.5544630026600544e-110
Epoch 2632: train_loss 0.021864502 val_loss 0.025498118
ttest: -25.432571578190412 pValue 1.9925201983130414e-110
Epoch 2633: train_loss 0.02186356 val_loss 0.025498305
ttest: -25.416657684227815 pValue 2.548178382242449e-110
Epoch 2634: train_loss 0.021862619 val_loss 0.025498474
ttest: -25.40076390018983 pValue 3.257669218502701e-110
Epoch 2635: train_loss 0.021861676 val_loss 0.025498655
ttest: -25.384953596789128 pValue 4.1591903174801646e-110
Epoch 2636: train_loss 0.021860735 val_loss 0.025498832
ttest: -25.369231534201486 pValue 5.302778271115212e-110
Epoch 2637: train_loss 0.021859797 val_loss 0.025499027
ttest: -25.353497045675134 pValue 6.761868819009532e-110
Epoch 2638: train_loss 0.021858856 val_loss 0.025499197
ttest: -25.33791318336751 pValue 8.602099529686288e-110
Epoch 2639: train_loss 0.021857915 val_loss 0.025499372
ttest: -25.32232050763174 pValue 1.0944267070193053e-109
Epoch 2640: train_loss 0.021856973 val_loss 0.02549956
ttest: -25.30681359279579 pValue 1.3905252077155057e-109
Epoch 2641: train_loss 0.021856034 val_loss 0.025499735
ttest: -25.291427754617633 pValue 1.7633736355839478e-109
Epoch 2642: train_loss 0.021855092 val_loss 0.025499916
ttest: -25.275999473256103 pValue 2.2375873456176606e-109
Epoch 2643: train_loss 0.021854151 val_loss 0.02550009
ttest: -25.260757816749013 pValue 2.8310636240759246e-109
Epoch 2644: train_loss 0.02185321 val_loss 0.025500271
ttest: -25.245476331023045 pValue 3.584032997138316e-109
Epoch 2645: train_loss 0.021852273 val_loss 0.025500447
ttest: -25.230382502434274 pValue 4.523992958050485e-109
Epoch 2646: train_loss 0.021851333 val_loss 0.025500624
ttest: -25.215247105207634 pValue 5.713948404013947e-109
Epoch 2647: train_loss 0.021850392 val_loss 0.025500802
ttest: -25.200205221520225 pValue 7.206256146924839e-109
Epoch 2648: train_loss 0.021849452 val_loss 0.025500977
ttest: -25.185253984346037 pValue 9.0753100017551e-109
Epoch 2649: train_loss 0.021848515 val_loss 0.025501166
ttest: -25.17029726736835 pValue 1.1429727588856973e-108
Epoch 2650: train_loss 0.021847572 val_loss 0.025501342
ttest: -25.155465025586118 pValue 1.4366872808716896e-108
Epoch 2651: train_loss 0.021846633 val_loss 0.025501527
ttest: -25.140693278780645 pValue 1.804136591265122e-108
Epoch 2652: train_loss 0.021845695 val_loss 0.02550171
ttest: -25.125984157249846 pValue 2.263305698866096e-108
Epoch 2653: train_loss 0.021844754 val_loss 0.025501886
ttest: -25.111266802956532 pValue 2.8396074529546773e-108
Epoch 2654: train_loss 0.021843819 val_loss 0.025502061
ttest: -25.096747506070272 pValue 3.551675588267196e-108
Epoch 2655: train_loss 0.021842876 val_loss 0.025502246
ttest: -25.082187423516537 pValue 4.444958588252172e-108
Epoch 2656: train_loss 0.021841936 val_loss 0.025502425
ttest: -25.06772494182293 pValue 5.554372866641021e-108
Epoch 2657: train_loss 0.021840999 val_loss 0.025502604
ttest: -25.05332457219105 pValue 6.933826797610278e-108
Epoch 2658: train_loss 0.021840062 val_loss 0.025502784
ttest: -25.038989467184024 pValue 8.646902971445492e-108
Epoch 2659: train_loss 0.021839123 val_loss 0.025502965
ttest: -25.024717968969345 pValue 1.0772314667984468e-107
Epoch 2660: train_loss 0.02183818 val_loss 0.025503147
ttest: -25.010509343895986 pValue 1.3406745285207676e-107
Epoch 2661: train_loss 0.021837244 val_loss 0.025503322
ttest: -24.99636779490737 pValue 1.6667695623806768e-107
Epoch 2662: train_loss 0.021836307 val_loss 0.025503505
ttest: -24.98229061859094 pValue 2.070064327325699e-107
Epoch 2663: train_loss 0.02183537 val_loss 0.025503684
ttest: -24.96828209074741 pValue 2.5681460421644737e-107
Epoch 2664: train_loss 0.02183443 val_loss 0.025503863
ttest: -24.954374779774103 pValue 3.181013592302509e-107
Epoch 2665: train_loss 0.02183349 val_loss 0.025504047
ttest: -24.940465065245647 pValue 3.9401642974080856e-107
Epoch 2666: train_loss 0.021832554 val_loss 0.02550423
ttest: -24.926623539114843 pValue 4.8752189929663514e-107
Epoch 2667: train_loss 0.021831617 val_loss 0.025504407
ttest: -24.912887272021045 pValue 6.022228382132949e-107
Epoch 2668: train_loss 0.021830678 val_loss 0.025504587
ttest: -24.899149288642448 pValue 7.439073856854344e-107
Epoch 2669: train_loss 0.02182974 val_loss 0.025504766
ttest: -24.885587138811175 pValue 9.164154385645106e-107
Epoch 2670: train_loss 0.021828802 val_loss 0.025504952
ttest: -24.871989414173317 pValue 1.1295121982930025e-106
Epoch 2671: train_loss 0.021827867 val_loss 0.025505133
ttest: -24.858536899454027 pValue 1.389013233740546e-106
Epoch 2672: train_loss 0.02182693 val_loss 0.025505314
ttest: -24.845119087993663 pValue 1.7071723515888606e-106
Epoch 2673: train_loss 0.021825992 val_loss 0.025505485
ttest: -24.83173685832976 pValue 2.0969981709943515e-106
Epoch 2674: train_loss 0.021825055 val_loss 0.025505671
ttest: -24.818355364331104 pValue 2.5757350975832708e-106
Epoch 2675: train_loss 0.021824116 val_loss 0.02550585
ttest: -24.805194875771775 pValue 3.1529402987316885e-106
Epoch 2676: train_loss 0.02182318 val_loss 0.025506034
ttest: -24.791997439080024 pValue 3.861576764057803e-106
Epoch 2677: train_loss 0.021822244 val_loss 0.025506215
ttest: -24.778839927230788 pValue 4.726444865314239e-106
Epoch 2678: train_loss 0.021821309 val_loss 0.025506401
ttest: -24.765867578819567 pValue 5.768407980922348e-106
Epoch 2679: train_loss 0.021820372 val_loss 0.025506582
ttest: -24.752861926447657 pValue 7.043483006051408e-106
Epoch 2680: train_loss 0.021819433 val_loss 0.025506757
ttest: -24.739970796001938 pValue 8.585040838623974e-106
Epoch 2681: train_loss 0.0218185 val_loss 0.025506942
ttest: -24.727081022522974 pValue 1.0463478009775994e-105
Epoch 2682: train_loss 0.021817563 val_loss 0.025507119
ttest: -24.7143480189564 pValue 1.2721887352029072e-105
Epoch 2683: train_loss 0.021816624 val_loss 0.025507303
ttest: -24.701540870429657 pValue 1.54849474523536e-105
Epoch 2684: train_loss 0.021815691 val_loss 0.025507484
ttest: -24.68896811274843 pValue 1.8779852812945343e-105
Epoch 2685: train_loss 0.021814754 val_loss 0.025507668
ttest: -24.67628422668336 pValue 2.281413834945044e-105
Epoch 2686: train_loss 0.021813821 val_loss 0.025507845
ttest: -24.663797580510348 pValue 2.76304827622136e-105
Epoch 2687: train_loss 0.021812884 val_loss 0.025508028
ttest: -24.651355268681026 pValue 3.343993815800495e-105
Epoch 2688: train_loss 0.021811947 val_loss 0.025508208
ttest: -24.6389571165033 pValue 4.0442341530826876e-105
Epoch 2689: train_loss 0.021811014 val_loss 0.025508396
ttest: -24.626522986520307 pValue 4.893676624080266e-105
Epoch 2690: train_loss 0.021810079 val_loss 0.025508571
ttest: -24.614292479946805 pValue 5.902894955985847e-105
Epoch 2691: train_loss 0.021809144 val_loss 0.02550875
ttest: -24.60207033648353 pValue 7.119140678826472e-105
Epoch 2692: train_loss 0.02180821 val_loss 0.025508935
ttest: -24.589931778360242 pValue 8.574749895496114e-105
Epoch 2693: train_loss 0.021807272 val_loss 0.025509115
ttest: -24.577882252504494 pValue 1.0313606879668826e-104
Epoch 2694: train_loss 0.021806339 val_loss 0.025509296
ttest: -24.565839089719052 pValue 1.2403547175732345e-104
Epoch 2695: train_loss 0.021805402 val_loss 0.025509484
ttest: -24.553924066892336 pValue 1.4887304840897755e-104
Epoch 2696: train_loss 0.02180447 val_loss 0.025509667
ttest: -24.541939087830286 pValue 1.788714421616206e-104
Epoch 2697: train_loss 0.021803536 val_loss 0.025509845
ttest: -24.530124372462502 pValue 2.143484955587842e-104
Epoch 2698: train_loss 0.0218026 val_loss 0.025510028
ttest: -24.518400789618376 pValue 2.5649665861232387e-104
Epoch 2699: train_loss 0.021801667 val_loss 0.025510203
ttest: -24.506687789266167 pValue 3.068749345815829e-104
Epoch 2700: train_loss 0.021800732 val_loss 0.025510387
ttest: -24.495028714172218 pValue 3.668351141384839e-104
Epoch 2701: train_loss 0.021799797 val_loss 0.025510572
ttest: -24.483502350843956 pValue 4.37608480727818e-104
Epoch 2702: train_loss 0.021798864 val_loss 0.025510754
ttest: -24.471949734061045 pValue 5.222330671868718e-104
Epoch 2703: train_loss 0.021797933 val_loss 0.025510928
ttest: -24.460489735120376 pValue 6.223225101924484e-104
Epoch 2704: train_loss 0.021796996 val_loss 0.025511106
ttest: -24.44912836250766 pValue 7.404560710839783e-104
Epoch 2705: train_loss 0.021796064 val_loss 0.025511296
ttest: -24.43773923764564 pValue 8.813672365838992e-104
Epoch 2706: train_loss 0.021795131 val_loss 0.02551148
ttest: -24.426489555051656 pValue 1.0468289734344971e-103
Epoch 2707: train_loss 0.021794196 val_loss 0.02551166
ttest: -24.41533909665589 pValue 1.2414340323108793e-103
Epoch 2708: train_loss 0.021793265 val_loss 0.025511842
ttest: -24.40420255603026 pValue 1.4718660311104932e-103
Epoch 2709: train_loss 0.021792334 val_loss 0.025512021
ttest: -24.393127441329572 pValue 1.743386332264289e-103
Epoch 2710: train_loss 0.021791399 val_loss 0.025512205
ttest: -24.382067440289443 pValue 2.064466306617493e-103
Epoch 2711: train_loss 0.021790465 val_loss 0.025512388
ttest: -24.371152965178787 pValue 2.439177972850248e-103
Epoch 2712: train_loss 0.021789532 val_loss 0.02551257
ttest: -24.36029926730115 pValue 2.8791514921501865e-103
Epoch 2713: train_loss 0.021788599 val_loss 0.02551275
ttest: -24.349417722639878 pValue 3.3998535572554355e-103
Epoch 2714: train_loss 0.02178767 val_loss 0.025512934
ttest: -24.338688672507864 pValue 4.005267488010372e-103
Epoch 2715: train_loss 0.021786736 val_loss 0.025513124
ttest: -24.32806668603371 pValue 4.710648786786791e-103
Epoch 2716: train_loss 0.021785801 val_loss 0.025513297
ttest: -24.317374338768747 pValue 5.546095035278784e-103
Epoch 2717: train_loss 0.021784868 val_loss 0.025513487
ttest: -24.306749284649 pValue 6.522832078223999e-103
Epoch 2718: train_loss 0.021783939 val_loss 0.025513666
ttest: -24.296276517134366 pValue 7.653546041735061e-103
Epoch 2719: train_loss 0.02178301 val_loss 0.025513856
ttest: -24.285828283170797 pValue 8.976683749439911e-103
Epoch 2720: train_loss 0.021782074 val_loss 0.025514035
ttest: -24.27539988156371 pValue 1.0525121813596965e-102
Epoch 2721: train_loss 0.021781143 val_loss 0.025514219
ttest: -24.26504187173913 pValue 1.2327086085208713e-102
Epoch 2722: train_loss 0.021780215 val_loss 0.025514403
ttest: -24.254885719901626 pValue 1.4392712975805508e-102
Epoch 2723: train_loss 0.021779284 val_loss 0.025514586
ttest: -24.244619802775752 pValue 1.6832317827624934e-102
Epoch 2724: train_loss 0.021778349 val_loss 0.025514768
ttest: -24.2345144831314 pValue 1.963669657952898e-102
Epoch 2725: train_loss 0.02177742 val_loss 0.025514951
ttest: -24.22443689651749 pValue 2.289806253921231e-102
Epoch 2726: train_loss 0.021776486 val_loss 0.025515134
ttest: -24.21443219534581 pValue 2.6670738514619265e-102
Epoch 2727: train_loss 0.021775559 val_loss 0.025515325
ttest: -24.20435894505144 pValue 3.1096849605446963e-102
Epoch 2728: train_loss 0.021774627 val_loss 0.025515504
ttest: -24.194499333872997 pValue 3.61384395612309e-102
Epoch 2729: train_loss 0.021773696 val_loss 0.02551568
ttest: -24.1847205539695 pValue 4.194458933949371e-102
Epoch 2730: train_loss 0.021772763 val_loss 0.025515873
ttest: -24.174920549671135 pValue 4.86982616724153e-102
Epoch 2731: train_loss 0.021771835 val_loss 0.025516048
ttest: -24.16515291028068 pValue 5.651012149638189e-102
Epoch 2732: train_loss 0.021770906 val_loss 0.02551623
ttest: -24.155558300627852 pValue 6.5400472232154354e-102
Epoch 2733: train_loss 0.021769974 val_loss 0.025516419
ttest: -24.145801538742443 pValue 7.587550310438278e-102
Epoch 2734: train_loss 0.021769045 val_loss 0.025516594
ttest: -24.136415868535497 pValue 8.752866143789385e-102
Epoch 2735: train_loss 0.021768117 val_loss 0.02551679
ttest: -24.12686820568944 pValue 1.012194345721874e-101
Epoch 2736: train_loss 0.021767186 val_loss 0.025516964
ttest: -24.11750133682219 pValue 1.167262189173029e-101
Epoch 2737: train_loss 0.021766257 val_loss 0.02551715
ttest: -24.108020925724094 pValue 1.3483921824058462e-101
Epoch 2738: train_loss 0.021765325 val_loss 0.025517331
ttest: -24.098772888955477 pValue 1.552076231838705e-101
Epoch 2739: train_loss 0.021764398 val_loss 0.02551752
ttest: -24.08951233547642 pValue 1.7868296011852326e-101
Epoch 2740: train_loss 0.021763468 val_loss 0.025517702
ttest: -24.080335801125795 pValue 2.0544063990496718e-101
Epoch 2741: train_loss 0.02176254 val_loss 0.025517885
ttest: -24.071248345983673 pValue 2.3587898279312676e-101
Epoch 2742: train_loss 0.021761611 val_loss 0.02551806
ttest: -24.062149480206816 pValue 2.7086825195563636e-101
Epoch 2743: train_loss 0.021760682 val_loss 0.025518239
ttest: -24.05308956182661 pValue 3.108558438760057e-101
Epoch 2744: train_loss 0.02175975 val_loss 0.025518429
ttest: -24.044222682511403 pValue 3.5568922136560396e-101
Epoch 2745: train_loss 0.021758825 val_loss 0.025518613
ttest: -24.035294025618906 pValue 4.073635587098427e-101
Epoch 2746: train_loss 0.021757897 val_loss 0.025518803
ttest: -24.026409468795727 pValue 4.6622077178418364e-101
Epoch 2747: train_loss 0.021756968 val_loss 0.025518978
ttest: -24.01761709001939 pValue 5.328200244621599e-101
Epoch 2748: train_loss 0.021756038 val_loss 0.025519164
ttest: -24.008922414616805 pValue 6.080123279342688e-101
Epoch 2749: train_loss 0.021755112 val_loss 0.025519341
ttest: -24.000270502884014 pValue 6.933480203248454e-101
Epoch 2750: train_loss 0.021754183 val_loss 0.025519522
ttest: -23.991613862535534 pValue 7.907001317936847e-101
Epoch 2751: train_loss 0.021753255 val_loss 0.025519706
ttest: -23.98305445943484 pValue 9.003647847203662e-101
Epoch 2752: train_loss 0.02175233 val_loss 0.025519896
ttest: -23.974543014032726 pValue 1.0244666312092869e-100
Epoch 2753: train_loss 0.0217514 val_loss 0.025520075
ttest: -23.966133445258613 pValue 1.1638385329997807e-100
Epoch 2754: train_loss 0.021750472 val_loss 0.025520246
ttest: -23.957771639724317 pValue 1.3211791814565708e-100
Epoch 2755: train_loss 0.021749543 val_loss 0.025520436
ttest: -23.94940912207552 pValue 1.499773558629709e-100
Epoch 2756: train_loss 0.02174862 val_loss 0.02552062
ttest: -23.941150484305115 pValue 1.6997776459716847e-100
Epoch 2757: train_loss 0.021747692 val_loss 0.0255208
ttest: -23.932891165935214 pValue 1.9264303936786738e-100
Epoch 2758: train_loss 0.021746762 val_loss 0.025520988
ttest: -23.92462773929006 pValue 2.1833929287428318e-100
Epoch 2759: train_loss 0.021745836 val_loss 0.025521167
ttest: -23.916584067197636 pValue 2.4663007038991674e-100
Epoch 2760: train_loss 0.021744909 val_loss 0.025521347
ttest: -23.908485907334075 pValue 2.7881175285260083e-100
Epoch 2761: train_loss 0.02174398 val_loss 0.02552153
ttest: -23.900443225364068 pValue 3.1491918945935255e-100
Epoch 2762: train_loss 0.021743057 val_loss 0.025521716
ttest: -23.892515024947492 pValue 3.550746794718325e-100
Epoch 2763: train_loss 0.021742132 val_loss 0.025521895
ttest: -23.88458775750876 pValue 4.003356860105707e-100
Epoch 2764: train_loss 0.021741206 val_loss 0.02552208
ttest: -23.87677764183408 pValue 4.505509130371354e-100
Epoch 2765: train_loss 0.021740278 val_loss 0.025522275
ttest: -23.86897121638545 pValue 5.070246958065594e-100
Epoch 2766: train_loss 0.021739352 val_loss 0.025522446
ttest: -23.861167496318444 pValue 5.705406210870363e-100
Epoch 2767: train_loss 0.021738427 val_loss 0.025522634
ttest: -23.853597415785412 pValue 6.397179182056156e-100
Epoch 2768: train_loss 0.021737501 val_loss 0.025522811
ttest: -23.845807389801184 pValue 7.19673305200574e-100
Epoch 2769: train_loss 0.021736575 val_loss 0.025522994
ttest: -23.838195129647872 pValue 8.074142545024974e-100
Epoch 2770: train_loss 0.021735651 val_loss 0.025523184
ttest: -23.830589841490564 pValue 9.05735136280186e-100
Epoch 2771: train_loss 0.021734726 val_loss 0.025523365
ttest: -23.823111016089637 pValue 1.0140505801722091e-99
Epoch 2772: train_loss 0.021733802 val_loss 0.025523547
ttest: -23.81564255728589 pValue 1.1351136760307728e-99
Epoch 2773: train_loss 0.021732872 val_loss 0.025523726
ttest: -23.808243789620267 pValue 1.2692528336906434e-99
Epoch 2774: train_loss 0.02173195 val_loss 0.025523916
ttest: -23.800975258284755 pValue 1.4163991675817723e-99
Epoch 2775: train_loss 0.021731025 val_loss 0.025524097
ttest: -23.793601662350003 pValue 1.5830971170221513e-99
Epoch 2776: train_loss 0.0217301 val_loss 0.025524285
ttest: -23.78635931677974 pValue 1.765840473080907e-99
Epoch 2777: train_loss 0.021729177 val_loss 0.02552446
ttest: -23.779074774594232 pValue 1.970897146607168e-99
Epoch 2778: train_loss 0.021728253 val_loss 0.025524652
ttest: -23.77192537237044 pValue 2.195193295636165e-99
Epoch 2779: train_loss 0.021727333 val_loss 0.025524829
ttest: -23.76485232897326 pValue 2.442117308903724e-99
Epoch 2780: train_loss 0.021726407 val_loss 0.025525011
ttest: -23.75779850630445 pValue 2.715956338395509e-99
Epoch 2781: train_loss 0.02172548 val_loss 0.025525192
ttest: -23.750700532136335 pValue 3.0224577537392057e-99
Epoch 2782: train_loss 0.021724558 val_loss 0.02552538
ttest: -23.7437445748395 pValue 3.3562060821821295e-99
Epoch 2783: train_loss 0.021723634 val_loss 0.025525568
ttest: -23.736749124345177 pValue 3.728955642666855e-99
Epoch 2784: train_loss 0.021722712 val_loss 0.025525747
ttest: -23.729897994731928 pValue 4.1339154696235265e-99
Epoch 2785: train_loss 0.021721786 val_loss 0.025525926
ttest: -23.72294525189264 pValue 4.589828212711115e-99
Epoch 2786: train_loss 0.021720864 val_loss 0.025526108
ttest: -23.716202258417 pValue 5.0796620064578006e-99
Epoch 2787: train_loss 0.021719944 val_loss 0.025526285
ttest: -23.709485606327153 pValue 5.619379102648035e-99
Epoch 2788: train_loss 0.021719016 val_loss 0.025526471
ttest: -23.702728947576208 pValue 6.2200669752829084e-99
Epoch 2789: train_loss 0.021718098 val_loss 0.025526652
ttest: -23.69599947919973 pValue 6.8819431953164025e-99
Epoch 2790: train_loss 0.021717174 val_loss 0.02552684
ttest: -23.689427743329308 pValue 7.59581247340449e-99
Epoch 2791: train_loss 0.021716252 val_loss 0.025527013
ttest: -23.682754737772225 pValue 8.396434953639929e-99
Epoch 2792: train_loss 0.021715328 val_loss 0.0255272
ttest: -23.676239401696844 pValue 9.258979805661191e-99
Epoch 2793: train_loss 0.021714406 val_loss 0.02552738
ttest: -23.66962488061318 pValue 1.0225270377694274e-98
Epoch 2794: train_loss 0.021713482 val_loss 0.025527567
ttest: -23.663237797020336 pValue 1.1253129990343591e-98
Epoch 2795: train_loss 0.02171256 val_loss 0.025527738
ttest: -23.656752945536926 pValue 1.2402401782331219e-98
Epoch 2796: train_loss 0.021711638 val_loss 0.025527926
ttest: -23.65030154432855 pValue 1.3661729395472327e-98
Epoch 2797: train_loss 0.02171072 val_loss 0.025528105
ttest: -23.643951016588495 pValue 1.5025465034555298e-98
Epoch 2798: train_loss 0.021709794 val_loss 0.025528291
ttest: -23.637502981182777 pValue 1.654938334026387e-98
Epoch 2799: train_loss 0.021708876 val_loss 0.025528476
ttest: -23.63136132484522 pValue 1.814268611685123e-98
Epoch 2800: train_loss 0.021707954 val_loss 0.025528653
ttest: -23.624990231533477 pValue 1.9958299395792347e-98
Epoch 2801: train_loss 0.021707032 val_loss 0.025528831
ttest: -23.61872598582323 pValue 2.191937282428897e-98
Epoch 2802: train_loss 0.021706114 val_loss 0.025529023
ttest: -23.612639126372056 pValue 2.400765997219719e-98
Epoch 2803: train_loss 0.02170519 val_loss 0.025529204
ttest: -23.606456665121055 pValue 2.6332372616414274e-98
Epoch 2804: train_loss 0.02170427 val_loss 0.025529377
ttest: -23.60038789597513 pValue 2.883152117553829e-98
Epoch 2805: train_loss 0.02170335 val_loss 0.025529554
ttest: -23.594223593321175 pValue 3.161278830554446e-98
Epoch 2806: train_loss 0.021702431 val_loss 0.025529744
ttest: -23.58817348422656 pValue 3.4601255226150524e-98
Epoch 2807: train_loss 0.02170151 val_loss 0.025529925
ttest: -23.582100275536654 pValue 3.788439927248568e-98
Epoch 2808: train_loss 0.02170059 val_loss 0.025530104
ttest: -23.576144649587828 pValue 4.14038141888075e-98
Epoch 2809: train_loss 0.021699673 val_loss 0.025530284
ttest: -23.570170848612094 pValue 4.52613038747821e-98
Epoch 2810: train_loss 0.02169875 val_loss 0.02553047
ttest: -23.564247802291103 pValue 4.943854876765894e-98
Epoch 2811: train_loss 0.02169783 val_loss 0.025530651
ttest: -23.558234342460832 pValue 5.407384613151286e-98
Epoch 2812: train_loss 0.02169691 val_loss 0.025530826
ttest: -23.552416790737407 pValue 5.896622321414558e-98
Epoch 2813: train_loss 0.021695992 val_loss 0.02553101
ttest: -23.546513629676706 pValue 6.438285653422417e-98
Epoch 2814: train_loss 0.021695072 val_loss 0.025531197
ttest: -23.540662223871557 pValue 7.023960015748743e-98
Epoch 2815: train_loss 0.021694154 val_loss 0.025531381
ttest: -23.534872591274105 pValue 7.65548132243981e-98
Epoch 2816: train_loss 0.021693235 val_loss 0.025531555
ttest: -23.52891897435306 pValue 8.364309913673777e-98
Epoch 2817: train_loss 0.021692317 val_loss 0.025531739
ttest: -23.52317404186131 pValue 9.109562482996985e-98
Epoch 2818: train_loss 0.021691397 val_loss 0.02553192
ttest: -23.517491951168395 pValue 9.911428389714109e-98
Epoch 2819: train_loss 0.021690482 val_loss 0.025532104
ttest: -23.511797625006494 pValue 1.0785520158873286e-97
Epoch 2820: train_loss 0.021689558 val_loss 0.025532281
ttest: -23.506015456629846 pValue 1.175195966656459e-97
Epoch 2821: train_loss 0.021688644 val_loss 0.025532465
ttest: -23.500300272082168 pValue 1.2791545248363892e-97
Epoch 2822: train_loss 0.021687722 val_loss 0.025532648
ttest: -23.49465448640956 pValue 1.3907953077595708e-97
Epoch 2823: train_loss 0.021686807 val_loss 0.02553283
ttest: -23.48907609251219 pValue 1.510581129318185e-97
Epoch 2824: train_loss 0.021685889 val_loss 0.025533019
ttest: -23.483417847809385 pValue 1.6426146837058302e-97
Epoch 2825: train_loss 0.02168497 val_loss 0.025533184
ttest: -23.477831661527688 pValue 1.784173257873642e-97
Epoch 2826: train_loss 0.021684052 val_loss 0.025533367
ttest: -23.47216188190912 pValue 1.940320519648526e-97
Epoch 2827: train_loss 0.021683138 val_loss 0.025533553
ttest: -23.466569062206997 pValue 2.1075931526842198e-97
Epoch 2828: train_loss 0.021682221 val_loss 0.025533728
ttest: -23.460896100453017 pValue 2.291986545058891e-97
Epoch 2829: train_loss 0.021681305 val_loss 0.025533922
ttest: -23.45546051217751 pValue 2.483458280831869e-97
Epoch 2830: train_loss 0.021680387 val_loss 0.025534092
ttest: -23.44994675876926 pValue 2.6940170462979283e-97
Epoch 2831: train_loss 0.02167947 val_loss 0.025534274
ttest: -23.44435287817616 pValue 2.9258700265709395e-97
Epoch 2832: train_loss 0.021678552 val_loss 0.025534455
ttest: -23.438922536986475 pValue 3.169680163886525e-97
Epoch 2833: train_loss 0.021677637 val_loss 0.025534645
ttest: -23.433336167012865 pValue 3.4418073896316836e-97
Epoch 2834: train_loss 0.021676721 val_loss 0.025534822
ttest: -23.427670976492802 pValue 3.741619219248789e-97
Epoch 2835: train_loss 0.021675805 val_loss 0.025535002
ttest: -23.422258234486314 pValue 4.051839065694544e-97
Epoch 2836: train_loss 0.021674886 val_loss 0.02553518
ttest: -23.416851933389527 pValue 4.3871729659691947e-97
Epoch 2837: train_loss 0.021673972 val_loss 0.025535366
ttest: -23.411289949864912 pValue 4.761291420735472e-97
Epoch 2838: train_loss 0.02167306 val_loss 0.025535544
ttest: -23.405736108514912 pValue 5.166458769110009e-97
Epoch 2839: train_loss 0.02167214 val_loss 0.025535721
ttest: -23.400275481409867 pValue 5.59794919326141e-97
Epoch 2840: train_loss 0.021671228 val_loss 0.0255359
ttest: -23.394662010957713 pValue 6.079290267150062e-97
Epoch 2841: train_loss 0.021670312 val_loss 0.025536083
ttest: -23.38931546544325 pValue 6.575067343983865e-97
Epoch 2842: train_loss 0.021669395 val_loss 0.025536267
ttest: -23.383813397470195 pValue 7.127752508267936e-97
Epoch 2843: train_loss 0.021668483 val_loss 0.025536448
ttest: -23.378328094577054 pValue 7.72458820079848e-97
Epoch 2844: train_loss 0.021667568 val_loss 0.025536628
ttest: -23.37277511693961 pValue 8.37961571902209e-97
Epoch 2845: train_loss 0.021666652 val_loss 0.0255368
ttest: -23.367239439262658 pValue 9.087392291231297e-97
Epoch 2846: train_loss 0.021665739 val_loss 0.025536986
ttest: -23.361637194619234 pValue 9.864443698781371e-97
Epoch 2847: train_loss 0.021664824 val_loss 0.025537161
ttest: -23.35622978583985 pValue 1.0675863481126397e-96
Epoch 2848: train_loss 0.021663912 val_loss 0.025537338
ttest: -23.350669657683643 pValue 1.1580261470001437e-96
Epoch 2849: train_loss 0.021662999 val_loss 0.025537526
ttest: -23.34522424064411 pValue 1.2538850545732597e-96
Epoch 2850: train_loss 0.021662084 val_loss 0.025537701
ttest: -23.33971919711467 pValue 1.3588409032576335e-96
Epoch 2851: train_loss 0.021661166 val_loss 0.025537888
ttest: -23.333970076564693 pValue 1.4779668228745518e-96
Epoch 2852: train_loss 0.021660255 val_loss 0.025538055
ttest: -23.328339451770926 pValue 1.6045725064139847e-96
Epoch 2853: train_loss 0.021659344 val_loss 0.02553824
ttest: -23.322920622619094 pValue 1.7363534997677675e-96
Epoch 2854: train_loss 0.02165843 val_loss 0.025538424
ttest: -23.317263543829707 pValue 1.885657470348857e-96
Epoch 2855: train_loss 0.021657515 val_loss 0.025538605
ttest: -23.311638519060327 pValue 2.046699592100652e-96
Epoch 2856: train_loss 0.021656604 val_loss 0.025538785
ttest: -23.306048580886088 pValue 2.220198554270595e-96
Epoch 2857: train_loss 0.021655692 val_loss 0.02553897
ttest: -23.30049443092704 pValue 2.4069717276432912e-96
Epoch 2858: train_loss 0.021654781 val_loss 0.025539137
ttest: -23.294701252567933 pValue 2.618785697995587e-96
Epoch 2859: train_loss 0.021653865 val_loss 0.025539314
ttest: -23.289037114008206 pValue 2.843520203548347e-96
Epoch 2860: train_loss 0.021652956 val_loss 0.025539493
ttest: -23.283225703911477 pValue 3.094267090008092e-96
Epoch 2861: train_loss 0.021652041 val_loss 0.025539672
ttest: -23.27745559276621 pValue 3.3648323212163618e-96
Epoch 2862: train_loss 0.02165113 val_loss 0.025539847
ttest: -23.271537117658777 pValue 3.667082670662411e-96
Epoch 2863: train_loss 0.021650217 val_loss 0.025540028
ttest: -23.265855710630902 pValue 3.9819290932120635e-96
Epoch 2864: train_loss 0.021649309 val_loss 0.025540216
ttest: -23.260127407819038 pValue 4.326629544233631e-96
Epoch 2865: train_loss 0.021648394 val_loss 0.025540397
ttest: -23.25425511272318 pValue 4.711158894658668e-96
Epoch 2866: train_loss 0.021647485 val_loss 0.02554056
ttest: -23.24823724962226 pValue 5.140884129486373e-96
Epoch 2867: train_loss 0.021646574 val_loss 0.025540752
ttest: -23.24217302410873 pValue 5.613406329309159e-96
Epoch 2868: train_loss 0.021645665 val_loss 0.025540927
ttest: -23.236258441887998 pValue 6.115113948574444e-96
Epoch 2869: train_loss 0.02164475 val_loss 0.025541104
ttest: -23.230401574342945 pValue 6.655433817937624e-96
Epoch 2870: train_loss 0.021643844 val_loss 0.02554129
ttest: -23.224204049391524 pValue 7.2805051801995075e-96
Epoch 2871: train_loss 0.02164293 val_loss 0.025541466
ttest: -23.218163677104908 pValue 7.944850859459418e-96
Epoch 2872: train_loss 0.02164202 val_loss 0.025541633
ttest: -23.2119824338991 pValue 8.687781324662571e-96
Epoch 2873: train_loss 0.021641111 val_loss 0.025541812
ttest: -23.205761891187226 pValue 9.505222492300297e-96
Epoch 2874: train_loss 0.0216402 val_loss 0.025542008
ttest: -23.19939870594941 pValue 1.0421374245383907e-95
Epoch 2875: train_loss 0.021639293 val_loss 0.025542172
ttest: -23.193308442801843 pValue 1.1377960668182791e-95
Epoch 2876: train_loss 0.021638384 val_loss 0.025542362
ttest: -23.18676972862268 pValue 1.2506139605118414e-95
Epoch 2877: train_loss 0.021637471 val_loss 0.025542533
ttest: -23.18050570480214 pValue 1.3688205160259463e-95
Epoch 2878: train_loss 0.021636564 val_loss 0.025542708
ttest: -23.174004328251343 pValue 1.503489410815137e-95
Epoch 2879: train_loss 0.021635655 val_loss 0.02554289
ttest: -23.167467726144316 pValue 1.6521729198625172e-95
Epoch 2880: train_loss 0.021634744 val_loss 0.025543064
ttest: -23.16100651090361 pValue 1.8133615970586938e-95
Epoch 2881: train_loss 0.021633836 val_loss 0.025543245
ttest: -23.1543050501535 pValue 1.997383764273257e-95
Epoch 2882: train_loss 0.021632927 val_loss 0.02554342
ttest: -23.147678466957174 pValue 2.197430069229905e-95
Epoch 2883: train_loss 0.021632018 val_loss 0.025543593
ttest: -23.140812026270762 pValue 2.4261264115135217e-95
Epoch 2884: train_loss 0.021631109 val_loss 0.025543774
ttest: -23.134241221676618 pValue 2.6664680088939177e-95
Epoch 2885: train_loss 0.021630201 val_loss 0.025543956
ttest: -23.127213892975902 pValue 2.9507147401404524e-95
Epoch 2886: train_loss 0.021629296 val_loss 0.025544126
ttest: -23.12027054595836 pValue 3.2608678232013806e-95
Epoch 2887: train_loss 0.021628385 val_loss 0.025544306
ttest: -23.11330434215324 pValue 3.604590287746227e-95
Epoch 2888: train_loss 0.021627478 val_loss 0.025544485
ttest: -23.106207261593674 pValue 3.992130865885492e-95
Epoch 2889: train_loss 0.021626571 val_loss 0.025544658
ttest: -23.098981622114884 pValue 4.429597245217237e-95
Epoch 2890: train_loss 0.021625662 val_loss 0.025544839
ttest: -23.091739548962362 pValue 4.915829763154239e-95
Epoch 2891: train_loss 0.021624757 val_loss 0.025545023
ttest: -23.084368074496645 pValue 5.465683345180428e-95
Epoch 2892: train_loss 0.021623848 val_loss 0.025545185
ttest: -23.07721150706905 pValue 6.056821699127385e-95
Epoch 2893: train_loss 0.02162294 val_loss 0.025545368
ttest: -23.0695868436871 pValue 6.759031400899933e-95
Epoch 2894: train_loss 0.021622038 val_loss 0.025545543
ttest: -23.061951452645644 pValue 7.54324383225069e-95
Epoch 2895: train_loss 0.021621129 val_loss 0.02554572
ttest: -23.054417269017165 pValue 8.404853596229737e-95
Epoch 2896: train_loss 0.021620223 val_loss 0.025545903
ttest: -23.046647158148236 pValue 9.397545254736678e-95
Epoch 2897: train_loss 0.021619318 val_loss 0.025546081
ttest: -23.038868737186927 pValue 1.0507890720482336e-94
Epoch 2898: train_loss 0.021618413 val_loss 0.02554625
ttest: -23.030615533101823 pValue 1.1833060059335387e-94
Epoch 2899: train_loss 0.021617504 val_loss 0.025546428
ttest: -23.022709555020977 pValue 1.3254372230294442e-94
Epoch 2900: train_loss 0.021616599 val_loss 0.025546605
ttest: -23.014682709583173 pValue 1.4872226403069728e-94
Epoch 2901: train_loss 0.021615693 val_loss 0.025546778
ttest: -23.00641856478741 pValue 1.674602335706006e-94
Epoch 2902: train_loss 0.021614792 val_loss 0.025546959
ttest: -22.998154591335062 pValue 1.8854111777499431e-94
Epoch 2903: train_loss 0.021613885 val_loss 0.025547128
ttest: -22.98965115195492 pValue 2.1302595197854222e-94
Epoch 2904: train_loss 0.021612976 val_loss 0.025547309
ttest: -22.98115424887749 pValue 2.406438484316181e-94
Epoch 2905: train_loss 0.021612074 val_loss 0.02554748
ttest: -22.972541378573638 pValue 2.722933639697421e-94
Epoch 2906: train_loss 0.02161117 val_loss 0.025547655
ttest: -22.963937400510222 pValue 3.08034194685308e-94
Epoch 2907: train_loss 0.021610266 val_loss 0.025547836
ttest: -22.95497527822583 pValue 3.5032358672034655e-94
Epoch 2908: train_loss 0.021609362 val_loss 0.025548019
ttest: -22.945899056559256 pValue 3.9906683689770866e-94
Epoch 2909: train_loss 0.021608457 val_loss 0.02554819
ttest: -22.936837149428513 pValue 4.5444815316282603e-94
Epoch 2910: train_loss 0.021607555 val_loss 0.025548358
ttest: -22.927536658628714 pValue 5.193321546459817e-94
Epoch 2911: train_loss 0.021606648 val_loss 0.025548534
ttest: -22.918250613472487 pValue 5.932893822923754e-94
Epoch 2912: train_loss 0.021605747 val_loss 0.025548717
ttest: -22.908727062328435 pValue 6.801449625923703e-94
Epoch 2913: train_loss 0.021604843 val_loss 0.02554888
ttest: -22.899095219417774 pValue 7.809112434948796e-94
Epoch 2914: train_loss 0.021603938 val_loss 0.025549058
ttest: -22.889227124880378 pValue 8.997169806118934e-94
Epoch 2915: train_loss 0.021603035 val_loss 0.025549233
ttest: -22.879640419233194 pValue 1.0320889091804126e-93
Epoch 2916: train_loss 0.021602131 val_loss 0.0255494
ttest: -22.869429610876193 pValue 1.195022850635315e-93
Epoch 2917: train_loss 0.02160123 val_loss 0.025549576
ttest: -22.859376621548048 pValue 1.3802317671950922e-93
Epoch 2918: train_loss 0.021600328 val_loss 0.025549756
ttest: -22.84922385435022 pValue 1.5963711200400434e-93
Epoch 2919: train_loss 0.021599427 val_loss 0.025549928
ttest: -22.838572033527687 pValue 1.8601191517603723e-93
Epoch 2920: train_loss 0.021598522 val_loss 0.02555011
ttest: -22.828084459973716 pValue 2.1618244476139805e-93
Epoch 2921: train_loss 0.021597618 val_loss 0.025550278
ttest: -22.817367179986046 pValue 2.52090758353176e-93
Epoch 2922: train_loss 0.021596717 val_loss 0.025550451
ttest: -22.80641610961546 pValue 2.949687814772686e-93
Epoch 2923: train_loss 0.021595817 val_loss 0.025550628
ttest: -22.79536919247326 pValue 3.4559899279967346e-93
Epoch 2924: train_loss 0.021594916 val_loss 0.025550794
ttest: -22.784224258207615 pValue 4.054712761028273e-93
Epoch 2925: train_loss 0.021594014 val_loss 0.02555097
ttest: -22.77285036610207 pValue 4.773056030254432e-93
Epoch 2926: train_loss 0.021593114 val_loss 0.025551146
ttest: -22.761385048660983 pValue 5.625737094478693e-93
Epoch 2927: train_loss 0.021592213 val_loss 0.025551325
ttest: -22.74968916323098 pValue 6.653039419239934e-93
Epoch 2928: train_loss 0.021591313 val_loss 0.025551494
ttest: -22.737905928181906 pValue 7.877335874532446e-93
Epoch 2929: train_loss 0.02159041 val_loss 0.025551666
ttest: -22.725615484213733 pValue 9.397414813548628e-93
Epoch 2930: train_loss 0.02158951 val_loss 0.02555184
ttest: -22.713519500238164 pValue 1.1176535544027995e-92
Epoch 2931: train_loss 0.02158861 val_loss 0.02555201
ttest: -22.701196716026026 pValue 1.3336334611240038e-92
Epoch 2932: train_loss 0.021587709 val_loss 0.025552202
ttest: -22.688790448258015 pValue 1.593146809142637e-92
Epoch 2933: train_loss 0.02158681 val_loss 0.025552364
ttest: -22.675871918923185 pValue 1.9176727575739666e-92
Epoch 2934: train_loss 0.021585908 val_loss 0.02555254
ttest: -22.663159123400696 pValue 2.3008450271044847e-92
Epoch 2935: train_loss 0.02158501 val_loss 0.0255527
ttest: -22.649788419482082 pValue 2.787737872372124e-92
Epoch 2936: train_loss 0.021584114 val_loss 0.025552873
ttest: -22.6366276132658 pValue 3.366528269041139e-92
Epoch 2937: train_loss 0.02158321 val_loss 0.025553051
ttest: -22.62309795960715 pValue 4.0875998803152105e-92
Epoch 2938: train_loss 0.021582315 val_loss 0.025553219
ttest: -22.609639378193254 pValue 4.9571124977833106e-92
Epoch 2939: train_loss 0.021581413 val_loss 0.025553396
ttest: -22.595517082126584 pValue 6.071191021975735e-92
Epoch 2940: train_loss 0.021580514 val_loss 0.02555356
ttest: -22.581319368417383 pValue 7.443050207875908e-92
Epoch 2941: train_loss 0.021579616 val_loss 0.025553735
ttest: -22.566898973332247 pValue 9.154254422316174e-92
Epoch 2942: train_loss 0.021578718 val_loss 0.025553908
ttest: -22.552409116709576 pValue 1.1269025462792999e-91
Epoch 2943: train_loss 0.02157782 val_loss 0.025554072
ttest: -22.53770254599961 pValue 1.3915675652922639e-91
Epoch 2944: train_loss 0.021576922 val_loss 0.025554253
ttest: -22.52277561947796 pValue 1.7238498344549809e-91
Epoch 2945: train_loss 0.021576023 val_loss 0.025554428
ttest: -22.50778816184637 pValue 2.137101469376069e-91
Epoch 2946: train_loss 0.021575127 val_loss 0.025554601
ttest: -22.492583304791502 pValue 2.6577087744604947e-91
Epoch 2947: train_loss 0.02157423 val_loss 0.025554763
ttest: -22.476853470200812 pValue 3.3308693267492533e-91
Epoch 2948: train_loss 0.02157333 val_loss 0.025554936
ttest: -22.460906720921493 pValue 4.1875357371269326e-91
Epoch 2949: train_loss 0.021572433 val_loss 0.02555511
ttest: -22.445063501300137 pValue 5.255498835052595e-91
Epoch 2950: train_loss 0.02157154 val_loss 0.025555285
ttest: -22.42853186672159 pValue 6.663492033400251e-91
Epoch 2951: train_loss 0.021570642 val_loss 0.025555449
ttest: -22.412103259972252 pValue 8.434248124119758e-91
Epoch 2952: train_loss 0.021569744 val_loss 0.025555622
ttest: -22.395630736740163 pValue 1.0680885900158259e-90
Epoch 2953: train_loss 0.02156885 val_loss 0.025555791
ttest: -22.378306520723065 pValue 1.369821172625745e-90
Epoch 2954: train_loss 0.021567954 val_loss 0.025555963
ttest: -22.36109400482111 pValue 1.7535487271443917e-90
Epoch 2955: train_loss 0.021567056 val_loss 0.02555613
ttest: -22.34334631559476 pValue 2.2625373742781986e-90
Epoch 2956: train_loss 0.02156616 val_loss 0.025556305
ttest: -22.32572091895764 pValue 2.9134001506032845e-90
Epoch 2957: train_loss 0.021565264 val_loss 0.025556475
ttest: -22.3075615951861 pValue 3.781095217867341e-90
Epoch 2958: train_loss 0.02156437 val_loss 0.025556652
ttest: -22.289531552903792 pValue 4.896827156234966e-90
Epoch 2959: train_loss 0.021563472 val_loss 0.025556821
ttest: -22.270799781125493 pValue 6.40793144197656e-90
Epoch 2960: train_loss 0.021562578 val_loss 0.025556993
ttest: -22.252199535636123 pValue 8.367289331141056e-90
Epoch 2961: train_loss 0.021561682 val_loss 0.025557159
ttest: -22.23306385112864 pValue 1.1012072160032905e-89
Epoch 2962: train_loss 0.02156079 val_loss 0.02555733
ttest: -22.213899410789008 pValue 1.4496453822527605e-89
Epoch 2963: train_loss 0.021559894 val_loss 0.0255575
ttest: -22.194025816737366 pValue 1.9284220491450004e-89
Epoch 2964: train_loss 0.021559 val_loss 0.025557665
ttest: -22.174295854378396 pValue 2.5593158443711736e-89
Epoch 2965: train_loss 0.021558106 val_loss 0.025557838
ttest: -22.15385540520951 pValue 3.4324034706399095e-89
Epoch 2966: train_loss 0.02155721 val_loss 0.025558002
ttest: -22.13356635169122 pValue 4.5920048204353315e-89
Epoch 2967: train_loss 0.021556318 val_loss 0.025558172
ttest: -22.11255812007953 pValue 6.208861251639158e-89
Epoch 2968: train_loss 0.021555424 val_loss 0.025558345
ttest: -22.091888871890983 pValue 8.350758421114167e-89
Epoch 2969: train_loss 0.021554532 val_loss 0.02555851
ttest: -22.07032718823999 pValue 1.1380761158577344e-88
Epoch 2970: train_loss 0.021553636 val_loss 0.02555868
ttest: -22.048751959882047 pValue 1.5510204904521775e-88
Epoch 2971: train_loss 0.021552745 val_loss 0.025558848
ttest: -22.026810990937946 pValue 2.1249994806490983e-88
Epoch 2972: train_loss 0.021551851 val_loss 0.02555901
ttest: -22.00468841020021 pValue 2.9187302146132e-88
Epoch 2973: train_loss 0.021550957 val_loss 0.025559183
ttest: -21.982023817530077 pValue 4.040807950353946e-88
Epoch 2974: train_loss 0.021550067 val_loss 0.025559353
ttest: -21.95935727399019 pValue 5.593244854455896e-88
Epoch 2975: train_loss 0.02154917 val_loss 0.02555953
ttest: -21.93614938401929 pValue 7.803517824916438e-88
Epoch 2976: train_loss 0.021548282 val_loss 0.025559686
ttest: -21.912764717711838 pValue 1.0913750471246589e-87
Epoch 2977: train_loss 0.02154739 val_loss 0.025559856
ttest: -21.888836322581568 pValue 1.5385097458736923e-87
Epoch 2978: train_loss 0.021546496 val_loss 0.025560014
ttest: -21.864544026813466 pValue 2.1802033800769033e-87
Epoch 2979: train_loss 0.021545604 val_loss 0.025560189
ttest: -21.840082815373272 pValue 3.096661953496294e-87
Epoch 2980: train_loss 0.021544712 val_loss 0.025560353
ttest: -21.81526311890583 pValue 4.421018917031399e-87
Epoch 2981: train_loss 0.021543821 val_loss 0.025560519
ttest: -21.78989588924413 pValue 6.362228175896012e-87
Epoch 2982: train_loss 0.021542931 val_loss 0.02556069
ttest: -21.76474212356428 pValue 9.12445838314334e-87
Epoch 2983: train_loss 0.021542042 val_loss 0.02556086
ttest: -21.738663663316565 pValue 1.3265046483899442e-86
Epoch 2984: train_loss 0.021541152 val_loss 0.025561025
ttest: -21.712613604594324 pValue 1.9271752375077892e-86
Epoch 2985: train_loss 0.021540256 val_loss 0.0255612
ttest: -21.68640344713095 pValue 2.805869699924072e-86
Epoch 2986: train_loss 0.02153937 val_loss 0.025561364
ttest: -21.659062839349343 pValue 4.153780162043072e-86
Epoch 2987: train_loss 0.02153848 val_loss 0.025561523
ttest: -21.631957472090278 pValue 6.126092834068315e-86
Epoch 2988: train_loss 0.021537589 val_loss 0.025561696
ttest: -21.604697700579916 pValue 9.0534690036209e-86
Epoch 2989: train_loss 0.021536699 val_loss 0.02556186
ttest: -21.576498048352423 pValue 1.3565300441511593e-85
Epoch 2990: train_loss 0.021535806 val_loss 0.025562035
ttest: -21.54814648155837 pValue 2.036636484516355e-85
Epoch 2991: train_loss 0.021534918 val_loss 0.025562197
ttest: -21.519447619638584 pValue 3.0727886547426735e-85
Epoch 2992: train_loss 0.02153403 val_loss 0.025562363
ttest: -21.49061092497281 pValue 4.644374292189983e-85
Epoch 2993: train_loss 0.021533139 val_loss 0.02556253
ttest: -21.461023935130598 pValue 7.096786823980539e-85
Epoch 2994: train_loss 0.02153225 val_loss 0.02556269
ttest: -21.431299255444333 pValue 1.0863415115821028e-84
Epoch 2995: train_loss 0.021531364 val_loss 0.025562854
ttest: -21.401026941384977 pValue 1.6760698759893088e-84
Epoch 2996: train_loss 0.021530474 val_loss 0.025563024
ttest: -21.37041618060521 pValue 2.5982542135984324e-84
Epoch 2997: train_loss 0.021529581 val_loss 0.025563188
ttest: -21.339674803597358 pValue 4.034495008691662e-84
Epoch 2998: train_loss 0.021528699 val_loss 0.02556336
ttest: -21.308178617970214 pValue 6.333544718802811e-84
Epoch 2999: train_loss 0.021527808 val_loss 0.025563523
ttest: -21.276138090056424 pValue 1.0020503730405057e-83
Epoch 3000: train_loss 0.021526922 val_loss 0.025563681
ttest: -21.2443935337552 pValue 1.5779296379696766e-83
Epoch 3001: train_loss 0.021526035 val_loss 0.025563845
ttest: -21.211474382404056 pValue 2.5277474523136034e-83
Epoch 3002: train_loss 0.021525148 val_loss 0.025564006
ttest: -21.178860242720727 pValue 4.029709299074333e-83
Epoch 3003: train_loss 0.02152426 val_loss 0.025564183
ttest: -21.14528303338892 pValue 6.514560686018512e-83
Epoch 3004: train_loss 0.021523373 val_loss 0.02556434
ttest: -21.11159233846351 pValue 1.0545938370503359e-82
Epoch 3005: train_loss 0.021522487 val_loss 0.025564509
ttest: -21.07736451427096 pValue 1.7202896205656638e-82
Epoch 3006: train_loss 0.021521598 val_loss 0.025564672
ttest: -21.04259757441509 pValue 2.8277583290008734e-82
Epoch 3007: train_loss 0.021520711 val_loss 0.025564838
ttest: -21.007730672630885 pValue 4.6534547217132414e-82
Epoch 3008: train_loss 0.021519825 val_loss 0.025564998
ttest: -20.972324082987168 pValue 7.716667994015578e-82
Epoch 3009: train_loss 0.021518942 val_loss 0.025565166
ttest: -20.93660534428954 pValue 1.285096361736894e-81
Epoch 3010: train_loss 0.021518057 val_loss 0.025565324
ttest: -20.900127778522776 pValue 2.1635211217211225e-81
Epoch 3011: train_loss 0.021517167 val_loss 0.0255655
ttest: -20.86311228493639 pValue 3.6701116048722764e-81
Epoch 3012: train_loss 0.021516284 val_loss 0.025565658
ttest: -20.82601088897743 pValue 6.231363703405478e-81
Epoch 3013: train_loss 0.021515395 val_loss 0.025565822
ttest: -20.788150103138822 pValue 1.0695284990093957e-80
Epoch 3014: train_loss 0.021514514 val_loss 0.025565986
ttest: -20.750210375469408 pValue 1.8371097517212576e-80
Epoch 3015: train_loss 0.021513628 val_loss 0.025566148
ttest: -20.71197373278689 pValue 3.16816230526333e-80
Epoch 3016: train_loss 0.021512741 val_loss 0.025566308
ttest: -20.672976849582533 pValue 5.522985482628445e-80
Epoch 3017: train_loss 0.02151186 val_loss 0.025566474
ttest: -20.63322359878845 pValue 9.731957424742095e-80
Epoch 3018: train_loss 0.021510975 val_loss 0.025566638
ttest: -20.593635722195692 pValue 1.7099402981931707e-79
Epoch 3019: train_loss 0.021510089 val_loss 0.025566796
ttest: -20.553292509461468 pValue 3.036717909224583e-79
Epoch 3020: train_loss 0.021509204 val_loss 0.025566954
ttest: -20.51242404772362 pValue 5.432391797264827e-79
Epoch 3021: train_loss 0.021508323 val_loss 0.025567122
ttest: -20.471270375980158 pValue 9.754408077363067e-79
Epoch 3022: train_loss 0.021507438 val_loss 0.025567286
ttest: -20.429356951794833 pValue 1.7703465263993947e-78
Epoch 3023: train_loss 0.021506554 val_loss 0.025567442
ttest: -20.38740049189883 pValue 3.2135373327618687e-78
Epoch 3024: train_loss 0.02150567 val_loss 0.025567614
ttest: -20.344448655314782 pValue 5.916191560033267e-78
Epoch 3025: train_loss 0.021504791 val_loss 0.025567768
ttest: -20.301703233988313 pValue 1.0853507204229531e-77
Epoch 3026: train_loss 0.021503907 val_loss 0.025567936
ttest: -20.257476004944834 pValue 2.0338097518740053e-77
Epoch 3027: train_loss 0.021503024 val_loss 0.0255681
ttest: -20.213709745930903 pValue 3.783457662046783e-77
Epoch 3028: train_loss 0.021502139 val_loss 0.02556825
ttest: -20.16943675338672 pValue 7.086979249473538e-77
Epoch 3029: train_loss 0.021501256 val_loss 0.025568414
ttest: -20.123921415282247 pValue 1.3511027527231901e-76
Epoch 3030: train_loss 0.021500377 val_loss 0.025568571
ttest: -20.078390618018716 pValue 2.574954760784643e-76
Epoch 3031: train_loss 0.021499496 val_loss 0.025568742
ttest: -20.032357343424994 pValue 4.9407047572481443e-76
Epoch 3032: train_loss 0.021498611 val_loss 0.025568895
ttest: -19.986068956718604 pValue 9.50984452800475e-76
Epoch 3033: train_loss 0.02149773 val_loss 0.025569063
ttest: -19.93878049028903 pValue 1.8562269260348868e-75
Epoch 3034: train_loss 0.02149685 val_loss 0.025569221
ttest: -19.89175358689075 pValue 3.6071841882891493e-75
Epoch 3035: train_loss 0.021495966 val_loss 0.025569381
ttest: -19.842965372305855 pValue 7.187328809083725e-75
Epoch 3036: train_loss 0.021495087 val_loss 0.025569547
ttest: -19.79470324896643 pValue 1.4202352349103505e-74
Epoch 3037: train_loss 0.021494206 val_loss 0.025569702
ttest: -19.745692603362038 pValue 2.835266883724659e-74
Epoch 3038: train_loss 0.021493323 val_loss 0.025569864
ttest: -19.696196228272477 pValue 5.696359451466721e-74
Epoch 3039: train_loss 0.021492444 val_loss 0.025570024
ttest: -19.646214952067815 pValue 1.1517443425465888e-73
Epoch 3040: train_loss 0.021491565 val_loss 0.025570186
ttest: -19.595490042321934 pValue 2.352267988427705e-73
Epoch 3041: train_loss 0.021490682 val_loss 0.025570344
ttest: -19.54455014093136 pValue 4.815633122366675e-73
Epoch 3042: train_loss 0.021489805 val_loss 0.025570506
ttest: -19.492608296149484 pValue 9.995127613258737e-73
Epoch 3043: train_loss 0.021488924 val_loss 0.025570657
ttest: -19.440981019223493 pValue 2.063572633447036e-72
Epoch 3044: train_loss 0.021488046 val_loss 0.025570817
ttest: -19.388355418446555 pValue 4.3188961597423507e-72
Epoch 3045: train_loss 0.021487167 val_loss 0.02557098
ttest: -19.3352610883446 pValue 9.0931221832612e-72
Epoch 3046: train_loss 0.021486288 val_loss 0.025571132
ttest: -19.281164397843643 pValue 1.94072654836182e-71
Epoch 3047: train_loss 0.021485409 val_loss 0.0255713
ttest: -19.227146788731666 pValue 4.133826749235236e-71
Epoch 3048: train_loss 0.02148453 val_loss 0.025571456
ttest: -19.172123705434284 pValue 8.925647190931226e-71
Epoch 3049: train_loss 0.02148365 val_loss 0.02557162
ttest: -19.116918656011936 pValue 1.930519439067909e-70
Epoch 3050: train_loss 0.021482773 val_loss 0.025571775
ttest: -19.060984195612864 pValue 4.2155604032462544e-70
Epoch 3051: train_loss 0.021481896 val_loss 0.025571939
ttest: -19.00515489564452 pValue 9.182869548424895e-70
Epoch 3052: train_loss 0.021481019 val_loss 0.025572091
ttest: -18.948326949592026 pValue 2.0271883307137335e-69
Epoch 3053: train_loss 0.02148014 val_loss 0.025572248
ttest: -18.89078263458115 pValue 4.516752681160434e-69
Epoch 3054: train_loss 0.021479262 val_loss 0.02557241
ttest: -18.83251787620212 pValue 1.0157373328872596e-68
Epoch 3055: train_loss 0.021478381 val_loss 0.025572564
ttest: -18.774377985077283 pValue 2.2777928581577365e-68
Epoch 3056: train_loss 0.021477507 val_loss 0.025572723
ttest: -18.71524433556709 pValue 5.175232021178806e-68
Epoch 3057: train_loss 0.021476628 val_loss 0.025572887
ttest: -18.655399054126676 pValue 1.1864796906919959e-67
Epoch 3058: train_loss 0.021475755 val_loss 0.025573043
ttest: -18.595128077552157 pValue 2.7335385593914537e-67
Epoch 3059: train_loss 0.021474877 val_loss 0.025573188
ttest: -18.534723873034807 pValue 6.30250325801531e-67
Epoch 3060: train_loss 0.021474002 val_loss 0.025573349
ttest: -18.47332927854389 pValue 1.4718948627694445e-66
Epoch 3061: train_loss 0.021473126 val_loss 0.0255735
ttest: -18.411812431214702 pValue 3.4392730183032197e-66
Epoch 3062: train_loss 0.02147225 val_loss 0.025573665
ttest: -18.349307896608483 pValue 8.138971904236156e-66
Epoch 3063: train_loss 0.021471372 val_loss 0.025573822
ttest: -18.286398665861853 pValue 1.9346087635072715e-65
Epoch 3064: train_loss 0.021470496 val_loss 0.025573974
ttest: -18.22249840460722 pValue 4.65692210840083e-65
Epoch 3065: train_loss 0.021469621 val_loss 0.025574137
ttest: -18.159091552814836 pValue 1.1117625205526794e-64
Epoch 3066: train_loss 0.021468746 val_loss 0.025574286
ttest: -18.094408973895632 pValue 2.698365205862612e-64
Epoch 3067: train_loss 0.021467874 val_loss 0.025574448
ttest: -18.029338939057386 pValue 6.575508017481327e-64
Epoch 3068: train_loss 0.021466997 val_loss 0.025574602
ttest: -17.963585294977104 pValue 1.615384689269527e-63
Epoch 3069: train_loss 0.021466123 val_loss 0.025574768
ttest: -17.897453827192923 pValue 3.983496513703777e-63
Epoch 3070: train_loss 0.021465247 val_loss 0.025574904
ttest: -17.83065116307552 pValue 9.900344345294331e-63
Epoch 3071: train_loss 0.021464372 val_loss 0.025575066
ttest: -17.76347778018152 pValue 2.4694140375417085e-62
Epoch 3072: train_loss 0.0214635 val_loss 0.025575226
ttest: -17.69533347829957 pValue 6.232876888568247e-62
Epoch 3073: train_loss 0.021462627 val_loss 0.025575375
ttest: -17.626827547697456 pValue 1.578477588328032e-61
Epoch 3074: train_loss 0.021461753 val_loss 0.025575541
ttest: -17.557353236447522 pValue 4.0444504132195554e-61
Epoch 3075: train_loss 0.021460878 val_loss 0.025575683
ttest: -17.488147613180775 pValue 1.0306586014412215e-60
Epoch 3076: train_loss 0.021460004 val_loss 0.02557584
ttest: -17.417976255343543 pValue 2.6569003718255706e-60
Epoch 3077: train_loss 0.021459132 val_loss 0.025575992
ttest: -17.346841969453862 pValue 6.927599147281911e-60
Epoch 3078: train_loss 0.021458259 val_loss 0.02557615
ttest: -17.275998339967785 pValue 1.7957367621532542e-59
Epoch 3079: train_loss 0.021457387 val_loss 0.025576293
ttest: -17.203570612520213 pValue 4.7476730311604774e-59
Epoch 3080: train_loss 0.021456515 val_loss 0.02557646
ttest: -17.13176651633482 pValue 1.2421105315832409e-58
Epoch 3081: train_loss 0.02145564 val_loss 0.025576614
ttest: -17.058058202953706 pValue 3.3283393300874436e-58
Epoch 3082: train_loss 0.021454768 val_loss 0.025576765
ttest: -16.98467141192505 pValue 8.861067433249705e-58
Epoch 3083: train_loss 0.0214539 val_loss 0.025576917
ttest: -16.910337694131346 pValue 2.3844924113068906e-57
Epoch 3084: train_loss 0.021453023 val_loss 0.025577068
ttest: -16.83570153685116 pValue 6.428826947744094e-57
Epoch 3085: train_loss 0.021452157 val_loss 0.025577227
ttest: -16.76044734138564 pValue 1.7438834798569978e-56
Epoch 3086: train_loss 0.021451285 val_loss 0.025577381
ttest: -16.684904323147578 pValue 4.738032220239609e-56
Epoch 3087: train_loss 0.021450413 val_loss 0.025577525
ttest: -16.608095898723878 pValue 1.30628610901433e-55
Epoch 3088: train_loss 0.02144954 val_loss 0.025577689
ttest: -16.531337059610646 pValue 3.590342395358063e-55
Epoch 3089: train_loss 0.021448672 val_loss 0.025577838
ttest: -16.45331396692976 pValue 1.0011399311921632e-54
Epoch 3090: train_loss 0.0214478 val_loss 0.025577994
ttest: -16.375354093057762 pValue 2.782133723606252e-54
Epoch 3091: train_loss 0.021446932 val_loss 0.025578137
ttest: -16.296802990673054 pValue 7.772315149385094e-54
Epoch 3092: train_loss 0.02144606 val_loss 0.025578288
ttest: -16.21699223133885 pValue 2.201980224415363e-53
Epoch 3093: train_loss 0.02144519 val_loss 0.025578452
ttest: -16.137267537990905 pValue 6.214210721949827e-53
Epoch 3094: train_loss 0.02144432 val_loss 0.025578596
ttest: -16.05628745739169 pValue 1.7779779761924084e-52
Epoch 3095: train_loss 0.021443453 val_loss 0.025578748
ttest: -15.975746375201508 pValue 5.043226462078063e-52
Epoch 3096: train_loss 0.021442581 val_loss 0.025578903
ttest: -15.894295024889418 pValue 1.4434656353238263e-51
Epoch 3097: train_loss 0.021441717 val_loss 0.025579046
ttest: -15.81159203979418 pValue 4.186935069464274e-51
Epoch 3098: train_loss 0.021440843 val_loss 0.025579205
ttest: -15.72935987420409 pValue 1.2033049219454743e-50
Epoch 3099: train_loss 0.021439975 val_loss 0.025579356
ttest: -15.645189115942863 pValue 3.5352300118457524e-50
Epoch 3100: train_loss 0.021439107 val_loss 0.025579503
ttest: -15.561502604077756 pValue 1.0287606889514342e-49
Epoch 3101: train_loss 0.02143824 val_loss 0.025579648
ttest: -15.476923316430664 pValue 3.0184125734777174e-49
Epoch 3102: train_loss 0.021437371 val_loss 0.025579806
ttest: -15.3921509334479 pValue 8.848057435761767e-49
Epoch 3103: train_loss 0.021436507 val_loss 0.02557996
ttest: -15.306492947589705 pValue 2.6142348140288587e-48
Epoch 3104: train_loss 0.021435633 val_loss 0.025580108
ttest: -15.2199474690375 pValue 7.78421662495353e-48
Epoch 3105: train_loss 0.021434767 val_loss 0.02558025
ttest: -15.13322991894964 pValue 2.3144502655021423e-47
Epoch 3106: train_loss 0.021433901 val_loss 0.025580414
ttest: -15.045991134498193 pValue 6.901386061310577e-47
Epoch 3107: train_loss 0.021433033 val_loss 0.02558056
ttest: -14.957520947479061 pValue 2.0822040193568494e-46
Epoch 3108: train_loss 0.021432167 val_loss 0.025580706
ttest: -14.869616346642216 pValue 6.213225982357006e-46
Epoch 3109: train_loss 0.0214313 val_loss 0.025580857
ttest: -14.780848261716955 pValue 1.866820290988186e-45
Epoch 3110: train_loss 0.021430433 val_loss 0.025581008
ttest: -14.69049657635117 pValue 5.697802567618378e-45
Epoch 3111: train_loss 0.021429569 val_loss 0.025581162
ttest: -14.600374549680673 pValue 1.726787966494826e-44
Epoch 3112: train_loss 0.021428697 val_loss 0.0255813
ttest: -14.509765314508375 pValue 5.242583691868464e-44
Epoch 3113: train_loss 0.021427833 val_loss 0.025581451
ttest: -14.41904224484526 pValue 1.58701394765372e-43
Epoch 3114: train_loss 0.021426968 val_loss 0.025581606
ttest: -14.326736462397289 pValue 4.8768899654683716e-43
Epoch 3115: train_loss 0.021426102 val_loss 0.025581745
ttest: -14.233956366791867 pValue 1.5004826937481296e-42
Epoch 3116: train_loss 0.021425236 val_loss 0.025581896
ttest: -14.141080612796717 pValue 4.600391172625933e-42
Epoch 3117: train_loss 0.02142437 val_loss 0.025582045
ttest: -14.04774301801146 pValue 1.4116733712720155e-41
Epoch 3118: train_loss 0.021423507 val_loss 0.025582192
ttest: -13.9532010077051 pValue 4.374054613201378e-41
Epoch 3119: train_loss 0.021422641 val_loss 0.025582341
ttest: -13.858585562545128 pValue 1.3497356353132328e-40
Epoch 3120: train_loss 0.021421777 val_loss 0.025582492
ttest: -13.763525693452516 pValue 4.166071353558577e-40
Epoch 3121: train_loss 0.021420913 val_loss 0.025582636
ttest: -13.667270521262413 pValue 1.29759778084039e-39
Epoch 3122: train_loss 0.021420049 val_loss 0.02558278
ttest: -13.571343585449695 pValue 4.0045927490621335e-39
Epoch 3123: train_loss 0.021419186 val_loss 0.025582923
ttest: -13.474610445129013 pValue 1.2410274499891202e-38
Epoch 3124: train_loss 0.02141832 val_loss 0.025583073
ttest: -13.37591882722805 pValue 3.9136640998101945e-38
Epoch 3125: train_loss 0.021417458 val_loss 0.025583223
ttest: -13.277962687086832 pValue 1.2166592585677086e-37
Epoch 3126: train_loss 0.021416595 val_loss 0.025583364
ttest: -13.179985836668724 pValue 3.761649772036811e-37
Epoch 3127: train_loss 0.021415731 val_loss 0.025583519
ttest: -13.080834858814816 pValue 1.1720851241536392e-36
Epoch 3128: train_loss 0.021414867 val_loss 0.025583658
ttest: -12.98089722935288 pValue 3.663254282021114e-36
Epoch 3129: train_loss 0.021414002 val_loss 0.025583806
ttest: -12.88057093059898 pValue 1.1429979160989152e-35
Epoch 3130: train_loss 0.021413142 val_loss 0.025583953
ttest: -12.77985859853591 pValue 3.5598146694085953e-35
Epoch 3131: train_loss 0.02141228 val_loss 0.025584107
ttest: -12.678768626111214 pValue 1.1064088022494554e-34
Epoch 3132: train_loss 0.021411417 val_loss 0.025584243
ttest: -12.576915523940716 pValue 3.4461209367512184e-34
Epoch 3133: train_loss 0.021410555 val_loss 0.0255844
ttest: -12.473897945520836 pValue 1.0802456578115313e-33
Epoch 3134: train_loss 0.021409694 val_loss 0.025584534
ttest: -12.37131741226523 pValue 3.347124459752188e-33
Epoch 3135: train_loss 0.021408834 val_loss 0.025584685
ttest: -12.267987177608632 pValue 1.0386253448570404e-32
Epoch 3136: train_loss 0.021407973 val_loss 0.025584826
ttest: -12.1643094977626 pValue 3.2128075831070663e-32
Epoch 3137: train_loss 0.02140711 val_loss 0.025584972
ttest: -12.059485941504587 pValue 9.992097090459395e-32
Epoch 3138: train_loss 0.021406246 val_loss 0.025585117
ttest: -11.95392161647786 pValue 3.109905149779763e-31
Epoch 3139: train_loss 0.021405386 val_loss 0.025585271
ttest: -11.848436172948801 pValue 9.599511577922042e-31
Epoch 3140: train_loss 0.021404525 val_loss 0.025585419
ttest: -11.741399706781825 pValue 2.9900193790559486e-30
Epoch 3141: train_loss 0.02140367 val_loss 0.025585555
ttest: -11.635277118157894 pValue 9.152533905984997e-30
Epoch 3142: train_loss 0.021402806 val_loss 0.025585698
ttest: -11.528852290173315 pValue 2.7890250393544166e-29
Epoch 3143: train_loss 0.021401946 val_loss 0.025585841
ttest: -11.420058113334317 pValue 8.64367564836232e-29
Epoch 3144: train_loss 0.021401087 val_loss 0.025585981
ttest: -11.311794011709859 pValue 2.6424407715357078e-28
Epoch 3145: train_loss 0.021400226 val_loss 0.025586132
ttest: -11.203241587251926 pValue 8.036083278922195e-28
Epoch 3146: train_loss 0.021399364 val_loss 0.025586277
ttest: -11.093150549471074 pValue 2.4619911177038468e-27
Epoch 3147: train_loss 0.021398507 val_loss 0.025586423
ttest: -10.983198585022635 pValue 7.467737267365761e-27
Epoch 3148: train_loss 0.021397647 val_loss 0.025586562
ttest: -10.872978567266944 pValue 2.2516967709968245e-26
Epoch 3149: train_loss 0.02139679 val_loss 0.025586708
ttest: -10.762073912937597 pValue 6.776283854770384e-26
Epoch 3150: train_loss 0.021395933 val_loss 0.025586855
ttest: -10.650062134885019 pValue 2.043343688611249e-25
Epoch 3151: train_loss 0.02139507 val_loss 0.025587
ttest: -10.538221471538424 pValue 6.095222594460152e-25
Epoch 3152: train_loss 0.021394214 val_loss 0.025587142
ttest: -10.426139901853826 pValue 1.8057587032788546e-24
Epoch 3153: train_loss 0.021393359 val_loss 0.025587283
ttest: -10.312962907693493 pValue 5.3562411065336355e-24
Epoch 3154: train_loss 0.021392494 val_loss 0.025587419
ttest: -10.199123947815558 pValue 1.5836176143825185e-23
Epoch 3155: train_loss 0.021391641 val_loss 0.025587559
ttest: -10.085925778535799 pValue 4.6088719076756137e-23
Epoch 3156: train_loss 0.021390783 val_loss 0.025587708
ttest: -9.970775827715627 pValue 1.3528567573553585e-22
Epoch 3157: train_loss 0.021389926 val_loss 0.025587847
ttest: -9.855410103760283 pValue 3.939092797028774e-22
Epoch 3158: train_loss 0.021389069 val_loss 0.025587989
ttest: -9.739838567501144 pValue 1.1374551444845846e-21
Epoch 3159: train_loss 0.02138821 val_loss 0.025588125
ttest: -9.623630688645878 pValue 3.2697066121193707e-21
Epoch 3160: train_loss 0.021387355 val_loss 0.02558827
ttest: -9.506346586935017 pValue 9.391400662327116e-21
Epoch 3161: train_loss 0.021386497 val_loss 0.025588412
ttest: -9.39064360241515 pValue 2.6314176763960465e-20
Epoch 3162: train_loss 0.02138564 val_loss 0.025588555
ttest: -9.273441206536782 pValue 7.392802551538574e-20
Epoch 3163: train_loss 0.021384787 val_loss 0.025588695
ttest: -9.154731557037554 pValue 2.0816493404443398e-19
Epoch 3164: train_loss 0.02138393 val_loss 0.025588835
ttest: -9.035852857013557 pValue 5.804596836863861e-19
Epoch 3165: train_loss 0.021383075 val_loss 0.025588976
ttest: -8.916815013859166 pValue 1.602566975381286e-18
Epoch 3166: train_loss 0.021382218 val_loss 0.025589114
ttest: -8.797174768845247 pValue 4.396525508009022e-18
Epoch 3167: train_loss 0.021381363 val_loss 0.025589257
ttest: -8.676936889831744 pValue 1.1982019582709972e-17
Epoch 3168: train_loss 0.021380508 val_loss 0.025589395
ttest: -8.556106433549708 pValue 3.2430560633134674e-17
Epoch 3169: train_loss 0.021379652 val_loss 0.025589537
ttest: -8.435141600967423 pValue 8.682893713133402e-17
Epoch 3170: train_loss 0.0213788 val_loss 0.025589682
ttest: -8.313594147837227 pValue 2.3076663545270806e-16
Epoch 3171: train_loss 0.021377945 val_loss 0.025589824
ttest: -8.191468637998645 pValue 6.086390563099023e-16
Epoch 3172: train_loss 0.021377089 val_loss 0.025589952
ttest: -8.069230316123766 pValue 1.5868795544101698e-15
Epoch 3173: train_loss 0.021376234 val_loss 0.025590094
ttest: -7.945964369837541 pValue 4.1185201759559666e-15
Epoch 3174: train_loss 0.02137538 val_loss 0.02559024
ttest: -7.823059913791648 pValue 1.0524565469534391e-14
Epoch 3175: train_loss 0.021374527 val_loss 0.025590375
ttest: -7.699136647728465 pValue 2.6757411995294044e-14
Epoch 3176: train_loss 0.021373672 val_loss 0.025590518
ttest: -7.574660667769243 pValue 6.7421748657259e-14
Epoch 3177: train_loss 0.02137282 val_loss 0.025590658
ttest: -7.449169437659571 pValue 1.6890000751251881e-13
Epoch 3178: train_loss 0.021371966 val_loss 0.02559079
ttest: -7.323599428667845 pValue 4.1768610627141535e-13
Epoch 3179: train_loss 0.021371117 val_loss 0.025590936
ttest: -7.197961778421492 pValue 1.0195056644505384e-12
Epoch 3180: train_loss 0.021370264 val_loss 0.025591081
ttest: -7.07179209033519 pValue 2.463851802340377e-12
Epoch 3181: train_loss 0.021369409 val_loss 0.025591206
ttest: -6.944620225305017 pValue 5.913158965423896e-12
Epoch 3182: train_loss 0.021368556 val_loss 0.025591351
ttest: -6.817874234682231 pValue 1.3952894555998061e-11
Epoch 3183: train_loss 0.021367704 val_loss 0.02559148
ttest: -6.689179083238041 pValue 3.288570078097482e-11
Epoch 3184: train_loss 0.021366851 val_loss 0.025591621
ttest: -6.5614021918979475 pValue 7.593549612429774e-11
Epoch 3185: train_loss 0.021365998 val_loss 0.025591763
ttest: -6.432637075035427 pValue 1.739307692224927e-10
Epoch 3186: train_loss 0.02136515 val_loss 0.025591904
ttest: -6.303366606259458 pValue 3.938378559050317e-10
Epoch 3187: train_loss 0.021364296 val_loss 0.025592035
ttest: -6.173595159254908 pValue 8.813705008308519e-10
Epoch 3188: train_loss 0.021363446 val_loss 0.025592173
ttest: -6.043813650245833 pValue 1.9431897496994023e-09
Epoch 3189: train_loss 0.021362592 val_loss 0.025592308
ttest: -5.9130558415313175 pValue 4.244730774901879e-09
Epoch 3190: train_loss 0.021361746 val_loss 0.025592446
ttest: -5.782298687848586 pValue 9.13128745529293e-09
Epoch 3191: train_loss 0.021360893 val_loss 0.025592584
ttest: -5.652042403785279 pValue 1.9288865622564503e-08
Epoch 3192: train_loss 0.021360042 val_loss 0.025592716
ttest: -5.5188592655542275 pValue 4.078682022489852e-08
Epoch 3193: train_loss 0.021359194 val_loss 0.02559285
ttest: -5.386676486852856 pValue 8.441310010925159e-08
Epoch 3194: train_loss 0.021358343 val_loss 0.025592988
ttest: -5.254522939358766 pValue 1.719343161972387e-07
Epoch 3195: train_loss 0.021357493 val_loss 0.025593128
ttest: -5.120917189059227 pValue 3.473088263994317e-07
Epoch 3196: train_loss 0.021356644 val_loss 0.025593264
ttest: -4.986851806434325 pValue 6.918863813359548e-07
Epoch 3197: train_loss 0.021355791 val_loss 0.025593396
ttest: -4.8533299663852 pValue 1.35229787992676e-06
Epoch 3198: train_loss 0.021354942 val_loss 0.025593538
ttest: -4.717358003232667 pValue 2.631428740612047e-06
Epoch 3199: train_loss 0.021354094 val_loss 0.025593666
ttest: -4.582943985289963 pValue 4.997874129615965e-06
Epoch 3200: train_loss 0.021353249 val_loss 0.025593802
ttest: -4.4475905066983215 pValue 9.377175553968376e-06
Epoch 3201: train_loss 0.021352397 val_loss 0.025593938
ttest: -4.311801989529908 pValue 1.7334635629326873e-05
Epoch 3202: train_loss 0.021351548 val_loss 0.025594074
ttest: -4.175075849794725 pValue 3.163617192119106e-05
Epoch 3203: train_loss 0.021350702 val_loss 0.025594212
ttest: -4.038939505301459 pValue 5.6614547957842373e-05
Epoch 3204: train_loss 0.021349853 val_loss 0.025594348
ttest: -3.9013634318473356 pValue 0.0001001945916608195
Epoch 3205: train_loss 0.021349004 val_loss 0.025594478
ttest: -3.7638799663487617 pValue 0.0001742049909659029
Epoch 3206: train_loss 0.021348156 val_loss 0.025594613
ttest: -3.625984474153785 pValue 0.000298150203266918
Epoch 3207: train_loss 0.021347309 val_loss 0.025594756
ttest: -3.487680773687144 pValue 0.0005022256044727853
Epoch 3208: train_loss 0.021346461 val_loss 0.02559488
ttest: -3.348456913107724 pValue 0.0008340426486354606
Epoch 3209: train_loss 0.021345614 val_loss 0.02559502
ttest: -3.208830684405197 pValue 0.0013626527968247815
Epoch 3210: train_loss 0.021344766 val_loss 0.025595147
ttest: -3.069328114697505 pValue 0.0021861377086767977
Epoch 3211: train_loss 0.021343919 val_loss 0.025595289
ttest: -2.929434969279066 pValue 0.0034499883556067573
Epoch 3212: train_loss 0.021343071 val_loss 0.025595421
ttest: -2.78810995709141 pValue 0.005372184790890528
Epoch 3213: train_loss 0.021342227 val_loss 0.02559555
ttest: -2.6469226584591596 pValue 0.008212206678476138
Epoch 3214: train_loss 0.021341382 val_loss 0.02559568
ttest: -2.505882131968184 pValue 0.01232501182630092
Epoch 3215: train_loss 0.021340536 val_loss 0.025595814
ttest: -2.3628849781880477 pValue 0.018266254996550917
Epoch 3216: train_loss 0.021339688 val_loss 0.02559594
ttest: -2.221100320973372 pValue 0.026499657594652854
Epoch 3217: train_loss 0.021338843 val_loss 0.025596082
ttest: -2.0784210811822796 pValue 0.03784743741957008
Epoch 3218: train_loss 0.021337997 val_loss 0.025596213
ttest: -1.9348464506537555 pValue 0.053204477452516157
Epoch 3219: train_loss 0.021337153 val_loss 0.025596347
ttest: -1.7909108690684963 pValue 0.07351678481337064
Epoch 3220: train_loss 0.021336308 val_loss 0.025596473
ttest: -1.6476915030612589 pValue 0.09963286555887059
Epoch 3221: train_loss 0.021335462 val_loss 0.025596606
ttest: -1.502512685938947 pValue 0.13318223349489938
#################################################################
Target Domain Transfer
Test RMSE: 10.08928
Using TensorFlow backend.
2022-07-22 14:12:41.415939: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2
2022-07-22 14:12:41.579762: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties:
name: GeForce RTX 2080 Ti major: 7 minor: 5 memoryClockRate(GHz): 1.755
pciBusID: 0000:2e:00.0
totalMemory: 11.00GiB freeMemory: 9.90GiB
2022-07-22 14:12:41.580067: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0
2022-07-22 14:12:41.851040: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:
2022-07-22 14:12:41.851242: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0
2022-07-22 14:12:41.852163: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N
2022-07-22 14:12:41.852414: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9542 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:2e:00.0, compute capability: 7.5)
WARNING:tensorflow:From C:\Users\USER\.conda\envs\POI\lib\site-packages\tensorflow\python\ops\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
WARNING:tensorflow:From C:\Users\USER\.conda\envs\POI\lib\site-packages\tensorflow\python\ops\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.cast instead.
2022-07-22 14:12:42.877232: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0
2022-07-22 14:12:42.877393: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:
2022-07-22 14:12:42.878386: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0
2022-07-22 14:12:42.878841: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N
2022-07-22 14:12:42.879366: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9542 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:2e:00.0, compute capability: 7.5)
#################################################################
Source Domain information
DiDi Chengdu
source closeness shape: (1272, 256, 24, 1)
source test_y shape: (144, 256, 1)
Number of trainable variables 61921
Number of training samples 1272
#################################################################
Target Domain information
DiDi Xian
target closeness shape: (72, 253, 24, 1)
target test_y shape: (144, 253, 1)
Number of trainable variables 61921
Number of training samples 72
pretrain model not found. start training...
WARNING:tensorflow:From C:\Users\USER\.conda\envs\POI\lib\site-packages\tensorflow\python\training\saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
Found model in disk
Model converged, stop training
Not loading model from disk
Running Operation ('train_op',)
2022-07-22 14:12:44.734690: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library cublas64_100.dll locally
Epoch 0: train_loss 0.04360234 val_loss 0.039359782
Epoch 1: train_loss 0.043569826 val_loss 0.039283503
Epoch 2: train_loss 0.04350839 val_loss 0.03920101
Epoch 3: train_loss 0.043425847 val_loss 0.039121814
Epoch 4: train_loss 0.043330647 val_loss 0.039054874
Epoch 5: train_loss 0.0432307 val_loss 0.039007373
WARNING:tensorflow:From C:\Users\USER\.conda\envs\POI\lib\site-packages\tensorflow\python\training\saver.py:966: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to delete files with this prefix.
Epoch 6: train_loss 0.04313247 val_loss 0.03898396
Epoch 7: train_loss 0.04304037 val_loss 0.03898641
Epoch 8: train_loss 0.042956658 val_loss 0.039013896
Epoch 9: train_loss 0.042881668 val_loss 0.039063472
Epoch 10: train_loss 0.04281431 val_loss 0.03913097
Epoch 11: train_loss 0.04275277 val_loss 0.03921174
Epoch 12: train_loss 0.042695146 val_loss 0.039301347
Epoch 13: train_loss 0.04263998 val_loss 0.03939603
Epoch 14: train_loss 0.042586535 val_loss 0.039492846
Epoch 15: train_loss 0.042534843 val_loss 0.039589617
Epoch 16: train_loss 0.04248553 val_loss 0.039684694
Epoch 17: train_loss 0.042439543 val_loss 0.03977669
Epoch 18: train_loss 0.042397816 val_loss 0.03986423
Epoch 19: train_loss 0.042360973 val_loss 0.039945807
Epoch 20: train_loss 0.04232916 val_loss 0.040019695
Epoch 21: train_loss 0.042301945 val_loss 0.040084068
Epoch 22: train_loss 0.042278428 val_loss 0.04013712
Epoch 23: train_loss 0.042257342 val_loss 0.04017726
Epoch 24: train_loss 0.04223732 val_loss 0.04020335
Epoch 25: train_loss 0.042217072 val_loss 0.04021479
Epoch 26: train_loss 0.04219558 val_loss 0.04021159
Epoch 27: train_loss 0.04217219 val_loss 0.040194392
Epoch 28: train_loss 0.04214665 val_loss 0.04016435
Epoch 29: train_loss 0.04211905 val_loss 0.040122997
Epoch 30: train_loss 0.04208975 val_loss 0.04007207
Epoch 31: train_loss 0.04205922 val_loss 0.04001337
Epoch 32: train_loss 0.04202797 val_loss 0.03994862
Epoch 33: train_loss 0.04199641 val_loss 0.039879408
Epoch 34: train_loss 0.041964836 val_loss 0.039807107
Epoch 35: train_loss 0.041933395 val_loss 0.039732855
Epoch 36: train_loss 0.041902114 val_loss 0.039657626
Epoch 37: train_loss 0.041870926 val_loss 0.03958221
Epoch 38: train_loss 0.04183975 val_loss 0.039507296
Epoch 39: train_loss 0.04180851 val_loss 0.03943345
Epoch 40: train_loss 0.041777175 val_loss 0.039361168
Epoch 41: train_loss 0.041745774 val_loss 0.039290838
Epoch 42: train_loss 0.041714396 val_loss 0.039222803
Epoch 43: train_loss 0.04168316 val_loss 0.03915724
Epoch 44: train_loss 0.041652188 val_loss 0.039094266
Epoch 45: train_loss 0.04162158 val_loss 0.039033845
Epoch 46: train_loss 0.041591406 val_loss 0.038975883
Epoch 47: train_loss 0.041561674 val_loss 0.038920194
Epoch 48: train_loss 0.04153236 val_loss 0.038866557
Epoch 49: train_loss 0.041503385 val_loss 0.038814727
Epoch 50: train_loss 0.041474663 val_loss 0.038764488
Epoch 51: train_loss 0.04144609 val_loss 0.038715694
Epoch 52: train_loss 0.041417584 val_loss 0.0386682
Epoch 53: train_loss 0.04138908 val_loss 0.038622
Epoch 54: train_loss 0.04136054 val_loss 0.03857709
Epoch 55: train_loss 0.041331954 val_loss 0.03853355
Epoch 56: train_loss 0.04130333 val_loss 0.038491473
Epoch 57: train_loss 0.041274685 val_loss 0.038450953
Epoch 58: train_loss 0.041246045 val_loss 0.038412083
Epoch 59: train_loss 0.041217424 val_loss 0.038374934
Epoch 60: train_loss 0.041188836 val_loss 0.038339525
Epoch 61: train_loss 0.041160285 val_loss 0.03830587
Epoch 62: train_loss 0.041131765 val_loss 0.038273912
Epoch 63: train_loss 0.041103266 val_loss 0.03824357
Epoch 64: train_loss 0.04107479 val_loss 0.038214717
Epoch 65: train_loss 0.04104632 val_loss 0.038187206
Epoch 66: train_loss 0.041017856 val_loss 0.038160875
Epoch 67: train_loss 0.040989414 val_loss 0.038135514
Epoch 68: train_loss 0.04096099 val_loss 0.03811094
Epoch 69: train_loss 0.040932603 val_loss 0.038086936
Epoch 70: train_loss 0.04090426 val_loss 0.0380633
Epoch 71: train_loss 0.04087598 val_loss 0.038039822
Epoch 72: train_loss 0.040847756 val_loss 0.038016323
Epoch 73: train_loss 0.040819596 val_loss 0.037992626
Epoch 74: train_loss 0.0407915 val_loss 0.03796859
Epoch 75: train_loss 0.040763468 val_loss 0.0379441
Epoch 76: train_loss 0.040735487 val_loss 0.037919067
Epoch 77: train_loss 0.040707547 val_loss 0.037893444
Epoch 78: train_loss 0.04067965 val_loss 0.03786721
Epoch 79: train_loss 0.04065179 val_loss 0.03784037
Epoch 80: train_loss 0.040623963 val_loss 0.03781296
Epoch 81: train_loss 0.04059617 val_loss 0.03778503
Epoch 82: train_loss 0.04056841 val_loss 0.037756655
Epoch 83: train_loss 0.040540684 val_loss 0.0377279
Epoch 84: train_loss 0.04051299 val_loss 0.037698846
Epoch 85: train_loss 0.040485334 val_loss 0.03766958
Epoch 86: train_loss 0.040457714 val_loss 0.03764016
Epoch 87: train_loss 0.04043012 val_loss 0.037610676
Epoch 88: train_loss 0.04040256 val_loss 0.037581168
Epoch 89: train_loss 0.04037503 val_loss 0.037551697
Epoch 90: train_loss 0.040347535 val_loss 0.037522294
Epoch 91: train_loss 0.04032006 val_loss 0.03749299
Epoch 92: train_loss 0.040292617 val_loss 0.03746379
Epoch 93: train_loss 0.040265203 val_loss 0.03743472
Epoch 94: train_loss 0.040237814 val_loss 0.037405767
Epoch 95: train_loss 0.040210456 val_loss 0.037376948
Epoch 96: train_loss 0.04018313 val_loss 0.03734824
Epoch 97: train_loss 0.040155835 val_loss 0.037319638
Epoch 98: train_loss 0.040128566 val_loss 0.037291147
Epoch 99: train_loss 0.04010133 val_loss 0.037262764
Epoch 100: train_loss 0.040074125 val_loss 0.037234485
Epoch 101: train_loss 0.040046953 val_loss 0.03720632
Epoch 102: train_loss 0.040019803 val_loss 0.037178263
Epoch 103: train_loss 0.039992683 val_loss 0.03715035
Epoch 104: train_loss 0.039965592 val_loss 0.03712258
Epoch 105: train_loss 0.039938528 val_loss 0.03709497
Epoch 106: train_loss 0.039911486 val_loss 0.037067544
Epoch 107: train_loss 0.039884474 val_loss 0.037040327
Epoch 108: train_loss 0.039857488 val_loss 0.037013337
Epoch 109: train_loss 0.039830524 val_loss 0.036986586
Epoch 110: train_loss 0.039803587 val_loss 0.03696009
Epoch 111: train_loss 0.03977668 val_loss 0.036933858
Epoch 112: train_loss 0.039749794 val_loss 0.03690789
Epoch 113: train_loss 0.03972293 val_loss 0.03688219
Epoch 114: train_loss 0.039696094 val_loss 0.03685675
Epoch 115: train_loss 0.039669283 val_loss 0.03683156
Epoch 116: train_loss 0.03964249 val_loss 0.03680662
Epoch 117: train_loss 0.039615724 val_loss 0.0367819
Epoch 118: train_loss 0.039588977 val_loss 0.036757387
Epoch 119: train_loss 0.039562255 val_loss 0.036733065
Epoch 120: train_loss 0.039535552 val_loss 0.036708914
Epoch 121: train_loss 0.039508875 val_loss 0.03668491
Epoch 122: train_loss 0.03948222 val_loss 0.036661044
Epoch 123: train_loss 0.03945558 val_loss 0.03663729
Epoch 124: train_loss 0.03942897 val_loss 0.036613636
Epoch 125: train_loss 0.039402377 val_loss 0.036590073
Epoch 126: train_loss 0.039375804 val_loss 0.0365666
Epoch 127: train_loss 0.03934925 val_loss 0.036543198
Epoch 128: train_loss 0.03932272 val_loss 0.036519885
Epoch 129: train_loss 0.039296206 val_loss 0.036496636
Epoch 130: train_loss 0.039269716 val_loss 0.036473464
Epoch 131: train_loss 0.03924324 val_loss 0.036450375
Epoch 132: train_loss 0.039216787 val_loss 0.036427375
Epoch 133: train_loss 0.039190352 val_loss 0.03640445
Epoch 134: train_loss 0.03916394 val_loss 0.03638162
Epoch 135: train_loss 0.039137542 val_loss 0.03635889
Epoch 136: train_loss 0.039111163 val_loss 0.036336247
Epoch 137: train_loss 0.039084803 val_loss 0.036313698
Epoch 138: train_loss 0.039058458 val_loss 0.036291253
Epoch 139: train_loss 0.039032135 val_loss 0.036268905
Epoch 140: train_loss 0.039005827 val_loss 0.036246665
Epoch 141: train_loss 0.038979534 val_loss 0.036224518
Epoch 142: train_loss 0.038953263 val_loss 0.036202464
Epoch 143: train_loss 0.038927004 val_loss 0.036180504
Epoch 144: train_loss 0.038900763 val_loss 0.036158644
Epoch 145: train_loss 0.038874537 val_loss 0.036136873
Epoch 146: train_loss 0.038848333 val_loss 0.03611518
Epoch 147: train_loss 0.038822137 val_loss 0.036093585
Epoch 148: train_loss 0.03879596 val_loss 0.036072064
Epoch 149: train_loss 0.0387698 val_loss 0.03605064
Epoch 150: train_loss 0.038743652 val_loss 0.036029305
Epoch 151: train_loss 0.03871752 val_loss 0.036008056
Epoch 152: train_loss 0.038691405 val_loss 0.035986897
Epoch 153: train_loss 0.038665306 val_loss 0.03596583
Epoch 154: train_loss 0.038639218 val_loss 0.035944853
Epoch 155: train_loss 0.038613144 val_loss 0.035923976
Epoch 156: train_loss 0.03858709 val_loss 0.0359032
Epoch 157: train_loss 0.038561046 val_loss 0.03588253
Epoch 158: train_loss 0.038535018 val_loss 0.035861954
Epoch 159: train_loss 0.038509 val_loss 0.03584148
Epoch 160: train_loss 0.038482998 val_loss 0.035821117
Epoch 161: train_loss 0.038457006 val_loss 0.035800863
Epoch 162: train_loss 0.03843103 val_loss 0.035780706
Epoch 163: train_loss 0.03840507 val_loss 0.03576065
Epoch 164: train_loss 0.038379118 val_loss 0.035740696
Epoch 165: train_loss 0.03835318 val_loss 0.035720836
Epoch 166: train_loss 0.038327254 val_loss 0.03570107
Epoch 167: train_loss 0.03830134 val_loss 0.035681404
Epoch 168: train_loss 0.038275443 val_loss 0.035661828
Epoch 169: train_loss 0.038249552 val_loss 0.03564234
Epoch 170: train_loss 0.038223673 val_loss 0.03562294
Epoch 171: train_loss 0.03819781 val_loss 0.03560363
Epoch 172: train_loss 0.038171954 val_loss 0.035584405
Epoch 173: train_loss 0.038146112 val_loss 0.035565265
Epoch 174: train_loss 0.038120285 val_loss 0.03554621
Epoch 175: train_loss 0.038094465 val_loss 0.03552724
Epoch 176: train_loss 0.03806866 val_loss 0.035508346
Epoch 177: train_loss 0.038042862 val_loss 0.03548953
Epoch 178: train_loss 0.038017076 val_loss 0.035470795
Epoch 179: train_loss 0.0379913 val_loss 0.035452142
Epoch 180: train_loss 0.037965536 val_loss 0.035433576
Epoch 181: train_loss 0.03793978 val_loss 0.035415083
Epoch 182: train_loss 0.037914038 val_loss 0.03539668
Epoch 183: train_loss 0.037888303 val_loss 0.03537834
Epoch 184: train_loss 0.037862577 val_loss 0.035360094
Epoch 185: train_loss 0.03783687 val_loss 0.03534192
Epoch 186: train_loss 0.03781116 val_loss 0.035323832
Epoch 187: train_loss 0.037785467 val_loss 0.03530581
Epoch 188: train_loss 0.037759785 val_loss 0.03528787
Epoch 189: train_loss 0.03773411 val_loss 0.03527001
Epoch 190: train_loss 0.037708443 val_loss 0.035252217
Epoch 191: train_loss 0.037682787 val_loss 0.035234496
Epoch 192: train_loss 0.037657145 val_loss 0.03521685
Epoch 193: train_loss 0.037631504 val_loss 0.03519928
Epoch 194: train_loss 0.037605878 val_loss 0.03518178
Epoch 195: train_loss 0.037580255 val_loss 0.03516435
Epoch 196: train_loss 0.03755465 val_loss 0.03514699
Epoch 197: train_loss 0.037529048 val_loss 0.035129704
Epoch 198: train_loss 0.03750346 val_loss 0.03511248
Epoch 199: train_loss 0.037477873 val_loss 0.03509533
Epoch 200: train_loss 0.0374523 val_loss 0.03507826
Epoch 201: train_loss 0.037426732 val_loss 0.035061248
Epoch 202: train_loss 0.037401173 val_loss 0.035044316
Epoch 203: train_loss 0.03737562 val_loss 0.035027448
Epoch 204: train_loss 0.03735008 val_loss 0.03501066
Epoch 205: train_loss 0.037324548 val_loss 0.034993935
Epoch 206: train_loss 0.037299026 val_loss 0.034977276
Epoch 207: train_loss 0.037273508 val_loss 0.03496069
Epoch 208: train_loss 0.037248 val_loss 0.034944184
Epoch 209: train_loss 0.037222497 val_loss 0.034927737
Epoch 210: train_loss 0.037197005 val_loss 0.034911357
Epoch 211: train_loss 0.037171524 val_loss 0.03489505
Epoch 212: train_loss 0.037146047 val_loss 0.0348788
Epoch 213: train_loss 0.037120577 val_loss 0.034862623
Epoch 214: train_loss 0.03709511 val_loss 0.034846507
Epoch 215: train_loss 0.03706966 val_loss 0.03483046
Epoch 216: train_loss 0.037044212 val_loss 0.034814477
Epoch 217: train_loss 0.037018772 val_loss 0.034798555
Epoch 218: train_loss 0.03699334 val_loss 0.034782697
Epoch 219: train_loss 0.036967915 val_loss 0.03476691
Epoch 220: train_loss 0.0369425 val_loss 0.034751177
Epoch 221: train_loss 0.03691709 val_loss 0.034735505
Epoch 222: train_loss 0.036891684 val_loss 0.034719907
Epoch 223: train_loss 0.036866292 val_loss 0.034704354
Epoch 224: train_loss 0.036840905 val_loss 0.034688875
Epoch 225: train_loss 0.036815524 val_loss 0.03467345
Epoch 226: train_loss 0.036790147 val_loss 0.034658093
Epoch 227: train_loss 0.036764782 val_loss 0.03464279
Epoch 228: train_loss 0.036739424 val_loss 0.034627557
Epoch 229: train_loss 0.03671407 val_loss 0.034612373
Epoch 230: train_loss 0.036688723 val_loss 0.03459725
Epoch 231: train_loss 0.036663387 val_loss 0.03458219
Epoch 232: train_loss 0.036638055 val_loss 0.034567185
Epoch 233: train_loss 0.03661273 val_loss 0.034552243
Epoch 234: train_loss 0.03658741 val_loss 0.034537356
Epoch 235: train_loss 0.036562096 val_loss 0.034522533
Epoch 236: train_loss 0.036536794 val_loss 0.034507763
Epoch 237: train_loss 0.036511496 val_loss 0.034493048
Epoch 238: train_loss 0.03648621 val_loss 0.034478396
Epoch 239: train_loss 0.036460925 val_loss 0.0344638
Epoch 240: train_loss 0.036435645 val_loss 0.034449253
Epoch 241: train_loss 0.036410376 val_loss 0.034434766
Epoch 242: train_loss 0.03638511 val_loss 0.034420326
Epoch 243: train_loss 0.036359854 val_loss 0.034405947
Epoch 244: train_loss 0.036334608 val_loss 0.03439162
Epoch 245: train_loss 0.03630936 val_loss 0.034377344
Epoch 246: train_loss 0.036284126 val_loss 0.03436313
Epoch 247: train_loss 0.0362589 val_loss 0.03434896
Epoch 248: train_loss 0.036233675 val_loss 0.03433485
Epoch 249: train_loss 0.03620846 val_loss 0.03432079
Epoch 250: train_loss 0.03618325 val_loss 0.034306798
Epoch 251: train_loss 0.036158048 val_loss 0.03429285
Epoch 252: train_loss 0.03613285 val_loss 0.034278955
Epoch 253: train_loss 0.03610766 val_loss 0.034265116
Epoch 254: train_loss 0.03608248 val_loss 0.034251325
Epoch 255: train_loss 0.036057305 val_loss 0.03423759
Epoch 256: train_loss 0.036032137 val_loss 0.0342239
Epoch 257: train_loss 0.036006972 val_loss 0.034210265
Epoch 258: train_loss 0.035981815 val_loss 0.03419668
Epoch 259: train_loss 0.03595667 val_loss 0.034183145
Epoch 260: train_loss 0.035931528 val_loss 0.034169663
Epoch 261: train_loss 0.035906393 val_loss 0.034156226
Epoch 262: train_loss 0.035881266 val_loss 0.034142833
Epoch 263: train_loss 0.035856143 val_loss 0.034129497
Epoch 264: train_loss 0.03583103 val_loss 0.034116212
Epoch 265: train_loss 0.035805922 val_loss 0.034102976
Epoch 266: train_loss 0.035780825 val_loss 0.03408979
Epoch 267: train_loss 0.035755727 val_loss 0.034076642
Epoch 268: train_loss 0.035730645 val_loss 0.03406355
Epoch 269: train_loss 0.035705563 val_loss 0.034050513
Epoch 270: train_loss 0.03568049 val_loss 0.034037516
Epoch 271: train_loss 0.035655428 val_loss 0.034024566
Epoch 272: train_loss 0.03563037 val_loss 0.034011666
Epoch 273: train_loss 0.03560532 val_loss 0.03399882
Epoch 274: train_loss 0.035580274 val_loss 0.033986017
Epoch 275: train_loss 0.03555524 val_loss 0.033973254
Epoch 276: train_loss 0.03553021 val_loss 0.03396055
Epoch 277: train_loss 0.035505187 val_loss 0.033947885
Epoch 278: train_loss 0.03548017 val_loss 0.03393527
Epoch 279: train_loss 0.035455167 val_loss 0.03392269
Epoch 280: train_loss 0.035430167 val_loss 0.033910166
Epoch 281: train_loss 0.03540517 val_loss 0.03389768
Epoch 282: train_loss 0.03538019 val_loss 0.033885237
Epoch 283: train_loss 0.03535521 val_loss 0.033872847
Epoch 284: train_loss 0.03533024 val_loss 0.033860493
Epoch 285: train_loss 0.03530528 val_loss 0.033848196
Epoch 286: train_loss 0.035280325 val_loss 0.03383593
Epoch 287: train_loss 0.035255376 val_loss 0.03382372
Epoch 288: train_loss 0.035230435 val_loss 0.033811558
Epoch 289: train_loss 0.035205506 val_loss 0.033799436
Epoch 290: train_loss 0.03518058 val_loss 0.03378736
Epoch 291: train_loss 0.035155665 val_loss 0.033775322
Epoch 292: train_loss 0.03513076 val_loss 0.033763327
Epoch 293: train_loss 0.035105858 val_loss 0.03375138
Epoch 294: train_loss 0.035080966 val_loss 0.03373947
Epoch 295: train_loss 0.03505608 val_loss 0.0337276
Epoch 296: train_loss 0.03503121 val_loss 0.033715773
Epoch 297: train_loss 0.035006337 val_loss 0.033703987
Epoch 298: train_loss 0.03498148 val_loss 0.033692244
Epoch 299: train_loss 0.03495663 val_loss 0.033680554
Epoch 300: train_loss 0.034931786 val_loss 0.033668894
Epoch 301: train_loss 0.034906957 val_loss 0.03365728
Epoch 302: train_loss 0.034882132 val_loss 0.0336457
Epoch 303: train_loss 0.034857314 val_loss 0.033634175
Epoch 304: train_loss 0.034832507 val_loss 0.033622693
Epoch 305: train_loss 0.034807708 val_loss 0.033611238
Epoch 306: train_loss 0.03478292 val_loss 0.03359984
Epoch 307: train_loss 0.034758143 val_loss 0.03358847
Epoch 308: train_loss 0.03473337 val_loss 0.033577144
Epoch 309: train_loss 0.034708608 val_loss 0.03356586
Epoch 310: train_loss 0.034683853 val_loss 0.033554614
Epoch 311: train_loss 0.03465911 val_loss 0.033543404
Epoch 312: train_loss 0.034634374 val_loss 0.03353224
Epoch 313: train_loss 0.03460965 val_loss 0.03352111
Epoch 314: train_loss 0.034584932 val_loss 0.033510022
Epoch 315: train_loss 0.03456023 val_loss 0.033498973
Epoch 316: train_loss 0.03453553 val_loss 0.033487964
Epoch 317: train_loss 0.034510847 val_loss 0.033476993
Epoch 318: train_loss 0.034486163 val_loss 0.033466067
Epoch 319: train_loss 0.0344615 val_loss 0.03345517
Epoch 320: train_loss 0.03443684 val_loss 0.033444323
Epoch 321: train_loss 0.034412198 val_loss 0.0334335
Epoch 322: train_loss 0.034387562 val_loss 0.03342273
Epoch 323: train_loss 0.034362935 val_loss 0.033411987
Epoch 324: train_loss 0.034338318 val_loss 0.033401284
Epoch 325: train_loss 0.034313712 val_loss 0.03339063
Epoch 326: train_loss 0.03428912 val_loss 0.033379998
Epoch 327: train_loss 0.034264535 val_loss 0.03336941
Epoch 328: train_loss 0.03423996 val_loss 0.03335887
Epoch 329: train_loss 0.034215398 val_loss 0.033348355
Epoch 330: train_loss 0.03419085 val_loss 0.033337884
Epoch 331: train_loss 0.034166306 val_loss 0.033327457
Epoch 332: train_loss 0.03414177 val_loss 0.03331705
Epoch 333: train_loss 0.03411726 val_loss 0.0333067
Epoch 334: train_loss 0.03409275 val_loss 0.033296376
Epoch 335: train_loss 0.034068253 val_loss 0.03328609
Epoch 336: train_loss 0.034043767 val_loss 0.033275846
Epoch 337: train_loss 0.03401929 val_loss 0.03326563
Epoch 338: train_loss 0.03399483 val_loss 0.03325545
Epoch 339: train_loss 0.033970382 val_loss 0.033245303
Epoch 340: train_loss 0.033945944 val_loss 0.033235196
Epoch 341: train_loss 0.033921517 val_loss 0.033225127
Epoch 342: train_loss 0.033897102 val_loss 0.033215087
Epoch 343: train_loss 0.0338727 val_loss 0.03320509
Epoch 344: train_loss 0.033848315 val_loss 0.03319512
Epoch 345: train_loss 0.033823937 val_loss 0.03318519
Epoch 346: train_loss 0.033799574 val_loss 0.033175305
Epoch 347: train_loss 0.03377522 val_loss 0.033165447
Epoch 348: train_loss 0.033750884 val_loss 0.033155628
Epoch 349: train_loss 0.033726558 val_loss 0.03314584
Epoch 350: train_loss 0.033702243 val_loss 0.033136107
Epoch 351: train_loss 0.033677947 val_loss 0.033126395
Epoch 352: train_loss 0.033653658 val_loss 0.03311672
Epoch 353: train_loss 0.033629388 val_loss 0.033107076
Epoch 354: train_loss 0.03360513 val_loss 0.033097472
Epoch 355: train_loss 0.03358088 val_loss 0.033087905
Epoch 356: train_loss 0.03355665 val_loss 0.033078365
Epoch 357: train_loss 0.03353243 val_loss 0.033068854
Epoch 358: train_loss 0.033508226 val_loss 0.033059385
Epoch 359: train_loss 0.033484038 val_loss 0.033049937
Epoch 360: train_loss 0.03345986 val_loss 0.033040535
Epoch 361: train_loss 0.0334357 val_loss 0.03303115
Epoch 362: train_loss 0.033411555 val_loss 0.033021815
Epoch 363: train_loss 0.033387423 val_loss 0.033012513
Epoch 364: train_loss 0.0333633 val_loss 0.033003245
Epoch 365: train_loss 0.0333392 val_loss 0.032994013
Epoch 366: train_loss 0.03331511 val_loss 0.03298481
Epoch 367: train_loss 0.033291038 val_loss 0.032975648
Epoch 368: train_loss 0.033266984 val_loss 0.032966506
Epoch 369: train_loss 0.033242933 val_loss 0.032957412
Epoch 370: train_loss 0.033218913 val_loss 0.032948352
Epoch 371: train_loss 0.033194903 val_loss 0.03293932
Epoch 372: train_loss 0.03317091 val_loss 0.03293032
Epoch 373: train_loss 0.03314693 val_loss 0.032921348
Epoch 374: train_loss 0.033122968 val_loss 0.032912403
Epoch 375: train_loss 0.03309902 val_loss 0.0329035
Epoch 376: train_loss 0.03307509 val_loss 0.032894623
Epoch 377: train_loss 0.033051178 val_loss 0.032885782
Epoch 378: train_loss 0.03302728 val_loss 0.03287697
Epoch 379: train_loss 0.033003397 val_loss 0.032868188
Epoch 380: train_loss 0.032979537 val_loss 0.032859452
Epoch 381: train_loss 0.032955687 val_loss 0.032850735
Epoch 382: train_loss 0.03293186 val_loss 0.03284205
Epoch 383: train_loss 0.03290805 val_loss 0.032833412
Epoch 384: train_loss 0.03288426 val_loss 0.032824803
Epoch 385: train_loss 0.032860477 val_loss 0.032816224
Epoch 386: train_loss 0.032836717 val_loss 0.032807678
Epoch 387: train_loss 0.032812975 val_loss 0.032799166
Epoch 388: train_loss 0.032789253 val_loss 0.032790687
Epoch 389: train_loss 0.03276555 val_loss 0.032782238
Epoch 390: train_loss 0.032741863 val_loss 0.03277382
Epoch 391: train_loss 0.032718193 val_loss 0.03276543
Epoch 392: train_loss 0.03269455 val_loss 0.03275707
Epoch 393: train_loss 0.032670915 val_loss 0.03274874
Epoch 394: train_loss 0.032647304 val_loss 0.03274045
Epoch 395: train_loss 0.032623716 val_loss 0.032732185
Epoch 396: train_loss 0.032600142 val_loss 0.032723945
Epoch 397: train_loss 0.032576587 val_loss 0.03271574
Epoch 398: train_loss 0.032553054 val_loss 0.03270757
Epoch 399: train_loss 0.032529537 val_loss 0.03269943
Epoch 400: train_loss 0.032506045 val_loss 0.032691315
Epoch 401: train_loss 0.03248257 val_loss 0.032683246
Epoch 402: train_loss 0.032459114 val_loss 0.032675203
Epoch 403: train_loss 0.03243568 val_loss 0.032667186
Epoch 404: train_loss 0.03241227 val_loss 0.032659207
Epoch 405: train_loss 0.032388877 val_loss 0.032651253
Epoch 406: train_loss 0.032365505 val_loss 0.032643333
Epoch 407: train_loss 0.032342155 val_loss 0.032635447
Epoch 408: train_loss 0.032318827 val_loss 0.032627594
Epoch 409: train_loss 0.03229552 val_loss 0.03261977
Epoch 410: train_loss 0.032272235 val_loss 0.032611977
Epoch 411: train_loss 0.03224897 val_loss 0.032604214
Epoch 412: train_loss 0.032225728 val_loss 0.032596473
Epoch 413: train_loss 0.03220251 val_loss 0.032588765
Epoch 414: train_loss 0.03217931 val_loss 0.03258109
Epoch 415: train_loss 0.032156136 val_loss 0.03257344
Epoch 416: train_loss 0.032132983 val_loss 0.03256583
Epoch 417: train_loss 0.032109853 val_loss 0.032558233
Epoch 418: train_loss 0.032086745 val_loss 0.032550674
Epoch 419: train_loss 0.032063667 val_loss 0.03254314
Epoch 420: train_loss 0.032040603 val_loss 0.03253564
Epoch 421: train_loss 0.032017566 val_loss 0.032528166
Epoch 422: train_loss 0.03199455 val_loss 0.03252072
Epoch 423: train_loss 0.03197156 val_loss 0.032513313
Epoch 424: train_loss 0.031948596 val_loss 0.032505933
Epoch 425: train_loss 0.031925656 val_loss 0.032498587
Epoch 426: train_loss 0.031902738 val_loss 0.032491274
Epoch 427: train_loss 0.031879842 val_loss 0.032483988
Epoch 428: train_loss 0.031856976 val_loss 0.032476727
Epoch 429: train_loss 0.03183413 val_loss 0.0324695
Epoch 430: train_loss 0.03181131 val_loss 0.03246231
Epoch 431: train_loss 0.031788517 val_loss 0.032455146
Epoch 432: train_loss 0.031765755 val_loss 0.032448
Epoch 433: train_loss 0.03174301 val_loss 0.032440897
Epoch 434: train_loss 0.03172029 val_loss 0.03243381
Epoch 435: train_loss 0.0316976 val_loss 0.032426756
Epoch 436: train_loss 0.031674933 val_loss 0.03241973
Epoch 437: train_loss 0.031652298 val_loss 0.03241273
Epoch 438: train_loss 0.03162968 val_loss 0.032405768
Epoch 439: train_loss 0.031607095 val_loss 0.03239882
Epoch 440: train_loss 0.03158454 val_loss 0.032391917
Epoch 441: train_loss 0.031562008 val_loss 0.032385044
Epoch 442: train_loss 0.031539503 val_loss 0.032378197
Epoch 443: train_loss 0.031517025 val_loss 0.032371383
Epoch 444: train_loss 0.031494576 val_loss 0.0323646
Epoch 445: train_loss 0.031472154 val_loss 0.03235784
Epoch 446: train_loss 0.03144976 val_loss 0.032351106
Epoch 447: train_loss 0.03142739 val_loss 0.032344393
Epoch 448: train_loss 0.031405054 val_loss 0.032337718
Epoch 449: train_loss 0.031382743 val_loss 0.032331068
Epoch 450: train_loss 0.031360466 val_loss 0.032324445
Epoch 451: train_loss 0.03133821 val_loss 0.032317847
Epoch 452: train_loss 0.03131599 val_loss 0.032311272
Epoch 453: train_loss 0.031293795 val_loss 0.032304734
Epoch 454: train_loss 0.03127163 val_loss 0.03229822
Epoch 455: train_loss 0.031249495 val_loss 0.032291736
Epoch 456: train_loss 0.031227391 val_loss 0.032285288
Epoch 457: train_loss 0.031205315 val_loss 0.032278858
Epoch 458: train_loss 0.031183269 val_loss 0.03227246
Epoch 459: train_loss 0.031161256 val_loss 0.03226609
Epoch 460: train_loss 0.03113927 val_loss 0.032259755
Epoch 461: train_loss 0.031117318 val_loss 0.032253444
Epoch 462: train_loss 0.031095395 val_loss 0.032247167
Epoch 463: train_loss 0.031073503 val_loss 0.03224091
Epoch 464: train_loss 0.031051643 val_loss 0.032234687
Epoch 465: train_loss 0.031029815 val_loss 0.032228496
Epoch 466: train_loss 0.031008018 val_loss 0.032222323
Epoch 467: train_loss 0.030986251 val_loss 0.032216188
Epoch 468: train_loss 0.030964518 val_loss 0.032210078
Epoch 469: train_loss 0.030942814 val_loss 0.032203995
Epoch 470: train_loss 0.03092115 val_loss 0.03219793
Epoch 471: train_loss 0.030899514 val_loss 0.032191902
Epoch 472: train_loss 0.030877909 val_loss 0.0321859
Epoch 473: train_loss 0.03085634 val_loss 0.032179926
Epoch 474: train_loss 0.0308348 val_loss 0.032173976
Epoch 475: train_loss 0.030813297 val_loss 0.032168057
Epoch 476: train_loss 0.030791827 val_loss 0.032162167
Epoch 477: train_loss 0.03077039 val_loss 0.032156307
Epoch 478: train_loss 0.030748988 val_loss 0.032150462
Epoch 479: train_loss 0.03072762 val_loss 0.032144662
Epoch 480: train_loss 0.030706285 val_loss 0.03213888
Epoch 481: train_loss 0.030684985 val_loss 0.032133125
Epoch 482: train_loss 0.030663721 val_loss 0.032127403
Epoch 483: train_loss 0.03064249 val_loss 0.032121696
Epoch 484: train_loss 0.030621296 val_loss 0.032116026
Epoch 485: train_loss 0.030600134 val_loss 0.032110382
Epoch 486: train_loss 0.03057901 val_loss 0.032104764
Epoch 487: train_loss 0.030557923 val_loss 0.032099176
Epoch 488: train_loss 0.030536871 val_loss 0.03209361
Epoch 489: train_loss 0.030515855 val_loss 0.03208808
Epoch 490: train_loss 0.030494872 val_loss 0.032082573
Epoch 491: train_loss 0.03047393 val_loss 0.03207709
Epoch 492: train_loss 0.030453024 val_loss 0.032071635
Epoch 493: train_loss 0.030432153 val_loss 0.032066204
Epoch 494: train_loss 0.030411318 val_loss 0.032060802
Epoch 495: train_loss 0.030390522 val_loss 0.032055423
Epoch 496: train_loss 0.030369762 val_loss 0.03205007
Epoch 497: train_loss 0.030349042 val_loss 0.032044746
Epoch 498: train_loss 0.030328358 val_loss 0.03203945
Epoch 499: train_loss 0.03030771 val_loss 0.032034185
Epoch 500: train_loss 0.030287102 val_loss 0.03202894
Epoch 501: train_loss 0.030266535 val_loss 0.03202372
Epoch 502: train_loss 0.030246003 val_loss 0.032018535
Epoch 503: train_loss 0.03022551 val_loss 0.03201337
Epoch 504: train_loss 0.030205056 val_loss 0.032008234
Epoch 505: train_loss 0.030184643 val_loss 0.03200313
Epoch 506: train_loss 0.030164266 val_loss 0.03199805
Epoch 507: train_loss 0.030143932 val_loss 0.03199299
Epoch 508: train_loss 0.030123636 val_loss 0.031987965
Epoch 509: train_loss 0.03010338 val_loss 0.031982955
Epoch 510: train_loss 0.030083163 val_loss 0.031977974
Epoch 511: train_loss 0.030062988 val_loss 0.03197302
Epoch 512: train_loss 0.030042853 val_loss 0.031968098
Epoch 513: train_loss 0.030022755 val_loss 0.0319632
Epoch 514: train_loss 0.0300027 val_loss 0.031958327
Epoch 515: train_loss 0.029982686 val_loss 0.031953476
Epoch 516: train_loss 0.029962713 val_loss 0.03194866
Epoch 517: train_loss 0.02994278 val_loss 0.031943854
Epoch 518: train_loss 0.02992289 val_loss 0.031939086
Epoch 519: train_loss 0.029903043 val_loss 0.031934347
Epoch 520: train_loss 0.029883238 val_loss 0.031929635
Epoch 521: train_loss 0.029863475 val_loss 0.031924937
Epoch 522: train_loss 0.02984375 val_loss 0.031920265
Epoch 523: train_loss 0.029824069 val_loss 0.03191562
Epoch 524: train_loss 0.02980443 val_loss 0.03191099
Epoch 525: train_loss 0.029784838 val_loss 0.031906396
Epoch 526: train_loss 0.029765282 val_loss 0.031901825
Epoch 527: train_loss 0.029745776 val_loss 0.031897284
Epoch 528: train_loss 0.029726308 val_loss 0.03189277
Epoch 529: train_loss 0.029706886 val_loss 0.031888276
Epoch 530: train_loss 0.029687507 val_loss 0.031883802
Epoch 531: train_loss 0.029668167 val_loss 0.031879365
Epoch 532: train_loss 0.029648878 val_loss 0.03187495
Epoch 533: train_loss 0.029629631 val_loss 0.031870563
Epoch 534: train_loss 0.029610427 val_loss 0.031866193
Epoch 535: train_loss 0.029591266 val_loss 0.03186186
Epoch 536: train_loss 0.029572152 val_loss 0.031857535
Epoch 537: train_loss 0.029553082 val_loss 0.031853247
Epoch 538: train_loss 0.029534057 val_loss 0.03184899
Epoch 539: train_loss 0.029515075 val_loss 0.031844743
Epoch 540: train_loss 0.02949614 val_loss 0.031840526
Epoch 541: train_loss 0.029477248 val_loss 0.031836342
Epoch 542: train_loss 0.029458405 val_loss 0.031832185
Epoch 543: train_loss 0.029439606 val_loss 0.031828042
Epoch 544: train_loss 0.029420855 val_loss 0.03182393
Epoch 545: train_loss 0.029402146 val_loss 0.031819843
Epoch 546: train_loss 0.029383484 val_loss 0.031815767
Epoch 547: train_loss 0.02936487 val_loss 0.03181173
Epoch 548: train_loss 0.029346304 val_loss 0.031807717
Epoch 549: train_loss 0.02932778 val_loss 0.031803716
Epoch 550: train_loss 0.029309306 val_loss 0.03179974
Epoch 551: train_loss 0.029290877 val_loss 0.031795796
Epoch 552: train_loss 0.029272497 val_loss 0.031791873
Epoch 553: train_loss 0.029254163 val_loss 0.031787965
Epoch 554: train_loss 0.029235879 val_loss 0.031784095
Epoch 555: train_loss 0.029217638 val_loss 0.03178024
Epoch 556: train_loss 0.02919945 val_loss 0.03177641
Epoch 557: train_loss 0.029181303 val_loss 0.031772614
Epoch 558: train_loss 0.029163208 val_loss 0.03176883
Epoch 559: train_loss 0.02914516 val_loss 0.031765074
Epoch 560: train_loss 0.02912716 val_loss 0.03176134
Epoch 561: train_loss 0.02910921 val_loss 0.031757627
Epoch 562: train_loss 0.029091308 val_loss 0.031753942
Epoch 563: train_loss 0.029073453 val_loss 0.031750284
Epoch 564: train_loss 0.02905565 val_loss 0.03174664
Epoch 565: train_loss 0.029037891 val_loss 0.031743024
Epoch 566: train_loss 0.029020187 val_loss 0.03173943
Epoch 567: train_loss 0.029002527 val_loss 0.031735864
Epoch 568: train_loss 0.028984921 val_loss 0.031732317
Epoch 569: train_loss 0.02896736 val_loss 0.03172879
Epoch 570: train_loss 0.02894985 val_loss 0.031725295
Epoch 571: train_loss 0.028932389 val_loss 0.031721827
Epoch 572: train_loss 0.028914979 val_loss 0.031718384
Epoch 573: train_loss 0.028897619 val_loss 0.03171496
Epoch 574: train_loss 0.028880307 val_loss 0.031711563
Epoch 575: train_loss 0.028863046 val_loss 0.03170819
Epoch 576: train_loss 0.028845837 val_loss 0.03170483
Epoch 577: train_loss 0.028828677 val_loss 0.03170149
Epoch 578: train_loss 0.028811568 val_loss 0.031698175
Epoch 579: train_loss 0.028794512 val_loss 0.03169488
Epoch 580: train_loss 0.028777502 val_loss 0.031691603
Epoch 581: train_loss 0.028760545 val_loss 0.03168835
Epoch 582: train_loss 0.02874364 val_loss 0.031685118
Epoch 583: train_loss 0.028726783 val_loss 0.031681914
Epoch 584: train_loss 0.02870998 val_loss 0.031678732
Epoch 585: train_loss 0.028693225 val_loss 0.03167557
Epoch 586: train_loss 0.028676527 val_loss 0.031672437
Epoch 587: train_loss 0.028659876 val_loss 0.03166932
Epoch 588: train_loss 0.028643278 val_loss 0.031666234
Epoch 589: train_loss 0.028626733 val_loss 0.03166316
Epoch 590: train_loss 0.028610239 val_loss 0.03166011
Epoch 591: train_loss 0.028593795 val_loss 0.031657085
Epoch 592: train_loss 0.028577404 val_loss 0.031654082
Epoch 593: train_loss 0.028561065 val_loss 0.0316511
Epoch 594: train_loss 0.028544778 val_loss 0.031648137
Epoch 595: train_loss 0.028528543 val_loss 0.03164521
Epoch 596: train_loss 0.02851236 val_loss 0.03164229
Epoch 597: train_loss 0.02849623 val_loss 0.031639397
Epoch 598: train_loss 0.028480155 val_loss 0.03163653
Epoch 599: train_loss 0.028464131 val_loss 0.031633675
Epoch 600: train_loss 0.028448157 val_loss 0.031630844
Epoch 601: train_loss 0.028432239 val_loss 0.03162803
Epoch 602: train_loss 0.02841637 val_loss 0.031625245
Epoch 603: train_loss 0.028400555 val_loss 0.031622477
Epoch 604: train_loss 0.028384795 val_loss 0.031619724
Epoch 605: train_loss 0.028369088 val_loss 0.031616997
Epoch 606: train_loss 0.02835343 val_loss 0.031614292
Epoch 607: train_loss 0.02833783 val_loss 0.03161161
Epoch 608: train_loss 0.028322281 val_loss 0.03160893
Epoch 609: train_loss 0.028306786 val_loss 0.031606287
Epoch 610: train_loss 0.02829134 val_loss 0.03160366
Epoch 611: train_loss 0.028275952 val_loss 0.031601053
Epoch 612: train_loss 0.028260615 val_loss 0.031598467
Epoch 613: train_loss 0.028245332 val_loss 0.031595908
Epoch 614: train_loss 0.028230105 val_loss 0.031593356
Epoch 615: train_loss 0.028214928 val_loss 0.031590834
Epoch 616: train_loss 0.028199809 val_loss 0.031588327
Epoch 617: train_loss 0.02818474 val_loss 0.03158585
Epoch 618: train_loss 0.028169721 val_loss 0.03158338
Epoch 619: train_loss 0.028154764 val_loss 0.031580936
Epoch 620: train_loss 0.028139856 val_loss 0.031578507
Epoch 621: train_loss 0.028125005 val_loss 0.031576093
Epoch 622: train_loss 0.028110204 val_loss 0.03157371
Epoch 623: train_loss 0.028095458 val_loss 0.031571332
Epoch 624: train_loss 0.028080767 val_loss 0.031568978
Epoch 625: train_loss 0.02806613 val_loss 0.03156665
Epoch 626: train_loss 0.028051548 val_loss 0.03156434
Epoch 627: train_loss 0.028037017 val_loss 0.031562064
Epoch 628: train_loss 0.028022543 val_loss 0.031559803
Epoch 629: train_loss 0.02800812 val_loss 0.03155756
Epoch 630: train_loss 0.027993755 val_loss 0.031555347
Epoch 631: train_loss 0.02797944 val_loss 0.031553138
Epoch 632: train_loss 0.02796518 val_loss 0.031550955
Epoch 633: train_loss 0.027950976 val_loss 0.03154878
Epoch 634: train_loss 0.027936824 val_loss 0.031546626
Epoch 635: train_loss 0.027922729 val_loss 0.031544484
Epoch 636: train_loss 0.027908685 val_loss 0.03154237
Epoch 637: train_loss 0.027894694 val_loss 0.03154027
Epoch 638: train_loss 0.027880762 val_loss 0.031538196
Epoch 639: train_loss 0.027866883 val_loss 0.03153613
Epoch 640: train_loss 0.027853057 val_loss 0.03153408
Epoch 641: train_loss 0.027839283 val_loss 0.031532053
Epoch 642: train_loss 0.027825564 val_loss 0.03153005
Epoch 643: train_loss 0.027811898 val_loss 0.031528056
Epoch 644: train_loss 0.02779829 val_loss 0.03152609
Epoch 645: train_loss 0.027784733 val_loss 0.031524137
Epoch 646: train_loss 0.027771233 val_loss 0.031522207
Epoch 647: train_loss 0.027757788 val_loss 0.031520292
Epoch 648: train_loss 0.027744392 val_loss 0.031518403
Epoch 649: train_loss 0.027731054 val_loss 0.031516526
Epoch 650: train_loss 0.02771777 val_loss 0.03151466
Epoch 651: train_loss 0.027704537 val_loss 0.031512816
Epoch 652: train_loss 0.02769136 val_loss 0.03151099
Epoch 653: train_loss 0.027678238 val_loss 0.031509176
Epoch 654: train_loss 0.027665168 val_loss 0.031507377
Epoch 655: train_loss 0.027652154 val_loss 0.031505603
Epoch 656: train_loss 0.02763919 val_loss 0.031503845
Epoch 657: train_loss 0.027626282 val_loss 0.031502113
Epoch 658: train_loss 0.02761343 val_loss 0.031500384
Epoch 659: train_loss 0.02760063 val_loss 0.031498685
Epoch 660: train_loss 0.02758788 val_loss 0.031496998
Epoch 661: train_loss 0.02757519 val_loss 0.03149532
Epoch 662: train_loss 0.02756255 val_loss 0.031493656
Epoch 663: train_loss 0.027549963 val_loss 0.031492013
Epoch 664: train_loss 0.02753743 val_loss 0.031490378
Epoch 665: train_loss 0.027524952 val_loss 0.031488765
Epoch 666: train_loss 0.027512526 val_loss 0.031487174
Epoch 667: train_loss 0.027500156 val_loss 0.0314856
Epoch 668: train_loss 0.027487837 val_loss 0.031484034
Epoch 669: train_loss 0.027475573 val_loss 0.03148249
Epoch 670: train_loss 0.02746336 val_loss 0.03148096
Epoch 671: train_loss 0.0274512 val_loss 0.031479448
Epoch 672: train_loss 0.027439095 val_loss 0.03147795
Epoch 673: train_loss 0.02742704 val_loss 0.031476464
Epoch 674: train_loss 0.027415043 val_loss 0.031475
Epoch 675: train_loss 0.027403094 val_loss 0.031473555
Epoch 676: train_loss 0.027391203 val_loss 0.031472124
Epoch 677: train_loss 0.027379358 val_loss 0.0314707
Epoch 678: train_loss 0.027367571 val_loss 0.0314693
Epoch 679: train_loss 0.027355833 val_loss 0.031467922
Epoch 680: train_loss 0.02734415 val_loss 0.03146655
Epoch 681: train_loss 0.02733252 val_loss 0.03146519
Epoch 682: train_loss 0.02732094 val_loss 0.031463854
Epoch 683: train_loss 0.027309414 val_loss 0.031462535
Epoch 684: train_loss 0.027297938 val_loss 0.031461224
Epoch 685: train_loss 0.027286515 val_loss 0.031459935
Epoch 686: train_loss 0.027275147 val_loss 0.031458657
Epoch 687: train_loss 0.027263826 val_loss 0.031457387
Epoch 688: train_loss 0.02725256 val_loss 0.031456135
Epoch 689: train_loss 0.027241345 val_loss 0.031454895
Epoch 690: train_loss 0.027230179 val_loss 0.031453677
Epoch 691: train_loss 0.027219068 val_loss 0.031452473
Epoch 692: train_loss 0.02720801 val_loss 0.031451277
Epoch 693: train_loss 0.027197 val_loss 0.031450097
Epoch 694: train_loss 0.027186044 val_loss 0.031448927
Epoch 695: train_loss 0.027175134 val_loss 0.03144776
Epoch 696: train_loss 0.027164279 val_loss 0.031446613
Epoch 697: train_loss 0.027153473 val_loss 0.031445492
Epoch 698: train_loss 0.02714272 val_loss 0.03144437
Epoch 699: train_loss 0.027132016 val_loss 0.03144327
Epoch 700: train_loss 0.027121363 val_loss 0.03144219
Epoch 701: train_loss 0.02711076 val_loss 0.031441122
Epoch 702: train_loss 0.02710021 val_loss 0.031440075
Epoch 703: train_loss 0.027089706 val_loss 0.031439032
Epoch 704: train_loss 0.027079253 val_loss 0.031438008
Epoch 705: train_loss 0.02706885 val_loss 0.031437
Epoch 706: train_loss 0.027058499 val_loss 0.031436004
Epoch 707: train_loss 0.027048195 val_loss 0.031435013
Epoch 708: train_loss 0.027037943 val_loss 0.03143404
Epoch 709: train_loss 0.027027741 val_loss 0.03143308
Epoch 710: train_loss 0.027017582 val_loss 0.031432122
Epoch 711: train_loss 0.027007481 val_loss 0.031431183
Epoch 712: train_loss 0.026997423 val_loss 0.031430252
Epoch 713: train_loss 0.026987417 val_loss 0.03142935
Epoch 714: train_loss 0.026977459 val_loss 0.031428453
Epoch 715: train_loss 0.02696755 val_loss 0.03142757
Epoch 716: train_loss 0.026957685 val_loss 0.031426698
Epoch 717: train_loss 0.026947875 val_loss 0.03142584
Epoch 718: train_loss 0.02693811 val_loss 0.031424988
Epoch 719: train_loss 0.026928393 val_loss 0.031424154
Epoch 720: train_loss 0.026918726 val_loss 0.03142333
Epoch 721: train_loss 0.026909105 val_loss 0.031422514
Epoch 722: train_loss 0.026899533 val_loss 0.03142171
Epoch 723: train_loss 0.026890006 val_loss 0.031420913
Epoch 724: train_loss 0.026880529 val_loss 0.03142013
Epoch 725: train_loss 0.026871098 val_loss 0.03141936
Epoch 726: train_loss 0.026861714 val_loss 0.03141862
Epoch 727: train_loss 0.026852379 val_loss 0.031417884
Epoch 728: train_loss 0.02684309 val_loss 0.031417154
Epoch 729: train_loss 0.026833847 val_loss 0.031416442
Epoch 730: train_loss 0.026824651 val_loss 0.031415742
Epoch 731: train_loss 0.0268155 val_loss 0.03141505
Epoch 732: train_loss 0.026806394 val_loss 0.03141436
Epoch 733: train_loss 0.026797336 val_loss 0.0314137
Epoch 734: train_loss 0.026788322 val_loss 0.03141305
Epoch 735: train_loss 0.026779355 val_loss 0.031412408
Epoch 736: train_loss 0.026770437 val_loss 0.031411774
Epoch 737: train_loss 0.02676156 val_loss 0.03141114
Epoch 738: train_loss 0.02675273 val_loss 0.031410526
Epoch 739: train_loss 0.026743945 val_loss 0.03140992
Epoch 740: train_loss 0.026735203 val_loss 0.031409312
Epoch 741: train_loss 0.026726507 val_loss 0.03140873
Epoch 742: train_loss 0.026717855 val_loss 0.031408157
Epoch 743: train_loss 0.02670925 val_loss 0.031407595
Epoch 744: train_loss 0.026700685 val_loss 0.03140705
Epoch 745: train_loss 0.026692167 val_loss 0.03140651
Epoch 746: train_loss 0.02668369 val_loss 0.03140598
Epoch 747: train_loss 0.02667526 val_loss 0.03140547
Epoch 748: train_loss 0.02666687 val_loss 0.031404965
Epoch 749: train_loss 0.026658526 val_loss 0.031404465
Epoch 750: train_loss 0.026650226 val_loss 0.03140398
Epoch 751: train_loss 0.026641967 val_loss 0.031403508
Epoch 752: train_loss 0.026633749 val_loss 0.03140303
Epoch 753: train_loss 0.026625577 val_loss 0.03140256
Epoch 754: train_loss 0.026617447 val_loss 0.031402115
Epoch 755: train_loss 0.026609361 val_loss 0.03140167
Epoch 756: train_loss 0.026601315 val_loss 0.031401232
Epoch 757: train_loss 0.026593309 val_loss 0.031400815
Epoch 758: train_loss 0.026585346 val_loss 0.03140041
Epoch 759: train_loss 0.026577424 val_loss 0.03140001
Epoch 760: train_loss 0.026569545 val_loss 0.031399623
Epoch 761: train_loss 0.026561707 val_loss 0.03139924
Epoch 762: train_loss 0.02655391 val_loss 0.03139887
Epoch 763: train_loss 0.026546154 val_loss 0.03139851
Epoch 764: train_loss 0.026538435 val_loss 0.031398166
Epoch 765: train_loss 0.026530761 val_loss 0.031397816
Epoch 766: train_loss 0.026523126 val_loss 0.03139748
Epoch 767: train_loss 0.02651553 val_loss 0.03139715
Epoch 768: train_loss 0.02650798 val_loss 0.031396836
Epoch 769: train_loss 0.026500463 val_loss 0.031396527
Epoch 770: train_loss 0.026492987 val_loss 0.03139622
Epoch 771: train_loss 0.02648555 val_loss 0.03139594
Epoch 772: train_loss 0.026478155 val_loss 0.031395663
Epoch 773: train_loss 0.026470795 val_loss 0.03139539
Epoch 774: train_loss 0.026463479 val_loss 0.031395134
Epoch 775: train_loss 0.026456198 val_loss 0.031394888
Epoch 776: train_loss 0.026448954 val_loss 0.03139464
Epoch 777: train_loss 0.026441751 val_loss 0.031394396
Epoch 778: train_loss 0.026434585 val_loss 0.031394165
Epoch 779: train_loss 0.026427459 val_loss 0.03139394
Epoch 780: train_loss 0.026420368 val_loss 0.031393725
Epoch 781: train_loss 0.026413314 val_loss 0.031393524
Epoch 782: train_loss 0.026406301 val_loss 0.031393327
Epoch 783: train_loss 0.02639932 val_loss 0.031393148
Epoch 784: train_loss 0.026392382 val_loss 0.031392965
Epoch 785: train_loss 0.026385477 val_loss 0.031392787
Epoch 786: train_loss 0.02637861 val_loss 0.031392634
Epoch 787: train_loss 0.026371779 val_loss 0.031392477
Epoch 788: train_loss 0.026364984 val_loss 0.031392336
Epoch 789: train_loss 0.026358223 val_loss 0.0313922
Epoch 790: train_loss 0.026351498 val_loss 0.03139208
Epoch 791: train_loss 0.026344812 val_loss 0.031391952
Epoch 792: train_loss 0.02633816 val_loss 0.03139184
Epoch 793: train_loss 0.026331544 val_loss 0.03139172
Epoch 794: train_loss 0.02632496 val_loss 0.03139162
Epoch 795: train_loss 0.026318416 val_loss 0.031391524
Epoch 796: train_loss 0.026311902 val_loss 0.031391446
Epoch 797: train_loss 0.026305424 val_loss 0.031391364
Epoch 798: train_loss 0.02629898 val_loss 0.03139129
Epoch 799: train_loss 0.02629257 val_loss 0.031391237
Epoch 800: train_loss 0.026286196 val_loss 0.03139118
Epoch 801: train_loss 0.026279852 val_loss 0.031391136
Epoch 802: train_loss 0.026273547 val_loss 0.0313911
Epoch 803: train_loss 0.026267271 val_loss 0.031391066
Epoch 804: train_loss 0.02626103 val_loss 0.031391036
Epoch 805: train_loss 0.026254822 val_loss 0.031391017
Epoch 806: train_loss 0.026248645 val_loss 0.031391006
Epoch 807: train_loss 0.026242502 val_loss 0.031391002
Epoch 808: train_loss 0.026236393 val_loss 0.031391013
Epoch 809: train_loss 0.026230313 val_loss 0.031391025
Epoch 810: train_loss 0.026224265 val_loss 0.031391047
Epoch 811: train_loss 0.02621825 val_loss 0.03139107
Epoch 812: train_loss 0.026212266 val_loss 0.031391095
Epoch 813: train_loss 0.026206316 val_loss 0.03139113
Epoch 814: train_loss 0.026200397 val_loss 0.03139117
Epoch 815: train_loss 0.026194507 val_loss 0.03139121
Epoch 816: train_loss 0.02618865 val_loss 0.031391263
Epoch 817: train_loss 0.026182821 val_loss 0.031391326
Epoch 818: train_loss 0.026177024 val_loss 0.031391393
Epoch 819: train_loss 0.02617126 val_loss 0.031391464
Epoch 820: train_loss 0.02616552 val_loss 0.031391542
Epoch 821: train_loss 0.026159814 val_loss 0.03139162
Epoch 822: train_loss 0.026154138 val_loss 0.031391714
Epoch 823: train_loss 0.026148492 val_loss 0.031391814
Epoch 824: train_loss 0.026142877 val_loss 0.031391915
Epoch 825: train_loss 0.026137289 val_loss 0.031392027
Epoch 826: train_loss 0.02613173 val_loss 0.03139214
Epoch 827: train_loss 0.026126198 val_loss 0.031392265
Epoch 828: train_loss 0.026120696 val_loss 0.03139239
Epoch 829: train_loss 0.026115224 val_loss 0.031392533
Epoch 830: train_loss 0.02610978 val_loss 0.031392667
Epoch 831: train_loss 0.026104365 val_loss 0.031392816
Epoch 832: train_loss 0.026098976 val_loss 0.031392965
Epoch 833: train_loss 0.026093615 val_loss 0.03139311
Epoch 834: train_loss 0.026088284 val_loss 0.03139327
Epoch 835: train_loss 0.026082978 val_loss 0.031393442
Epoch 836: train_loss 0.026077697 val_loss 0.03139362
Epoch 837: train_loss 0.026072448 val_loss 0.031393804
Epoch 838: train_loss 0.026067225 val_loss 0.03139398
Epoch 839: train_loss 0.026062028 val_loss 0.031394165
Epoch 840: train_loss 0.026056862 val_loss 0.03139436
Epoch 841: train_loss 0.026051715 val_loss 0.031394567
Epoch 842: train_loss 0.026046596 val_loss 0.031394776
Epoch 843: train_loss 0.026041506 val_loss 0.031394992
Epoch 844: train_loss 0.026036441 val_loss 0.03139521
Epoch 845: train_loss 0.026031401 val_loss 0.031395447
Epoch 846: train_loss 0.026026385 val_loss 0.03139567
Epoch 847: train_loss 0.026021397 val_loss 0.031395905
Epoch 848: train_loss 0.026016433 val_loss 0.03139614
Epoch 849: train_loss 0.026011495 val_loss 0.031396385
Epoch 850: train_loss 0.026006581 val_loss 0.031396627
Epoch 851: train_loss 0.026001692 val_loss 0.031396873
Epoch 852: train_loss 0.02599683 val_loss 0.031397127
Epoch 853: train_loss 0.025991987 val_loss 0.03139738
Epoch 854: train_loss 0.025987172 val_loss 0.031397652
Epoch 855: train_loss 0.025982378 val_loss 0.03139793
Epoch 856: train_loss 0.025977613 val_loss 0.031398214
Epoch 857: train_loss 0.025972867 val_loss 0.031398498
Epoch 858: train_loss 0.025968151 val_loss 0.03139878
Epoch 859: train_loss 0.02596345 val_loss 0.03139907
Epoch 860: train_loss 0.025958778 val_loss 0.031399366
Epoch 861: train_loss 0.025954127 val_loss 0.031399664
Epoch 862: train_loss 0.0259495 val_loss 0.031399954
Epoch 863: train_loss 0.025944894 val_loss 0.031400263
Epoch 864: train_loss 0.02594031 val_loss 0.03140058
Epoch 865: train_loss 0.02593575 val_loss 0.0314009
Epoch 866: train_loss 0.025931215 val_loss 0.031401224
Epoch 867: train_loss 0.025926698 val_loss 0.03140156
Epoch 868: train_loss 0.025922205 val_loss 0.0314019
Epoch 869: train_loss 0.025917733 val_loss 0.031402238
Epoch 870: train_loss 0.025913285 val_loss 0.031402584
Epoch 871: train_loss 0.025908856 val_loss 0.031402927
Epoch 872: train_loss 0.025904447 val_loss 0.031403277
Epoch 873: train_loss 0.025900062 val_loss 0.031403624
Epoch 874: train_loss 0.0258957 val_loss 0.031403974
Epoch 875: train_loss 0.025891354 val_loss 0.031404335
Epoch 876: train_loss 0.025887031 val_loss 0.031404704
Epoch 877: train_loss 0.025882727 val_loss 0.03140508
Epoch 878: train_loss 0.025878448 val_loss 0.03140546
Epoch 879: train_loss 0.025874186 val_loss 0.031405836
Epoch 880: train_loss 0.025869945 val_loss 0.03140621
Epoch 881: train_loss 0.025865724 val_loss 0.031406596
Epoch 882: train_loss 0.025861522 val_loss 0.031406987
Epoch 883: train_loss 0.025857339 val_loss 0.031407386
Epoch 884: train_loss 0.025853176 val_loss 0.03140779
Epoch 885: train_loss 0.025849037 val_loss 0.0314082
Epoch 886: train_loss 0.02584491 val_loss 0.031408608
Epoch 887: train_loss 0.025840806 val_loss 0.03140902
Epoch 888: train_loss 0.025836721 val_loss 0.031409424
Epoch 889: train_loss 0.025832659 val_loss 0.031409834
Epoch 890: train_loss 0.025828607 val_loss 0.031410243
Epoch 891: train_loss 0.025824578 val_loss 0.031410668
Epoch 892: train_loss 0.025820568 val_loss 0.031411085
Epoch 893: train_loss 0.025816575 val_loss 0.03141152
Epoch 894: train_loss 0.025812604 val_loss 0.031411953
Epoch 895: train_loss 0.025808645 val_loss 0.0314124
Epoch 896: train_loss 0.02580471 val_loss 0.031412847
Epoch 897: train_loss 0.025800787 val_loss 0.031413287
Epoch 898: train_loss 0.025796887 val_loss 0.03141373
Epoch 899: train_loss 0.025793003 val_loss 0.03141419
Epoch 900: train_loss 0.025789132 val_loss 0.031414643
Epoch 901: train_loss 0.025785284 val_loss 0.031415105
Epoch 902: train_loss 0.02578145 val_loss 0.03141557
Epoch 903: train_loss 0.025777638 val_loss 0.03141604
Epoch 904: train_loss 0.025773834 val_loss 0.031416506
Epoch 905: train_loss 0.025770053 val_loss 0.031416975
Epoch 906: train_loss 0.025766289 val_loss 0.031417456
Epoch 907: train_loss 0.02576254 val_loss 0.031417932
Epoch 908: train_loss 0.025758808 val_loss 0.03141842
Epoch 909: train_loss 0.025755093 val_loss 0.03141891
Epoch 910: train_loss 0.025751393 val_loss 0.0314194
Epoch 911: train_loss 0.025747707 val_loss 0.0314199
Epoch 912: train_loss 0.02574404 val_loss 0.03142039
Epoch 913: train_loss 0.025740387 val_loss 0.03142089
Epoch 914: train_loss 0.025736753 val_loss 0.031421397
Epoch 915: train_loss 0.025733132 val_loss 0.031421896
Epoch 916: train_loss 0.025729528 val_loss 0.031422403
Epoch 917: train_loss 0.025725935 val_loss 0.031422902
Epoch 918: train_loss 0.02572236 val_loss 0.03142341
Epoch 919: train_loss 0.0257188 val_loss 0.031423923
Epoch 920: train_loss 0.025715256 val_loss 0.03142444
Epoch 921: train_loss 0.025711726 val_loss 0.03142496
Epoch 922: train_loss 0.025708212 val_loss 0.03142548
Epoch 923: train_loss 0.025704712 val_loss 0.031426
Epoch 924: train_loss 0.025701225 val_loss 0.031426523
Epoch 925: train_loss 0.025697753 val_loss 0.031427067
Epoch 926: train_loss 0.025694294 val_loss 0.031427607
Epoch 927: train_loss 0.025690854 val_loss 0.031428155
Epoch 928: train_loss 0.025687423 val_loss 0.031428702
Epoch 929: train_loss 0.02568401 val_loss 0.03142924
Epoch 930: train_loss 0.02568061 val_loss 0.03142978
Epoch 931: train_loss 0.025677223 val_loss 0.031430326
Epoch 932: train_loss 0.025673848 val_loss 0.031430878
Epoch 933: train_loss 0.02567049 val_loss 0.03143144
Epoch 934: train_loss 0.02566714 val_loss 0.031432
Epoch 935: train_loss 0.02566381 val_loss 0.031432565
Epoch 936: train_loss 0.02566049 val_loss 0.031433128
Epoch 937: train_loss 0.025657183 val_loss 0.031433698
Epoch 938: train_loss 0.02565389 val_loss 0.031434257
Epoch 939: train_loss 0.02565061 val_loss 0.031434823
Epoch 940: train_loss 0.025647342 val_loss 0.031435378
Epoch 941: train_loss 0.025644086 val_loss 0.03143594
Epoch 942: train_loss 0.025640847 val_loss 0.031436518
Epoch 943: train_loss 0.025637615 val_loss 0.031437088
Epoch 944: train_loss 0.0256344 val_loss 0.03143767
Epoch 945: train_loss 0.025631197 val_loss 0.031438258
Epoch 946: train_loss 0.025628002 val_loss 0.031438846
Epoch 947: train_loss 0.025624825 val_loss 0.03143944
Epoch 948: train_loss 0.025621656 val_loss 0.031440035
Epoch 949: train_loss 0.025618503 val_loss 0.03144063
Epoch 950: train_loss 0.025615359 val_loss 0.031441227
Epoch 951: train_loss 0.025612226 val_loss 0.031441826
Epoch 952: train_loss 0.025609106 val_loss 0.03144242
Epoch 953: train_loss 0.025605997 val_loss 0.03144301
Epoch 954: train_loss 0.025602901 val_loss 0.031443615
Epoch 955: train_loss 0.025599817 val_loss 0.031444225
Epoch 956: train_loss 0.025596742 val_loss 0.03144483
Epoch 957: train_loss 0.025593681 val_loss 0.031445436
Epoch 958: train_loss 0.02559063 val_loss 0.03144605
Epoch 959: train_loss 0.025587589 val_loss 0.031446677
Epoch 960: train_loss 0.025584562 val_loss 0.031447295
Epoch 961: train_loss 0.025581542 val_loss 0.031447917
Epoch 962: train_loss 0.025578534 val_loss 0.03144853
Epoch 963: train_loss 0.02557554 val_loss 0.031449154
Epoch 964: train_loss 0.025572557 val_loss 0.031449776
Epoch 965: train_loss 0.02556958 val_loss 0.031450395
Epoch 966: train_loss 0.025566617 val_loss 0.031451024
Epoch 967: train_loss 0.025563665 val_loss 0.03145165
Epoch 968: train_loss 0.025560724 val_loss 0.03145227
Epoch 969: train_loss 0.025557788 val_loss 0.0314529
Epoch 970: train_loss 0.025554866 val_loss 0.03145353
Epoch 971: train_loss 0.025551954 val_loss 0.03145417
Epoch 972: train_loss 0.025549054 val_loss 0.03145481
Epoch 973: train_loss 0.02554616 val_loss 0.031455457
Epoch 974: train_loss 0.025543278 val_loss 0.031456098
Epoch 975: train_loss 0.02554041 val_loss 0.031456746
Epoch 976: train_loss 0.025537547 val_loss 0.03145739
Epoch 977: train_loss 0.025534697 val_loss 0.031458035
Epoch 978: train_loss 0.025531853 val_loss 0.03145868
Epoch 979: train_loss 0.02552902 val_loss 0.031459324
Epoch 980: train_loss 0.025526198 val_loss 0.03145997
Epoch 981: train_loss 0.025523383 val_loss 0.031460606
Epoch 982: train_loss 0.02552058 val_loss 0.03146126
Epoch 983: train_loss 0.025517784 val_loss 0.031461924
Epoch 984: train_loss 0.025515001 val_loss 0.031462584
Epoch 985: train_loss 0.025512222 val_loss 0.03146325
Epoch 986: train_loss 0.025509458 val_loss 0.03146392
Epoch 987: train_loss 0.0255067 val_loss 0.031464584
Epoch 988: train_loss 0.02550395 val_loss 0.031465247
Epoch 989: train_loss 0.025501214 val_loss 0.031465918
Epoch 990: train_loss 0.02549848 val_loss 0.031466585
Epoch 991: train_loss 0.025495758 val_loss 0.03146724
Epoch 992: train_loss 0.025493044 val_loss 0.03146791
Epoch 993: train_loss 0.02549034 val_loss 0.031468574
Epoch 994: train_loss 0.025487646 val_loss 0.031469245
Epoch 995: train_loss 0.02548496 val_loss 0.031469908
Epoch 996: train_loss 0.025482282 val_loss 0.031470574
Epoch 997: train_loss 0.025479611 val_loss 0.03147124
Epoch 998: train_loss 0.02547695 val_loss 0.031471916
Epoch 999: train_loss 0.025474295 val_loss 0.03147259
Epoch 1000: train_loss 0.025471652 val_loss 0.03147327
Epoch 1001: train_loss 0.025469014 val_loss 0.031473957
Epoch 1002: train_loss 0.025466386 val_loss 0.031474646
Epoch 1003: train_loss 0.025463765 val_loss 0.031475328
Epoch 1004: train_loss 0.025461154 val_loss 0.031476006
Epoch 1005: train_loss 0.02545855 val_loss 0.0314767
Epoch 1006: train_loss 0.025455954 val_loss 0.03147739
Epoch 1007: train_loss 0.025453366 val_loss 0.031478092
Epoch 1008: train_loss 0.025450785 val_loss 0.03147878
Epoch 1009: train_loss 0.025448214 val_loss 0.03147947
Epoch 1010: train_loss 0.025445648 val_loss 0.031480152
Epoch 1011: train_loss 0.025443092 val_loss 0.031480845
Epoch 1012: train_loss 0.025440544 val_loss 0.03148153
Epoch 1013: train_loss 0.025438001 val_loss 0.031482216
Epoch 1014: train_loss 0.025435466 val_loss 0.031482913
Epoch 1015: train_loss 0.02543294 val_loss 0.031483606
Epoch 1016: train_loss 0.02543042 val_loss 0.031484302
Epoch 1017: train_loss 0.02542791 val_loss 0.031485
Epoch 1018: train_loss 0.025425404 val_loss 0.0314857
Epoch 1019: train_loss 0.025422908 val_loss 0.0314864
Epoch 1020: train_loss 0.02542042 val_loss 0.031487104
Epoch 1021: train_loss 0.025417937 val_loss 0.03148781
Epoch 1022: train_loss 0.025415463 val_loss 0.03148851
Epoch 1023: train_loss 0.025412994 val_loss 0.03148922
Epoch 1024: train_loss 0.025410535 val_loss 0.03148992
Epoch 1025: train_loss 0.02540808 val_loss 0.03149063
Epoch 1026: train_loss 0.025405632 val_loss 0.031491343
Epoch 1027: train_loss 0.025403194 val_loss 0.031492047
Epoch 1028: train_loss 0.02540076 val_loss 0.03149275
Epoch 1029: train_loss 0.025398333 val_loss 0.031493455
Epoch 1030: train_loss 0.025395915 val_loss 0.03149416
Epoch 1031: train_loss 0.025393503 val_loss 0.03149487
Epoch 1032: train_loss 0.025391096 val_loss 0.03149559
Epoch 1033: train_loss 0.025388697 val_loss 0.031496312
Epoch 1034: train_loss 0.025386304 val_loss 0.03149702
Epoch 1035: train_loss 0.02538392 val_loss 0.031497724
Epoch 1036: train_loss 0.025381539 val_loss 0.031498443
Epoch 1037: train_loss 0.025379168 val_loss 0.031499162
Epoch 1038: train_loss 0.0253768 val_loss 0.031499874
Epoch 1039: train_loss 0.025374439 val_loss 0.03150058
Epoch 1040: train_loss 0.025372086 val_loss 0.03150129
Epoch 1041: train_loss 0.02536974 val_loss 0.03150201
Epoch 1042: train_loss 0.025367402 val_loss 0.031502727
Epoch 1043: train_loss 0.025365064 val_loss 0.031503446
Epoch 1044: train_loss 0.025362737 val_loss 0.031504177
Epoch 1045: train_loss 0.025360415 val_loss 0.0315049
Epoch 1046: train_loss 0.025358103 val_loss 0.031505626
Epoch 1047: train_loss 0.02535579 val_loss 0.031506363
Epoch 1048: train_loss 0.025353488 val_loss 0.031507082
Epoch 1049: train_loss 0.02535119 val_loss 0.031507805
Epoch 1050: train_loss 0.025348898 val_loss 0.03150853
Epoch 1051: train_loss 0.025346614 val_loss 0.031509254
Epoch 1052: train_loss 0.025344335 val_loss 0.031509977
Epoch 1053: train_loss 0.02534206 val_loss 0.031510703
Epoch 1054: train_loss 0.025339795 val_loss 0.031511422
Epoch 1055: train_loss 0.025337532 val_loss 0.03151214
Epoch 1056: train_loss 0.025335278 val_loss 0.031512868
Epoch 1057: train_loss 0.025333026 val_loss 0.031513594
Epoch 1058: train_loss 0.025330784 val_loss 0.03151432
Epoch 1059: train_loss 0.025328547 val_loss 0.03151505
Epoch 1060: train_loss 0.02532631 val_loss 0.03151578
Epoch 1061: train_loss 0.025324086 val_loss 0.031516504
Epoch 1062: train_loss 0.025321864 val_loss 0.03151723
Epoch 1063: train_loss 0.025319647 val_loss 0.031517956
Epoch 1064: train_loss 0.025317438 val_loss 0.03151868
Epoch 1065: train_loss 0.025315234 val_loss 0.031519413
Epoch 1066: train_loss 0.025313035 val_loss 0.031520143
Epoch 1067: train_loss 0.02531084 val_loss 0.031520877
Epoch 1068: train_loss 0.025308654 val_loss 0.031521615
Epoch 1069: train_loss 0.025306469 val_loss 0.03152235
Epoch 1070: train_loss 0.025304295 val_loss 0.031523086
Epoch 1071: train_loss 0.025302123 val_loss 0.031523813
Epoch 1072: train_loss 0.025299955 val_loss 0.031524554
Epoch 1073: train_loss 0.025297794 val_loss 0.03152528
Epoch 1074: train_loss 0.025295641 val_loss 0.031526003
Epoch 1075: train_loss 0.025293488 val_loss 0.031526733
Epoch 1076: train_loss 0.025291344 val_loss 0.03152746
Epoch 1077: train_loss 0.025289204 val_loss 0.03152818
Epoch 1078: train_loss 0.02528707 val_loss 0.031528916
Epoch 1079: train_loss 0.02528494 val_loss 0.031529646
Epoch 1080: train_loss 0.025282817 val_loss 0.03153038
Epoch 1081: train_loss 0.025280695 val_loss 0.031531125
Epoch 1082: train_loss 0.025278581 val_loss 0.03153186
Epoch 1083: train_loss 0.025276473 val_loss 0.03153259
Epoch 1084: train_loss 0.025274366 val_loss 0.03153333
Epoch 1085: train_loss 0.025272267 val_loss 0.031534053
Epoch 1086: train_loss 0.025270173 val_loss 0.031534787
Epoch 1087: train_loss 0.025268085 val_loss 0.03153552
Epoch 1088: train_loss 0.025266 val_loss 0.03153626
Epoch 1089: train_loss 0.025263922 val_loss 0.031536993
Epoch 1090: train_loss 0.025261847 val_loss 0.03153773
Epoch 1091: train_loss 0.02525978 val_loss 0.031538464
Epoch 1092: train_loss 0.025257712 val_loss 0.03153921
Epoch 1093: train_loss 0.025255654 val_loss 0.031539943
Epoch 1094: train_loss 0.0252536 val_loss 0.03154067
Epoch 1095: train_loss 0.025251549 val_loss 0.0315414
Epoch 1096: train_loss 0.025249504 val_loss 0.031542137
Epoch 1097: train_loss 0.025247462 val_loss 0.03154286
Epoch 1098: train_loss 0.025245426 val_loss 0.031543598
Epoch 1099: train_loss 0.025243392 val_loss 0.031544328
Epoch 1100: train_loss 0.025241368 val_loss 0.031545065
Epoch 1101: train_loss 0.025239345 val_loss 0.0315458
Epoch 1102: train_loss 0.025237326 val_loss 0.031546537
Epoch 1103: train_loss 0.025235316 val_loss 0.031547274
Epoch 1104: train_loss 0.025233308 val_loss 0.03154802
Epoch 1105: train_loss 0.025231304 val_loss 0.03154875
Epoch 1106: train_loss 0.025229307 val_loss 0.031549487
Epoch 1107: train_loss 0.02522731 val_loss 0.03155022
Epoch 1108: train_loss 0.025225319 val_loss 0.031550944
Epoch 1109: train_loss 0.025223335 val_loss 0.031551678
Epoch 1110: train_loss 0.025221353 val_loss 0.031552408
Epoch 1111: train_loss 0.025219377 val_loss 0.031553145
Epoch 1112: train_loss 0.025217405 val_loss 0.03155388
Epoch 1113: train_loss 0.025215436 val_loss 0.03155461
Epoch 1114: train_loss 0.025213473 val_loss 0.03155534
Epoch 1115: train_loss 0.025211515 val_loss 0.031556074
Epoch 1116: train_loss 0.02520956 val_loss 0.03155681
Epoch 1117: train_loss 0.025207609 val_loss 0.031557534
Epoch 1118: train_loss 0.025205662 val_loss 0.031558264
Epoch 1119: train_loss 0.02520372 val_loss 0.03155899
Epoch 1120: train_loss 0.025201786 val_loss 0.031559717
Epoch 1121: train_loss 0.025199853 val_loss 0.031560447
Epoch 1122: train_loss 0.025197921 val_loss 0.031561185
Epoch 1123: train_loss 0.025195997 val_loss 0.031561904
Epoch 1124: train_loss 0.025194077 val_loss 0.03156263
Epoch 1125: train_loss 0.02519216 val_loss 0.031563357
Epoch 1126: train_loss 0.02519025 val_loss 0.031564083
Epoch 1127: train_loss 0.025188342 val_loss 0.03156482
Epoch 1128: train_loss 0.025186436 val_loss 0.03156555
Epoch 1129: train_loss 0.025184538 val_loss 0.03156629
Epoch 1130: train_loss 0.02518264 val_loss 0.031567015
Epoch 1131: train_loss 0.025180751 val_loss 0.03156773
Epoch 1132: train_loss 0.025178865 val_loss 0.03156846
Epoch 1133: train_loss 0.02517698 val_loss 0.03156919
Epoch 1134: train_loss 0.025175102 val_loss 0.031569935
Epoch 1135: train_loss 0.025173225 val_loss 0.031570666
Epoch 1136: train_loss 0.025171356 val_loss 0.031571392
Epoch 1137: train_loss 0.025169488 val_loss 0.031572115
Epoch 1138: train_loss 0.025167625 val_loss 0.031572834
Epoch 1139: train_loss 0.025165766 val_loss 0.03157355
Epoch 1140: train_loss 0.025163911 val_loss 0.031574268
Epoch 1141: train_loss 0.02516206 val_loss 0.031574983
Epoch 1142: train_loss 0.025160212 val_loss 0.03157571
Epoch 1143: train_loss 0.025158368 val_loss 0.031576436
Epoch 1144: train_loss 0.025156531 val_loss 0.031577155
Epoch 1145: train_loss 0.025154695 val_loss 0.031577874
Epoch 1146: train_loss 0.025152864 val_loss 0.031578586
Epoch 1147: train_loss 0.025151037 val_loss 0.0315793
Epoch 1148: train_loss 0.025149213 val_loss 0.031580016
Epoch 1149: train_loss 0.025147391 val_loss 0.03158073
Epoch 1150: train_loss 0.025145575 val_loss 0.031581454
Epoch 1151: train_loss 0.025143763 val_loss 0.031582195
Epoch 1152: train_loss 0.025141954 val_loss 0.03158293
Epoch 1153: train_loss 0.02514015 val_loss 0.03158365
Epoch 1154: train_loss 0.025138348 val_loss 0.03158437
Epoch 1155: train_loss 0.025136553 val_loss 0.031585086
Epoch 1156: train_loss 0.025134757 val_loss 0.031585798
Epoch 1157: train_loss 0.025132969 val_loss 0.03158651
Epoch 1158: train_loss 0.025131183 val_loss 0.031587217
Epoch 1159: train_loss 0.0251294 val_loss 0.03158792
Epoch 1160: train_loss 0.025127621 val_loss 0.03158863
Epoch 1161: train_loss 0.025125846 val_loss 0.031589344
Epoch 1162: train_loss 0.025124075 val_loss 0.031590067
Epoch 1163: train_loss 0.025122307 val_loss 0.031590793
Epoch 1164: train_loss 0.025120543 val_loss 0.031591505
Epoch 1165: train_loss 0.025118785 val_loss 0.03159222
Epoch 1166: train_loss 0.025117029 val_loss 0.031592928
Epoch 1167: train_loss 0.025115274 val_loss 0.031593632
Epoch 1168: train_loss 0.025113527 val_loss 0.031594336
Epoch 1169: train_loss 0.025111781 val_loss 0.031595044
Epoch 1170: train_loss 0.025110036 val_loss 0.031595748
Epoch 1171: train_loss 0.025108298 val_loss 0.03159645
Epoch 1172: train_loss 0.025106564 val_loss 0.031597164
Epoch 1173: train_loss 0.025104834 val_loss 0.031597864
Epoch 1174: train_loss 0.025103105 val_loss 0.03159858
Epoch 1175: train_loss 0.025101382 val_loss 0.031599287
Epoch 1176: train_loss 0.025099657 val_loss 0.031599987
Epoch 1177: train_loss 0.025097942 val_loss 0.03160069
Epoch 1178: train_loss 0.025096226 val_loss 0.031601384
Epoch 1179: train_loss 0.025094518 val_loss 0.03160208
Epoch 1180: train_loss 0.025092812 val_loss 0.031602778
Epoch 1181: train_loss 0.025091108 val_loss 0.031603478
Epoch 1182: train_loss 0.02508941 val_loss 0.031604167
Epoch 1183: train_loss 0.02508771 val_loss 0.031604867
Epoch 1184: train_loss 0.025086015 val_loss 0.03160557
Epoch 1185: train_loss 0.025084326 val_loss 0.031606272
Epoch 1186: train_loss 0.025082642 val_loss 0.031606972
Epoch 1187: train_loss 0.025080955 val_loss 0.031607665
Epoch 1188: train_loss 0.025079278 val_loss 0.03160835
Epoch 1189: train_loss 0.025077602 val_loss 0.031609036
Epoch 1190: train_loss 0.025075931 val_loss 0.03160972
Epoch 1191: train_loss 0.025074262 val_loss 0.031610418
Epoch 1192: train_loss 0.025072593 val_loss 0.0316111
Epoch 1193: train_loss 0.025070932 val_loss 0.03161178
Epoch 1194: train_loss 0.025069272 val_loss 0.031612474
Epoch 1195: train_loss 0.025067616 val_loss 0.031613156
Epoch 1196: train_loss 0.025065962 val_loss 0.031613838
Epoch 1197: train_loss 0.025064312 val_loss 0.03161453
Epoch 1198: train_loss 0.025062667 val_loss 0.03161522
Epoch 1199: train_loss 0.025061024 val_loss 0.031615898
Epoch 1200: train_loss 0.025059387 val_loss 0.031616576
Epoch 1201: train_loss 0.025057746 val_loss 0.03161726
Epoch 1202: train_loss 0.025056114 val_loss 0.031617936
Epoch 1203: train_loss 0.025054483 val_loss 0.031618614
Epoch 1204: train_loss 0.025052859 val_loss 0.031619288
Epoch 1205: train_loss 0.025051234 val_loss 0.03161996
Epoch 1206: train_loss 0.025049618 val_loss 0.031620633
Epoch 1207: train_loss 0.025047999 val_loss 0.03162131
Epoch 1208: train_loss 0.025046382 val_loss 0.031621985
Epoch 1209: train_loss 0.025044773 val_loss 0.03162266
Epoch 1210: train_loss 0.025043167 val_loss 0.03162333
Epoch 1211: train_loss 0.02504156 val_loss 0.031623997
Epoch 1212: train_loss 0.02503996 val_loss 0.031624667
Epoch 1213: train_loss 0.025038362 val_loss 0.031625334
Epoch 1214: train_loss 0.025036765 val_loss 0.031625997
Epoch 1215: train_loss 0.025035175 val_loss 0.031626668
Epoch 1216: train_loss 0.025033588 val_loss 0.031627335
Epoch 1217: train_loss 0.025032002 val_loss 0.031628005
Epoch 1218: train_loss 0.025030421 val_loss 0.03162866
Epoch 1219: train_loss 0.025028842 val_loss 0.031629335
Epoch 1220: train_loss 0.025027266 val_loss 0.031629995
Epoch 1221: train_loss 0.025025694 val_loss 0.031630658
Epoch 1222: train_loss 0.025024122 val_loss 0.03163132
Epoch 1223: train_loss 0.025022555 val_loss 0.03163198
Epoch 1224: train_loss 0.02502099 val_loss 0.031632632
Epoch 1225: train_loss 0.025019431 val_loss 0.031633284
Epoch 1226: train_loss 0.025017874 val_loss 0.031633943
Epoch 1227: train_loss 0.025016319 val_loss 0.0316346
Epoch 1228: train_loss 0.025014766 val_loss 0.03163525
Epoch 1229: train_loss 0.02501322 val_loss 0.0316359
Epoch 1230: train_loss 0.025011675 val_loss 0.031636555
Epoch 1231: train_loss 0.025010131 val_loss 0.031637203
Epoch 1232: train_loss 0.025008593 val_loss 0.031637844
Epoch 1233: train_loss 0.025007056 val_loss 0.031638477
Epoch 1234: train_loss 0.025005523 val_loss 0.031639114
Epoch 1235: train_loss 0.025003994 val_loss 0.031639755
Epoch 1236: train_loss 0.025002465 val_loss 0.0316404
Epoch 1237: train_loss 0.025000941 val_loss 0.03164105
Epoch 1238: train_loss 0.02499942 val_loss 0.031641692
Epoch 1239: train_loss 0.024997901 val_loss 0.031642336
Epoch 1240: train_loss 0.024996385 val_loss 0.03164298
Epoch 1241: train_loss 0.024994874 val_loss 0.031643618
Epoch 1242: train_loss 0.024993364 val_loss 0.031644255
Epoch 1243: train_loss 0.024991855 val_loss 0.031644884
Epoch 1244: train_loss 0.024990352 val_loss 0.031645514
Epoch 1245: train_loss 0.024988852 val_loss 0.031646136
Epoch 1246: train_loss 0.024987353 val_loss 0.03164677
Epoch 1247: train_loss 0.024985857 val_loss 0.03164741
Epoch 1248: train_loss 0.024984367 val_loss 0.031648047
Epoch 1249: train_loss 0.024982877 val_loss 0.031648684
Epoch 1250: train_loss 0.024981393 val_loss 0.03164932
Epoch 1251: train_loss 0.024979906 val_loss 0.031649947
Epoch 1252: train_loss 0.024978425 val_loss 0.031650566
Epoch 1253: train_loss 0.024976948 val_loss 0.031651184
Epoch 1254: train_loss 0.024975475 val_loss 0.031651806
Epoch 1255: train_loss 0.024974002 val_loss 0.031652424
Epoch 1256: train_loss 0.024972532 val_loss 0.031653047
Epoch 1257: train_loss 0.024971066 val_loss 0.031653658
Epoch 1258: train_loss 0.024969602 val_loss 0.03165427
Epoch 1259: train_loss 0.024968142 val_loss 0.031654887
Epoch 1260: train_loss 0.024966683 val_loss 0.031655498
Epoch 1261: train_loss 0.024965227 val_loss 0.031656127
Epoch 1262: train_loss 0.024963776 val_loss 0.031656746
Epoch 1263: train_loss 0.024962325 val_loss 0.031657357
Epoch 1264: train_loss 0.02496088 val_loss 0.03165796
Epoch 1265: train_loss 0.024959436 val_loss 0.03165857
Epoch 1266: train_loss 0.024957996 val_loss 0.03165917
Epoch 1267: train_loss 0.024956556 val_loss 0.03165978
Epoch 1268: train_loss 0.024955118 val_loss 0.031660385
Epoch 1269: train_loss 0.024953686 val_loss 0.031660993
Epoch 1270: train_loss 0.024952257 val_loss 0.031661592
Epoch 1271: train_loss 0.024950828 val_loss 0.031662192
Epoch 1272: train_loss 0.024949403 val_loss 0.031662796
Epoch 1273: train_loss 0.02494798 val_loss 0.031663388
Epoch 1274: train_loss 0.024946561 val_loss 0.031663973
Epoch 1275: train_loss 0.024945145 val_loss 0.031664565
Epoch 1276: train_loss 0.024943732 val_loss 0.03166515
Epoch 1277: train_loss 0.02494232 val_loss 0.03166574
Epoch 1278: train_loss 0.024940912 val_loss 0.031666327
Epoch 1279: train_loss 0.024939505 val_loss 0.031666905
Epoch 1280: train_loss 0.024938103 val_loss 0.031667493
Epoch 1281: train_loss 0.0249367 val_loss 0.031668082
Epoch 1282: train_loss 0.024935305 val_loss 0.03166866
Epoch 1283: train_loss 0.02493391 val_loss 0.031669244
Epoch 1284: train_loss 0.024932515 val_loss 0.03166982
Epoch 1285: train_loss 0.024931127 val_loss 0.031670395
Epoch 1286: train_loss 0.024929736 val_loss 0.031670965
Epoch 1287: train_loss 0.024928354 val_loss 0.031671543
Epoch 1288: train_loss 0.02492697 val_loss 0.031672128
Epoch 1289: train_loss 0.02492559 val_loss 0.031672712
Epoch 1290: train_loss 0.024924215 val_loss 0.031673297
Epoch 1291: train_loss 0.024922838 val_loss 0.03167387
Epoch 1292: train_loss 0.024921468 val_loss 0.03167444
Epoch 1293: train_loss 0.0249201 val_loss 0.031675003
Epoch 1294: train_loss 0.024918733 val_loss 0.03167557
Epoch 1295: train_loss 0.02491737 val_loss 0.03167613
Epoch 1296: train_loss 0.02491601 val_loss 0.031676695
Epoch 1297: train_loss 0.024914648 val_loss 0.03167726
Epoch 1298: train_loss 0.024913294 val_loss 0.03167782
Epoch 1299: train_loss 0.02491194 val_loss 0.031678375
Epoch 1300: train_loss 0.024910588 val_loss 0.031678934
Epoch 1301: train_loss 0.02490924 val_loss 0.03167949
Epoch 1302: train_loss 0.024907893 val_loss 0.031680033
Epoch 1303: train_loss 0.024906551 val_loss 0.031680577
Epoch 1304: train_loss 0.02490521 val_loss 0.031681124
Epoch 1305: train_loss 0.02490387 val_loss 0.031681668
Epoch 1306: train_loss 0.024902536 val_loss 0.031682212
Epoch 1307: train_loss 0.024901202 val_loss 0.03168276
Epoch 1308: train_loss 0.02489987 val_loss 0.031683303
Epoch 1309: train_loss 0.024898542 val_loss 0.031683855
Epoch 1310: train_loss 0.024897218 val_loss 0.031684402
Epoch 1311: train_loss 0.024895893 val_loss 0.031684943
Epoch 1312: train_loss 0.024894571 val_loss 0.031685494
Epoch 1313: train_loss 0.024893252 val_loss 0.03168603
Epoch 1314: train_loss 0.024891939 val_loss 0.031686563
Epoch 1315: train_loss 0.024890624 val_loss 0.031687103
Epoch 1316: train_loss 0.024889313 val_loss 0.031687632
Epoch 1317: train_loss 0.024888003 val_loss 0.031688157
Epoch 1318: train_loss 0.024886698 val_loss 0.031688686
Epoch 1319: train_loss 0.024885394 val_loss 0.031689208
Epoch 1320: train_loss 0.024884092 val_loss 0.031689733
Epoch 1321: train_loss 0.024882793 val_loss 0.03169026
Epoch 1322: train_loss 0.024881497 val_loss 0.031690784
Epoch 1323: train_loss 0.024880202 val_loss 0.031691305
Epoch 1324: train_loss 0.024878914 val_loss 0.03169183
Epoch 1325: train_loss 0.024877623 val_loss 0.031692356
Epoch 1326: train_loss 0.024876336 val_loss 0.031692874
Epoch 1327: train_loss 0.02487505 val_loss 0.031693373
Epoch 1328: train_loss 0.02487377 val_loss 0.031693883
Epoch 1329: train_loss 0.024872491 val_loss 0.031694397
Epoch 1330: train_loss 0.024871213 val_loss 0.031694908
Epoch 1331: train_loss 0.024869937 val_loss 0.031695426
Epoch 1332: train_loss 0.024868667 val_loss 0.031695932
Epoch 1333: train_loss 0.024867395 val_loss 0.031696443
Epoch 1334: train_loss 0.024866126 val_loss 0.031696945
Epoch 1335: train_loss 0.02486486 val_loss 0.031697452
Epoch 1336: train_loss 0.0248636 val_loss 0.03169795
Epoch 1337: train_loss 0.024862338 val_loss 0.031698458
Epoch 1338: train_loss 0.02486108 val_loss 0.031698953
Epoch 1339: train_loss 0.024859825 val_loss 0.031699453
Epoch 1340: train_loss 0.02485857 val_loss 0.031699948
Epoch 1341: train_loss 0.02485732 val_loss 0.031700443
Epoch 1342: train_loss 0.024856068 val_loss 0.03170093
Epoch 1343: train_loss 0.024854826 val_loss 0.031701412
Epoch 1344: train_loss 0.02485358 val_loss 0.0317019
Epoch 1345: train_loss 0.024852337 val_loss 0.031702377
Epoch 1346: train_loss 0.024851095 val_loss 0.031702872
Epoch 1347: train_loss 0.024849858 val_loss 0.03170335
Epoch 1348: train_loss 0.024848621 val_loss 0.031703833
Epoch 1349: train_loss 0.02484739 val_loss 0.031704314
Epoch 1350: train_loss 0.024846159 val_loss 0.031704783
Epoch 1351: train_loss 0.024844928 val_loss 0.031705253
Epoch 1352: train_loss 0.024843704 val_loss 0.031705715
Epoch 1353: train_loss 0.02484248 val_loss 0.031706184
Epoch 1354: train_loss 0.024841256 val_loss 0.031706654
Epoch 1355: train_loss 0.024840038 val_loss 0.031707127
Epoch 1356: train_loss 0.02483882 val_loss 0.031707596
Epoch 1357: train_loss 0.024837606 val_loss 0.03170807
Epoch 1358: train_loss 0.024836391 val_loss 0.03170855
Epoch 1359: train_loss 0.024835182 val_loss 0.03170901
Epoch 1360: train_loss 0.024833972 val_loss 0.03170947
Epoch 1361: train_loss 0.024832767 val_loss 0.031709924
Epoch 1362: train_loss 0.024831563 val_loss 0.03171038
Epoch 1363: train_loss 0.02483036 val_loss 0.031710826
Epoch 1364: train_loss 0.024829162 val_loss 0.031711277
Epoch 1365: train_loss 0.024827965 val_loss 0.031711724
Epoch 1366: train_loss 0.024826767 val_loss 0.031712182
Epoch 1367: train_loss 0.024825573 val_loss 0.03171263
Epoch 1368: train_loss 0.024824385 val_loss 0.03171309
Epoch 1369: train_loss 0.024823194 val_loss 0.031713538
Epoch 1370: train_loss 0.02482201 val_loss 0.031713985
Epoch 1371: train_loss 0.024820823 val_loss 0.031714424
Epoch 1372: train_loss 0.02481964 val_loss 0.03171485
Epoch 1373: train_loss 0.02481846 val_loss 0.031715278
Epoch 1374: train_loss 0.024817282 val_loss 0.031715702
Epoch 1375: train_loss 0.024816107 val_loss 0.031716134
Epoch 1376: train_loss 0.024814934 val_loss 0.031716567
Epoch 1377: train_loss 0.02481376 val_loss 0.031717
Epoch 1378: train_loss 0.024812592 val_loss 0.031717442
Epoch 1379: train_loss 0.024811424 val_loss 0.03171787
Epoch 1380: train_loss 0.02481026 val_loss 0.031718303
Epoch 1381: train_loss 0.024809094 val_loss 0.031718723
Epoch 1382: train_loss 0.024807934 val_loss 0.031719156
Epoch 1383: train_loss 0.024806777 val_loss 0.031719554
Epoch 1384: train_loss 0.02480562 val_loss 0.031719964
Epoch 1385: train_loss 0.024804464 val_loss 0.03172037
Epoch 1386: train_loss 0.024803312 val_loss 0.03172078
Epoch 1387: train_loss 0.024802161 val_loss 0.031721186
Epoch 1388: train_loss 0.02480101 val_loss 0.0317216
Epoch 1389: train_loss 0.024799867 val_loss 0.031722013
Epoch 1390: train_loss 0.02479872 val_loss 0.031722423
Epoch 1391: train_loss 0.024797577 val_loss 0.03172282
Epoch 1392: train_loss 0.024796437 val_loss 0.03172322
Epoch 1393: train_loss 0.0247953 val_loss 0.031723607
Epoch 1394: train_loss 0.024794163 val_loss 0.031724002
Epoch 1395: train_loss 0.024793027 val_loss 0.031724397
Epoch 1396: train_loss 0.024791896 val_loss 0.03172479
Epoch 1397: train_loss 0.024790766 val_loss 0.031725187
Epoch 1398: train_loss 0.024789637 val_loss 0.03172558
Epoch 1399: train_loss 0.024788512 val_loss 0.031725973
Epoch 1400: train_loss 0.024787387 val_loss 0.031726353
Epoch 1401: train_loss 0.024786266 val_loss 0.031726744
Epoch 1402: train_loss 0.024785144 val_loss 0.031727128
Epoch 1403: train_loss 0.024784029 val_loss 0.031727508
Epoch 1404: train_loss 0.02478291 val_loss 0.031727884
Epoch 1405: train_loss 0.024781795 val_loss 0.031728253
Epoch 1406: train_loss 0.024780683 val_loss 0.031728614
Epoch 1407: train_loss 0.024779573 val_loss 0.031728983
Epoch 1408: train_loss 0.024778467 val_loss 0.03172935
Epoch 1409: train_loss 0.02477736 val_loss 0.03172972
Epoch 1410: train_loss 0.024776256 val_loss 0.03173009
Epoch 1411: train_loss 0.024775153 val_loss 0.03173045
Epoch 1412: train_loss 0.024774052 val_loss 0.03173082
Epoch 1413: train_loss 0.024772951 val_loss 0.03173119
Epoch 1414: train_loss 0.024771856 val_loss 0.03173155
Epoch 1415: train_loss 0.02477076 val_loss 0.031731907
Epoch 1416: train_loss 0.024769668 val_loss 0.03173227
Epoch 1417: train_loss 0.024768576 val_loss 0.03173263
Epoch 1418: train_loss 0.024767486 val_loss 0.031732984
Epoch 1419: train_loss 0.0247664 val_loss 0.031733334
Epoch 1420: train_loss 0.024765316 val_loss 0.031733695
Epoch 1421: train_loss 0.024764232 val_loss 0.031734034
Epoch 1422: train_loss 0.02476315 val_loss 0.03173438
Epoch 1423: train_loss 0.02476207 val_loss 0.031734716
Epoch 1424: train_loss 0.024760993 val_loss 0.031735063
Epoch 1425: train_loss 0.024759917 val_loss 0.0317354
Epoch 1426: train_loss 0.024758844 val_loss 0.031735733
Epoch 1427: train_loss 0.024757773 val_loss 0.03173606
Epoch 1428: train_loss 0.024756702 val_loss 0.031736404
Epoch 1429: train_loss 0.024755634 val_loss 0.03173673
Epoch 1430: train_loss 0.024754565 val_loss 0.031737074
Epoch 1431: train_loss 0.024753502 val_loss 0.031737402
Epoch 1432: train_loss 0.024752438 val_loss 0.03173773
Epoch 1433: train_loss 0.02475138 val_loss 0.03173807
Epoch 1434: train_loss 0.024750318 val_loss 0.031738397
Epoch 1435: train_loss 0.024749262 val_loss 0.031738713
Epoch 1436: train_loss 0.024748204 val_loss 0.03173904
Epoch 1437: train_loss 0.024747154 val_loss 0.031739354
Epoch 1438: train_loss 0.024746098 val_loss 0.031739667
Epoch 1439: train_loss 0.02474505 val_loss 0.031739987
Epoch 1440: train_loss 0.024744004 val_loss 0.0317403
Epoch 1441: train_loss 0.024742957 val_loss 0.031740613
Epoch 1442: train_loss 0.024741914 val_loss 0.031740922
Epoch 1443: train_loss 0.024740871 val_loss 0.031741228
Epoch 1444: train_loss 0.02473983 val_loss 0.031741522
Epoch 1445: train_loss 0.02473879 val_loss 0.03174183
Epoch 1446: train_loss 0.024737753 val_loss 0.03174213
Epoch 1447: train_loss 0.02473672 val_loss 0.031742435
Epoch 1448: train_loss 0.024735682 val_loss 0.031742737
Epoch 1449: train_loss 0.024734652 val_loss 0.03174304
Epoch 1450: train_loss 0.02473362 val_loss 0.03174332
Epoch 1451: train_loss 0.024732593 val_loss 0.031743612
Epoch 1452: train_loss 0.024731565 val_loss 0.0317439
Epoch 1453: train_loss 0.024730543 val_loss 0.031744182
Epoch 1454: train_loss 0.024729518 val_loss 0.031744473
Epoch 1455: train_loss 0.024728497 val_loss 0.031744767
Epoch 1456: train_loss 0.024727475 val_loss 0.031745046
Epoch 1457: train_loss 0.024726458 val_loss 0.031745326
Epoch 1458: train_loss 0.024725443 val_loss 0.031745616
Epoch 1459: train_loss 0.024724426 val_loss 0.031745892
Epoch 1460: train_loss 0.024723412 val_loss 0.031746183
Epoch 1461: train_loss 0.024722403 val_loss 0.031746455
Epoch 1462: train_loss 0.024721395 val_loss 0.031746734
Epoch 1463: train_loss 0.024720386 val_loss 0.031747
Epoch 1464: train_loss 0.02471938 val_loss 0.031747263
Epoch 1465: train_loss 0.024718376 val_loss 0.031747524
Epoch 1466: train_loss 0.024717374 val_loss 0.031747796
Epoch 1467: train_loss 0.024716372 val_loss 0.031748064
Epoch 1468: train_loss 0.024715373 val_loss 0.03174833
Epoch 1469: train_loss 0.024714375 val_loss 0.031748593
Epoch 1470: train_loss 0.024713378 val_loss 0.031748835
Epoch 1471: train_loss 0.024712386 val_loss 0.03174909
Epoch 1472: train_loss 0.024711393 val_loss 0.031749323
Epoch 1473: train_loss 0.024710404 val_loss 0.031749573
Epoch 1474: train_loss 0.024709413 val_loss 0.031749815
Epoch 1475: train_loss 0.024708424 val_loss 0.031750064
Epoch 1476: train_loss 0.024707437 val_loss 0.031750306
Epoch 1477: train_loss 0.024706455 val_loss 0.031750564
Epoch 1478: train_loss 0.024705471 val_loss 0.031750813
Epoch 1479: train_loss 0.024704488 val_loss 0.03175105
Epoch 1480: train_loss 0.024703508 val_loss 0.031751283
Epoch 1481: train_loss 0.024702532 val_loss 0.03175151
Epoch 1482: train_loss 0.024701556 val_loss 0.031751733
Epoch 1483: train_loss 0.02470058 val_loss 0.031751968
Epoch 1484: train_loss 0.024699606 val_loss 0.031752184
Epoch 1485: train_loss 0.024698636 val_loss 0.03175242
Epoch 1486: train_loss 0.024697663 val_loss 0.031752646
Epoch 1487: train_loss 0.024696698 val_loss 0.031752866
Epoch 1488: train_loss 0.024695732 val_loss 0.03175308
Epoch 1489: train_loss 0.024694767 val_loss 0.031753294
Epoch 1490: train_loss 0.024693804 val_loss 0.0317535
Epoch 1491: train_loss 0.024692839 val_loss 0.031753715
Epoch 1492: train_loss 0.024691882 val_loss 0.031753927
Epoch 1493: train_loss 0.024690922 val_loss 0.03175414
Epoch 1494: train_loss 0.024689965 val_loss 0.03175435
Epoch 1495: train_loss 0.024689008 val_loss 0.03175456
Epoch 1496: train_loss 0.024688054 val_loss 0.031754762
Epoch 1497: train_loss 0.0246871 val_loss 0.031754967
Epoch 1498: train_loss 0.02468615 val_loss 0.031755168
Epoch 1499: train_loss 0.024685202 val_loss 0.031755365
Epoch 1500: train_loss 0.024684252 val_loss 0.031755563
Epoch 1501: train_loss 0.024683306 val_loss 0.031755757
Epoch 1502: train_loss 0.02468236 val_loss 0.031755947
Epoch 1503: train_loss 0.024681417 val_loss 0.031756133
Epoch 1504: train_loss 0.024680475 val_loss 0.031756327
Epoch 1505: train_loss 0.024679534 val_loss 0.031756517
Epoch 1506: train_loss 0.024678595 val_loss 0.031756707
Epoch 1507: train_loss 0.024677658 val_loss 0.031756885
Epoch 1508: train_loss 0.024676725 val_loss 0.031757064
Epoch 1509: train_loss 0.024675788 val_loss 0.031757243
Epoch 1510: train_loss 0.024674857 val_loss 0.031757403
Epoch 1511: train_loss 0.024673922 val_loss 0.031757567
Epoch 1512: train_loss 0.024672993 val_loss 0.031757727
Epoch 1513: train_loss 0.024672065 val_loss 0.03175789
Epoch 1514: train_loss 0.024671137 val_loss 0.031758066
Epoch 1515: train_loss 0.024670213 val_loss 0.031758245
Epoch 1516: train_loss 0.024669288 val_loss 0.031758405
Epoch 1517: train_loss 0.024668366 val_loss 0.03175857
Epoch 1518: train_loss 0.024667444 val_loss 0.031758726
Epoch 1519: train_loss 0.024666525 val_loss 0.031758882
Epoch 1520: train_loss 0.024665607 val_loss 0.03175904
Epoch 1521: train_loss 0.02466469 val_loss 0.031759188
Epoch 1522: train_loss 0.024663774 val_loss 0.031759344
Epoch 1523: train_loss 0.02466286 val_loss 0.031759508
Epoch 1524: train_loss 0.024661949 val_loss 0.03175965
Epoch 1525: train_loss 0.024661036 val_loss 0.031759795
Epoch 1526: train_loss 0.024660127 val_loss 0.031759936
Epoch 1527: train_loss 0.02465922 val_loss 0.031760085
Epoch 1528: train_loss 0.024658313 val_loss 0.03176023
Epoch 1529: train_loss 0.024657408 val_loss 0.031760372
Epoch 1530: train_loss 0.024656503 val_loss 0.031760514
Epoch 1531: train_loss 0.0246556 val_loss 0.031760655
Epoch 1532: train_loss 0.0246547 val_loss 0.03176079
Epoch 1533: train_loss 0.0246538 val_loss 0.031760916
Epoch 1534: train_loss 0.0246529 val_loss 0.031761035
Epoch 1535: train_loss 0.024652004 val_loss 0.031761155
Epoch 1536: train_loss 0.024651108 val_loss 0.03176128
Epoch 1537: train_loss 0.024650214 val_loss 0.031761404
Epoch 1538: train_loss 0.024649322 val_loss 0.03176153
Epoch 1539: train_loss 0.02464843 val_loss 0.031761654
Epoch 1540: train_loss 0.02464754 val_loss 0.031761773
Epoch 1541: train_loss 0.024646653 val_loss 0.031761885
Epoch 1542: train_loss 0.024645763 val_loss 0.03176199
Epoch 1543: train_loss 0.024644876 val_loss 0.031762104
Epoch 1544: train_loss 0.024643991 val_loss 0.031762213
Epoch 1545: train_loss 0.024643108 val_loss 0.031762324
Epoch 1546: train_loss 0.024642225 val_loss 0.031762436
Epoch 1547: train_loss 0.024641348 val_loss 0.031762544
Epoch 1548: train_loss 0.024640467 val_loss 0.031762645
Epoch 1549: train_loss 0.02463959 val_loss 0.031762745
Epoch 1550: train_loss 0.024638714 val_loss 0.031762846
Epoch 1551: train_loss 0.024637837 val_loss 0.031762943
Epoch 1552: train_loss 0.024636963 val_loss 0.031763043
Epoch 1553: train_loss 0.024636092 val_loss 0.03176314
Epoch 1554: train_loss 0.02463522 val_loss 0.031763226
Epoch 1555: train_loss 0.024634348 val_loss 0.031763315
Epoch 1556: train_loss 0.024633482 val_loss 0.031763386
Epoch 1557: train_loss 0.024632614 val_loss 0.031763468
Epoch 1558: train_loss 0.024631744 val_loss 0.03176355
Epoch 1559: train_loss 0.024630882 val_loss 0.031763636
Epoch 1560: train_loss 0.024630016 val_loss 0.031763718
Epoch 1561: train_loss 0.024629155 val_loss 0.031763792
Epoch 1562: train_loss 0.024628295 val_loss 0.031763867
Epoch 1563: train_loss 0.024627434 val_loss 0.031763937
Epoch 1564: train_loss 0.024626575 val_loss 0.03176402
Epoch 1565: train_loss 0.024625719 val_loss 0.03176409
Epoch 1566: train_loss 0.024624862 val_loss 0.03176415
Epoch 1567: train_loss 0.024624007 val_loss 0.03176422
Epoch 1568: train_loss 0.024623152 val_loss 0.031764273
Epoch 1569: train_loss 0.024622303 val_loss 0.03176433
Epoch 1570: train_loss 0.02462145 val_loss 0.031764388
Epoch 1571: train_loss 0.0246206 val_loss 0.031764444
Epoch 1572: train_loss 0.024619753 val_loss 0.0317645
Epoch 1573: train_loss 0.024618905 val_loss 0.03176455
Epoch 1574: train_loss 0.02461806 val_loss 0.03176459
Epoch 1575: train_loss 0.024617214 val_loss 0.031764634
Epoch 1576: train_loss 0.02461637 val_loss 0.031764682
Epoch 1577: train_loss 0.024615528 val_loss 0.031764723
Epoch 1578: train_loss 0.024614686 val_loss 0.03176477
Epoch 1579: train_loss 0.024613846 val_loss 0.031764828
Epoch 1580: train_loss 0.024613008 val_loss 0.031764876
Epoch 1581: train_loss 0.02461217 val_loss 0.031764913
Epoch 1582: train_loss 0.024611333 val_loss 0.031764947
Epoch 1583: train_loss 0.0246105 val_loss 0.03176498
Epoch 1584: train_loss 0.024609666 val_loss 0.031765014
Epoch 1585: train_loss 0.024608832 val_loss 0.031765036
Epoch 1586: train_loss 0.024608001 val_loss 0.031765055
Epoch 1587: train_loss 0.02460717 val_loss 0.031765077
Epoch 1588: train_loss 0.024606341 val_loss 0.031765103
Epoch 1589: train_loss 0.024605513 val_loss 0.031765126
Epoch 1590: train_loss 0.024604686 val_loss 0.031765155
Epoch 1591: train_loss 0.02460386 val_loss 0.031765174
Epoch 1592: train_loss 0.024603035 val_loss 0.03176519
Epoch 1593: train_loss 0.024602212 val_loss 0.031765193
Epoch 1594: train_loss 0.02460139 val_loss 0.0317652
Epoch 1595: train_loss 0.024600571 val_loss 0.031765196
Epoch 1596: train_loss 0.02459975 val_loss 0.031765204
Epoch 1597: train_loss 0.02459893 val_loss 0.031765208
Epoch 1598: train_loss 0.024598112 val_loss 0.031765223
Epoch 1599: train_loss 0.024597296 val_loss 0.031765234
Epoch 1600: train_loss 0.02459648 val_loss 0.031765237
Epoch 1601: train_loss 0.024595667 val_loss 0.03176523
Epoch 1602: train_loss 0.024594853 val_loss 0.03176522
Epoch 1603: train_loss 0.024594042 val_loss 0.031765196
Epoch 1604: train_loss 0.02459323 val_loss 0.03176518
Epoch 1605: train_loss 0.024592422 val_loss 0.031765178
Epoch 1606: train_loss 0.024591612 val_loss 0.031765174
Epoch 1607: train_loss 0.024590805 val_loss 0.031765167
Epoch 1608: train_loss 0.024589999 val_loss 0.03176515
Epoch 1609: train_loss 0.024589192 val_loss 0.031765137
Epoch 1610: train_loss 0.024588387 val_loss 0.031765115
Epoch 1611: train_loss 0.024587585 val_loss 0.031765085
Epoch 1612: train_loss 0.024586784 val_loss 0.031765062
Epoch 1613: train_loss 0.024585985 val_loss 0.03176503
Epoch 1614: train_loss 0.024585182 val_loss 0.031764995
Epoch 1615: train_loss 0.02458438 val_loss 0.031764973
Epoch 1616: train_loss 0.024583586 val_loss 0.03176494
Epoch 1617: train_loss 0.024582788 val_loss 0.031764906
Epoch 1618: train_loss 0.024581991 val_loss 0.031764872
Epoch 1619: train_loss 0.0245812 val_loss 0.031764835
Epoch 1620: train_loss 0.024580406 val_loss 0.03176478
Epoch 1621: train_loss 0.024579613 val_loss 0.031764742
Epoch 1622: train_loss 0.02457882 val_loss 0.031764686
Epoch 1623: train_loss 0.024578031 val_loss 0.031764634
Epoch 1624: train_loss 0.02457724 val_loss 0.031764593
Epoch 1625: train_loss 0.024576453 val_loss 0.031764545
Epoch 1626: train_loss 0.024575667 val_loss 0.031764507
Epoch 1627: train_loss 0.02457488 val_loss 0.031764444
Epoch 1628: train_loss 0.024574095 val_loss 0.031764396
Epoch 1629: train_loss 0.024573311 val_loss 0.03176434
Epoch 1630: train_loss 0.024572529 val_loss 0.031764265
Epoch 1631: train_loss 0.024571745 val_loss 0.0317642
Epoch 1632: train_loss 0.024570964 val_loss 0.031764124
Epoch 1633: train_loss 0.024570186 val_loss 0.031764064
Epoch 1634: train_loss 0.024569405 val_loss 0.031764004
Epoch 1635: train_loss 0.024568629 val_loss 0.031763937
Epoch 1636: train_loss 0.02456785 val_loss 0.031763867
Epoch 1637: train_loss 0.024567075 val_loss 0.031763796
Epoch 1638: train_loss 0.024566302 val_loss 0.031763718
Epoch 1639: train_loss 0.024565527 val_loss 0.031763636
Epoch 1640: train_loss 0.024564752 val_loss 0.031763554
Epoch 1641: train_loss 0.024563983 val_loss 0.03176347
Epoch 1642: train_loss 0.02456321 val_loss 0.031763382
Epoch 1643: train_loss 0.02456244 val_loss 0.031763308
Epoch 1644: train_loss 0.024561672 val_loss 0.031763226
Epoch 1645: train_loss 0.024560902 val_loss 0.03176313
Epoch 1646: train_loss 0.024560135 val_loss 0.031763043
Epoch 1647: train_loss 0.024559371 val_loss 0.031762943
Epoch 1648: train_loss 0.024558606 val_loss 0.031762842
Epoch 1649: train_loss 0.024557842 val_loss 0.031762745
Epoch 1650: train_loss 0.024557078 val_loss 0.03176265
Epoch 1651: train_loss 0.024556315 val_loss 0.031762548
Epoch 1652: train_loss 0.024555556 val_loss 0.031762455
Epoch 1653: train_loss 0.024554793 val_loss 0.031762347
Epoch 1654: train_loss 0.024554035 val_loss 0.03176224
Epoch 1655: train_loss 0.024553277 val_loss 0.03176213
Epoch 1656: train_loss 0.024552517 val_loss 0.03176201
Epoch 1657: train_loss 0.024551762 val_loss 0.03176189
Epoch 1658: train_loss 0.024551006 val_loss 0.03176178
Epoch 1659: train_loss 0.024550252 val_loss 0.031761676
Epoch 1660: train_loss 0.0245495 val_loss 0.03176156
Epoch 1661: train_loss 0.024548745 val_loss 0.03176145
Epoch 1662: train_loss 0.024547994 val_loss 0.031761326
Epoch 1663: train_loss 0.024547242 val_loss 0.031761196
Epoch 1664: train_loss 0.024546491 val_loss 0.03176107
Epoch 1665: train_loss 0.024545744 val_loss 0.03176093
Epoch 1666: train_loss 0.024544995 val_loss 0.031760797
Epoch 1667: train_loss 0.024544246 val_loss 0.03176065
Epoch 1668: train_loss 0.024543501 val_loss 0.031760514
Epoch 1669: train_loss 0.024542756 val_loss 0.031760383
Epoch 1670: train_loss 0.024542011 val_loss 0.031760253
Epoch 1671: train_loss 0.024541268 val_loss 0.031760123
Epoch 1672: train_loss 0.024540527 val_loss 0.031759985
Epoch 1673: train_loss 0.024539785 val_loss 0.031759836
Epoch 1674: train_loss 0.024539042 val_loss 0.031759683
Epoch 1675: train_loss 0.024538301 val_loss 0.03175953
Epoch 1676: train_loss 0.024537561 val_loss 0.03175938
Epoch 1677: train_loss 0.024536824 val_loss 0.03175923
Epoch 1678: train_loss 0.024536084 val_loss 0.03175908
Epoch 1679: train_loss 0.024535349 val_loss 0.031758934
Epoch 1680: train_loss 0.024534613 val_loss 0.031758785
Epoch 1681: train_loss 0.024533879 val_loss 0.03175863
Epoch 1682: train_loss 0.024533141 val_loss 0.031758465
Epoch 1683: train_loss 0.024532413 val_loss 0.031758294
Epoch 1684: train_loss 0.024531677 val_loss 0.031758122
Epoch 1685: train_loss 0.024530947 val_loss 0.031757943
Epoch 1686: train_loss 0.024530217 val_loss 0.031757772
Epoch 1687: train_loss 0.024529485 val_loss 0.031757616
Epoch 1688: train_loss 0.024528757 val_loss 0.03175745
Epoch 1689: train_loss 0.02452803 val_loss 0.031757284
Epoch 1690: train_loss 0.024527302 val_loss 0.031757113
Epoch 1691: train_loss 0.024526576 val_loss 0.03175694
Epoch 1692: train_loss 0.024525851 val_loss 0.031756755
Epoch 1693: train_loss 0.024525125 val_loss 0.031756558
Epoch 1694: train_loss 0.024524402 val_loss 0.03175637
Epoch 1695: train_loss 0.024523677 val_loss 0.03175619
Epoch 1696: train_loss 0.024522956 val_loss 0.031756006
Epoch 1697: train_loss 0.024522236 val_loss 0.031755816
Epoch 1698: train_loss 0.024521513 val_loss 0.031755634
Epoch 1699: train_loss 0.024520796 val_loss 0.031755447
Epoch 1700: train_loss 0.024520075 val_loss 0.031755254
Epoch 1701: train_loss 0.024519356 val_loss 0.031755056
Epoch 1702: train_loss 0.024518639 val_loss 0.031754866
Epoch 1703: train_loss 0.024517924 val_loss 0.031754665
Epoch 1704: train_loss 0.024517206 val_loss 0.03175446
Epoch 1705: train_loss 0.024516491 val_loss 0.031754255
Epoch 1706: train_loss 0.024515778 val_loss 0.031754054
Epoch 1707: train_loss 0.024515063 val_loss 0.03175386
Epoch 1708: train_loss 0.02451435 val_loss 0.031753656
Epoch 1709: train_loss 0.02451364 val_loss 0.03175344
Epoch 1710: train_loss 0.024512928 val_loss 0.031753235
Epoch 1711: train_loss 0.024512218 val_loss 0.031753022
Epoch 1712: train_loss 0.024511507 val_loss 0.03175281
Epoch 1713: train_loss 0.024510799 val_loss 0.031752586
Epoch 1714: train_loss 0.024510093 val_loss 0.03175236
Epoch 1715: train_loss 0.024509383 val_loss 0.031752136
Epoch 1716: train_loss 0.024508677 val_loss 0.03175192
Epoch 1717: train_loss 0.024507973 val_loss 0.031751696
Epoch 1718: train_loss 0.024507264 val_loss 0.03175146
Epoch 1719: train_loss 0.024506561 val_loss 0.03175123
Epoch 1720: train_loss 0.024505857 val_loss 0.031751007
Epoch 1721: train_loss 0.024505153 val_loss 0.031750772
Epoch 1722: train_loss 0.024504453 val_loss 0.03175054
Epoch 1723: train_loss 0.024503753 val_loss 0.0317503
Epoch 1724: train_loss 0.02450305 val_loss 0.031750064
Epoch 1725: train_loss 0.02450235 val_loss 0.031749822
Epoch 1726: train_loss 0.024501653 val_loss 0.031749584
Epoch 1727: train_loss 0.024500953 val_loss 0.03174934
Epoch 1728: train_loss 0.024500255 val_loss 0.031749096
Epoch 1729: train_loss 0.024499558 val_loss 0.031748854
Epoch 1730: train_loss 0.02449886 val_loss 0.031748608
Epoch 1731: train_loss 0.024498167 val_loss 0.031748362
Epoch 1732: train_loss 0.02449747 val_loss 0.03174812
Epoch 1733: train_loss 0.024496779 val_loss 0.031747866
Epoch 1734: train_loss 0.024496084 val_loss 0.03174761
Epoch 1735: train_loss 0.024495393 val_loss 0.031747345
Epoch 1736: train_loss 0.024494698 val_loss 0.03174708
Epoch 1737: train_loss 0.02449401 val_loss 0.031746812
Epoch 1738: train_loss 0.024493318 val_loss 0.031746548
Epoch 1739: train_loss 0.024492629 val_loss 0.031746283
Epoch 1740: train_loss 0.02449194 val_loss 0.031746015
Epoch 1741: train_loss 0.02449125 val_loss 0.031745736
Epoch 1742: train_loss 0.024490561 val_loss 0.031745467
Epoch 1743: train_loss 0.024489876 val_loss 0.031745207
Epoch 1744: train_loss 0.024489189 val_loss 0.031744935
Epoch 1745: train_loss 0.024488501 val_loss 0.03174467
Epoch 1746: train_loss 0.024487818 val_loss 0.031744394
Epoch 1747: train_loss 0.024487134 val_loss 0.03174411
Epoch 1748: train_loss 0.02448645 val_loss 0.031743824
Epoch 1749: train_loss 0.024485767 val_loss 0.031743545
Epoch 1750: train_loss 0.024485085 val_loss 0.03174326
Epoch 1751: train_loss 0.024484402 val_loss 0.031742964
Epoch 1752: train_loss 0.024483724 val_loss 0.03174267
Epoch 1753: train_loss 0.02448304 val_loss 0.03174237
Epoch 1754: train_loss 0.024482362 val_loss 0.031742077
Epoch 1755: train_loss 0.024481684 val_loss 0.031741776
Epoch 1756: train_loss 0.024481006 val_loss 0.031741485
Epoch 1757: train_loss 0.024480326 val_loss 0.0317412
Epoch 1758: train_loss 0.02447965 val_loss 0.0317409
Epoch 1759: train_loss 0.024478972 val_loss 0.031740606
Epoch 1760: train_loss 0.024478298 val_loss 0.031740308
Epoch 1761: train_loss 0.024477623 val_loss 0.03174001
Epoch 1762: train_loss 0.02447695 val_loss 0.031739708
Epoch 1763: train_loss 0.024476273 val_loss 0.03173939
Epoch 1764: train_loss 0.024475602 val_loss 0.031739086
Epoch 1765: train_loss 0.024474928 val_loss 0.031738777
Epoch 1766: train_loss 0.024474258 val_loss 0.031738468
Epoch 1767: train_loss 0.024473585 val_loss 0.031738166
Epoch 1768: train_loss 0.024472915 val_loss 0.03173785
Epoch 1769: train_loss 0.024472244 val_loss 0.03173753
Epoch 1770: train_loss 0.024471575 val_loss 0.031737193
Epoch 1771: train_loss 0.024470905 val_loss 0.031736866
Epoch 1772: train_loss 0.024470238 val_loss 0.031736534
Epoch 1773: train_loss 0.02446957 val_loss 0.03173621
Epoch 1774: train_loss 0.024468902 val_loss 0.031735882
Epoch 1775: train_loss 0.024468238 val_loss 0.03173556
Epoch 1776: train_loss 0.024467573 val_loss 0.031735227
Epoch 1777: train_loss 0.024466906 val_loss 0.031734888
Epoch 1778: train_loss 0.02446624 val_loss 0.031734552
Epoch 1779: train_loss 0.02446558 val_loss 0.031734213
Epoch 1780: train_loss 0.024464915 val_loss 0.031733878
Epoch 1781: train_loss 0.024464251 val_loss 0.03173354
Epoch 1782: train_loss 0.024463588 val_loss 0.031733204
Epoch 1783: train_loss 0.024462927 val_loss 0.031732857
Epoch 1784: train_loss 0.024462266 val_loss 0.031732507
Epoch 1785: train_loss 0.024461607 val_loss 0.031732157
Epoch 1786: train_loss 0.024460945 val_loss 0.031731807
Epoch 1787: train_loss 0.024460288 val_loss 0.03173146
Epoch 1788: train_loss 0.024459627 val_loss 0.031731114
Epoch 1789: train_loss 0.024458969 val_loss 0.03173076
Epoch 1790: train_loss 0.02445831 val_loss 0.031730413
Epoch 1791: train_loss 0.024457656 val_loss 0.031730056
Epoch 1792: train_loss 0.024456998 val_loss 0.0317297
Epoch 1793: train_loss 0.02445634 val_loss 0.031729326
Epoch 1794: train_loss 0.024455687 val_loss 0.03172896
Epoch 1795: train_loss 0.024455033 val_loss 0.031728595
Epoch 1796: train_loss 0.024454378 val_loss 0.03172823
Epoch 1797: train_loss 0.024453722 val_loss 0.03172786
Epoch 1798: train_loss 0.024453072 val_loss 0.031727497
Epoch 1799: train_loss 0.024452418 val_loss 0.031727128
Epoch 1800: train_loss 0.024451766 val_loss 0.031726763
Epoch 1801: train_loss 0.024451116 val_loss 0.031726383
Epoch 1802: train_loss 0.024450464 val_loss 0.03172601
Epoch 1803: train_loss 0.024449816 val_loss 0.031725626
Epoch 1804: train_loss 0.024449164 val_loss 0.031725246
Epoch 1805: train_loss 0.024448518 val_loss 0.03172486
Epoch 1806: train_loss 0.024447868 val_loss 0.03172448
Epoch 1807: train_loss 0.02444722 val_loss 0.0317241
Epoch 1808: train_loss 0.024446571 val_loss 0.031723727
Epoch 1809: train_loss 0.024445923 val_loss 0.031723347
Epoch 1810: train_loss 0.024445279 val_loss 0.031722963
Epoch 1811: train_loss 0.02444463 val_loss 0.031722575
Epoch 1812: train_loss 0.024443984 val_loss 0.031722173
Epoch 1813: train_loss 0.024443341 val_loss 0.03172177
Epoch 1814: train_loss 0.024442697 val_loss 0.03172138
Epoch 1815: train_loss 0.024442052 val_loss 0.031720977
Epoch 1816: train_loss 0.02444141 val_loss 0.03172059
Epoch 1817: train_loss 0.024440765 val_loss 0.031720188
Epoch 1818: train_loss 0.024440123 val_loss 0.0317198
Epoch 1819: train_loss 0.024439482 val_loss 0.0317194
Epoch 1820: train_loss 0.024438841 val_loss 0.031719
Epoch 1821: train_loss 0.024438199 val_loss 0.031718582
Epoch 1822: train_loss 0.024437558 val_loss 0.031718172
Epoch 1823: train_loss 0.024436919 val_loss 0.03171776
Epoch 1824: train_loss 0.02443628 val_loss 0.031717353
Epoch 1825: train_loss 0.02443564 val_loss 0.03171695
Epoch 1826: train_loss 0.024435002 val_loss 0.031716537
Epoch 1827: train_loss 0.024434365 val_loss 0.031716116
Epoch 1828: train_loss 0.024433728 val_loss 0.031715706
Epoch 1829: train_loss 0.02443309 val_loss 0.031715285
Epoch 1830: train_loss 0.024432452 val_loss 0.03171486
Epoch 1831: train_loss 0.02443182 val_loss 0.031714436
Epoch 1832: train_loss 0.02443118 val_loss 0.031714007
Epoch 1833: train_loss 0.024430545 val_loss 0.03171358
Epoch 1834: train_loss 0.024429914 val_loss 0.031713154
Epoch 1835: train_loss 0.02442928 val_loss 0.031712726
Epoch 1836: train_loss 0.024428645 val_loss 0.031712282
Epoch 1837: train_loss 0.024428012 val_loss 0.03171185
Epoch 1838: train_loss 0.024427379 val_loss 0.031711414
Epoch 1839: train_loss 0.024426747 val_loss 0.031710982
Epoch 1840: train_loss 0.024426116 val_loss 0.031710546
Epoch 1841: train_loss 0.024425484 val_loss 0.031710118
Epoch 1842: train_loss 0.024424853 val_loss 0.031709675
Epoch 1843: train_loss 0.024424223 val_loss 0.031709228
Epoch 1844: train_loss 0.024423592 val_loss 0.031708773
Epoch 1845: train_loss 0.024422964 val_loss 0.031708334
Epoch 1846: train_loss 0.024422333 val_loss 0.031707894
Epoch 1847: train_loss 0.024421705 val_loss 0.031707447
Epoch 1848: train_loss 0.02442108 val_loss 0.031706996
Epoch 1849: train_loss 0.02442045 val_loss 0.03170655
Epoch 1850: train_loss 0.024419824 val_loss 0.03170609
Epoch 1851: train_loss 0.024419196 val_loss 0.031705633
Epoch 1852: train_loss 0.02441857 val_loss 0.03170517
Epoch 1853: train_loss 0.024417944 val_loss 0.031704705
Epoch 1854: train_loss 0.024417318 val_loss 0.03170425
Epoch 1855: train_loss 0.024416693 val_loss 0.031703785
Epoch 1856: train_loss 0.02441607 val_loss 0.031703316
Epoch 1857: train_loss 0.024415446 val_loss 0.03170286
Epoch 1858: train_loss 0.024414822 val_loss 0.031702388
Epoch 1859: train_loss 0.024414197 val_loss 0.03170192
Epoch 1860: train_loss 0.024413574 val_loss 0.03170145
Epoch 1861: train_loss 0.024412954 val_loss 0.031700972
Epoch 1862: train_loss 0.024412332 val_loss 0.031700477
Epoch 1863: train_loss 0.024411712 val_loss 0.03170001
Epoch 1864: train_loss 0.024411088 val_loss 0.03169954
Epoch 1865: train_loss 0.02441047 val_loss 0.031699065
Epoch 1866: train_loss 0.024409847 val_loss 0.031698585
Epoch 1867: train_loss 0.024409227 val_loss 0.031698097
Epoch 1868: train_loss 0.024408609 val_loss 0.031697612
Epoch 1869: train_loss 0.024407988 val_loss 0.031697128
Epoch 1870: train_loss 0.02440737 val_loss 0.031696636
Epoch 1871: train_loss 0.024406753 val_loss 0.031696152
Epoch 1872: train_loss 0.024406133 val_loss 0.03169566
Epoch 1873: train_loss 0.024405517 val_loss 0.031695172
Epoch 1874: train_loss 0.0244049 val_loss 0.031694684
Epoch 1875: train_loss 0.024404284 val_loss 0.03169419
Epoch 1876: train_loss 0.024403667 val_loss 0.031693686
Epoch 1877: train_loss 0.02440305 val_loss 0.03169318
Epoch 1878: train_loss 0.024402438 val_loss 0.031692673
Epoch 1879: train_loss 0.024401821 val_loss 0.03169217
Epoch 1880: train_loss 0.024401207 val_loss 0.031691667
Epoch 1881: train_loss 0.024400592 val_loss 0.03169117
Epoch 1882: train_loss 0.024399977 val_loss 0.031690676
Epoch 1883: train_loss 0.024399366 val_loss 0.031690177
Epoch 1884: train_loss 0.024398752 val_loss 0.031689666
Epoch 1885: train_loss 0.024398139 val_loss 0.031689145
Epoch 1886: train_loss 0.024397528 val_loss 0.031688627
Epoch 1887: train_loss 0.024396917 val_loss 0.031688113
Epoch 1888: train_loss 0.0243963 val_loss 0.031687595
Epoch 1889: train_loss 0.024395691 val_loss 0.031687085
Epoch 1890: train_loss 0.02439508 val_loss 0.031686578
Epoch 1891: train_loss 0.02439447 val_loss 0.03168606
Epoch 1892: train_loss 0.024393858 val_loss 0.031685535
Epoch 1893: train_loss 0.024393251 val_loss 0.03168501
Epoch 1894: train_loss 0.024392642 val_loss 0.031684484
Epoch 1895: train_loss 0.024392033 val_loss 0.031683944
Epoch 1896: train_loss 0.024391424 val_loss 0.03168341
Epoch 1897: train_loss 0.024390817 val_loss 0.031682882
Epoch 1898: train_loss 0.02439021 val_loss 0.03168236
Epoch 1899: train_loss 0.024389599 val_loss 0.031681836
Epoch 1900: train_loss 0.024388993 val_loss 0.031681314
Epoch 1901: train_loss 0.024388386 val_loss 0.03168078
Epoch 1902: train_loss 0.024387779 val_loss 0.03168024
Epoch 1903: train_loss 0.024387175 val_loss 0.031679705
Epoch 1904: train_loss 0.02438657 val_loss 0.031679157
Epoch 1905: train_loss 0.024385963 val_loss 0.03167862
Epoch 1906: train_loss 0.024385357 val_loss 0.03167808
Epoch 1907: train_loss 0.024384754 val_loss 0.031677544
Epoch 1908: train_loss 0.024384148 val_loss 0.031677004
Epoch 1909: train_loss 0.024383543 val_loss 0.03167646
Epoch 1910: train_loss 0.024382941 val_loss 0.031675905
Epoch 1911: train_loss 0.024382336 val_loss 0.031675365
Epoch 1912: train_loss 0.024381734 val_loss 0.031674806
Epoch 1913: train_loss 0.024381131 val_loss 0.031674244
Epoch 1914: train_loss 0.02438053 val_loss 0.031673692
Epoch 1915: train_loss 0.024379928 val_loss 0.03167314
Epoch 1916: train_loss 0.024379326 val_loss 0.031672593
Epoch 1917: train_loss 0.024378724 val_loss 0.031672046
Epoch 1918: train_loss 0.024378125 val_loss 0.03167149
Epoch 1919: train_loss 0.024377523 val_loss 0.031670928
Epoch 1920: train_loss 0.024376921 val_loss 0.031670365
Epoch 1921: train_loss 0.024376323 val_loss 0.031669796
Epoch 1922: train_loss 0.024375722 val_loss 0.03166923
Epoch 1923: train_loss 0.024375124 val_loss 0.031668656
Epoch 1924: train_loss 0.024374524 val_loss 0.03166809
Epoch 1925: train_loss 0.024373928 val_loss 0.031667512
Epoch 1926: train_loss 0.024373326 val_loss 0.03166695
Epoch 1927: train_loss 0.02437273 val_loss 0.031666372
Epoch 1928: train_loss 0.02437213 val_loss 0.03166581
Epoch 1929: train_loss 0.024371533 val_loss 0.03166523
Epoch 1930: train_loss 0.024370937 val_loss 0.03166465
Epoch 1931: train_loss 0.024370339 val_loss 0.031664077
Epoch 1932: train_loss 0.024369743 val_loss 0.0316635
Epoch 1933: train_loss 0.024369147 val_loss 0.03166293
Epoch 1934: train_loss 0.02436855 val_loss 0.031662345
Epoch 1935: train_loss 0.024367956 val_loss 0.03166176
Epoch 1936: train_loss 0.024367359 val_loss 0.03166116
Epoch 1937: train_loss 0.024366762 val_loss 0.03166057
Epoch 1938: train_loss 0.02436617 val_loss 0.031659983
Epoch 1939: train_loss 0.024365574 val_loss 0.0316594
Epoch 1940: train_loss 0.02436498 val_loss 0.031658813
Epoch 1941: train_loss 0.024364386 val_loss 0.03165823
Epoch 1942: train_loss 0.024363793 val_loss 0.031657636
Epoch 1943: train_loss 0.0243632 val_loss 0.03165703
Epoch 1944: train_loss 0.024362607 val_loss 0.031656433
Epoch 1945: train_loss 0.024362016 val_loss 0.031655833
Epoch 1946: train_loss 0.024361424 val_loss 0.031655245
Epoch 1947: train_loss 0.024360828 val_loss 0.031654645
Epoch 1948: train_loss 0.024360238 val_loss 0.03165404
Epoch 1949: train_loss 0.024359649 val_loss 0.03165344
Epoch 1950: train_loss 0.024359057 val_loss 0.031652834
Epoch 1951: train_loss 0.024358463 val_loss 0.03165223
Epoch 1952: train_loss 0.024357872 val_loss 0.031651627
Epoch 1953: train_loss 0.024357283 val_loss 0.031651016
Epoch 1954: train_loss 0.024356693 val_loss 0.03165041
Epoch 1955: train_loss 0.024356103 val_loss 0.03164979
Epoch 1956: train_loss 0.024355512 val_loss 0.031649172
Epoch 1957: train_loss 0.024354924 val_loss 0.03164856
Epoch 1958: train_loss 0.024354333 val_loss 0.031647947
Epoch 1959: train_loss 0.024353746 val_loss 0.031647325
Epoch 1960: train_loss 0.024353156 val_loss 0.031646714
Epoch 1961: train_loss 0.02435257 val_loss 0.031646088
Epoch 1962: train_loss 0.02435198 val_loss 0.031645466
Epoch 1963: train_loss 0.024351396 val_loss 0.03164484
Epoch 1964: train_loss 0.024350805 val_loss 0.031644214
Epoch 1965: train_loss 0.02435022 val_loss 0.031643584
Epoch 1966: train_loss 0.024349632 val_loss 0.031642947
Epoch 1967: train_loss 0.024349045 val_loss 0.031642314
Epoch 1968: train_loss 0.024348458 val_loss 0.031641677
Epoch 1969: train_loss 0.024347872 val_loss 0.03164105
Epoch 1970: train_loss 0.024347285 val_loss 0.031640418
Epoch 1971: train_loss 0.024346702 val_loss 0.03163979
Epoch 1972: train_loss 0.024346117 val_loss 0.03163916
Epoch 1973: train_loss 0.02434553 val_loss 0.031638507
Epoch 1974: train_loss 0.024344945 val_loss 0.03163785
Epoch 1975: train_loss 0.024344359 val_loss 0.031637207
Epoch 1976: train_loss 0.024343776 val_loss 0.031636566
Epoch 1977: train_loss 0.024343193 val_loss 0.031635933
Epoch 1978: train_loss 0.02434261 val_loss 0.031635303
Epoch 1979: train_loss 0.024342025 val_loss 0.031634662
Epoch 1980: train_loss 0.02434144 val_loss 0.03163402
Epoch 1981: train_loss 0.024340857 val_loss 0.03163337
Epoch 1982: train_loss 0.024340276 val_loss 0.031632714
Epoch 1983: train_loss 0.02433969 val_loss 0.031632055
Epoch 1984: train_loss 0.02433911 val_loss 0.0316314
Epoch 1985: train_loss 0.024338529 val_loss 0.03163075
Epoch 1986: train_loss 0.024337944 val_loss 0.031630103
Epoch 1987: train_loss 0.024337362 val_loss 0.031629443
Epoch 1988: train_loss 0.024336781 val_loss 0.031628784
Epoch 1989: train_loss 0.0243362 val_loss 0.031628117
Epoch 1990: train_loss 0.024335619 val_loss 0.03162745
Epoch 1991: train_loss 0.024335038 val_loss 0.031626787
Epoch 1992: train_loss 0.024334457 val_loss 0.03162613
Epoch 1993: train_loss 0.024333876 val_loss 0.031625472
Epoch 1994: train_loss 0.024333296 val_loss 0.031624813
Epoch 1995: train_loss 0.024332717 val_loss 0.031624146
Epoch 1996: train_loss 0.024332138 val_loss 0.031623464
Epoch 1997: train_loss 0.024331557 val_loss 0.031622786
Epoch 1998: train_loss 0.024330977 val_loss 0.03162211
Epoch 1999: train_loss 0.024330398 val_loss 0.03162144
ttest: -20.44754300500156 pValue 6.263156975900275e-78
Epoch 2000: train_loss 0.024329819 val_loss 0.031620774
ttest: -20.40340914664759 pValue 1.1852284021154698e-77
Epoch 2001: train_loss 0.024329241 val_loss 0.0316201
ttest: -20.358726767868557 pValue 2.259473670111399e-77
Epoch 2002: train_loss 0.024328662 val_loss 0.03161943
ttest: -20.313569752631942 pValue 4.334469056082909e-77
Epoch 2003: train_loss 0.024328085 val_loss 0.031618748
ttest: -20.26791875505722 pValue 8.369492075798947e-77
Epoch 2004: train_loss 0.024327507 val_loss 0.031618062
ttest: -20.22179417480332 pValue 1.626142074792557e-76
Epoch 2005: train_loss 0.024326928 val_loss 0.03161737
ttest: -20.17541969587137 pValue 3.168881921843176e-76
Epoch 2006: train_loss 0.024326352 val_loss 0.031616688
ttest: -20.12905595937135 pValue 6.170328655674654e-76
Epoch 2007: train_loss 0.024325773 val_loss 0.03161601
ttest: -20.082805038759773 pValue 1.1987431542194594e-75
Epoch 2008: train_loss 0.024325194 val_loss 0.031615324
ttest: -20.037009655704555 pValue 2.3121957856808866e-75
Epoch 2009: train_loss 0.024324618 val_loss 0.031614646
ttest: -19.99165729868659 pValue 4.428808708558921e-75
Epoch 2010: train_loss 0.024324043 val_loss 0.031613976
ttest: -19.94712046818155 pValue 8.379216984858933e-75
Epoch 2011: train_loss 0.024323465 val_loss 0.03161329
ttest: -19.90347933328516 pValue 1.5641864404920455e-74
Epoch 2012: train_loss 0.02432289 val_loss 0.031612612
ttest: -19.86073123183964 pValue 2.881192932028271e-74
Epoch 2013: train_loss 0.024322314 val_loss 0.031611912
ttest: -19.81925967665046 pValue 5.208247204479275e-74
Epoch 2014: train_loss 0.024321739 val_loss 0.031611223
ttest: -19.77892561714024 pValue 9.258214111054991e-74
Epoch 2015: train_loss 0.024321161 val_loss 0.03161053
ttest: -19.739852637834463 pValue 1.6155929415537083e-73
Epoch 2016: train_loss 0.024320586 val_loss 0.03160984
ttest: -19.702174804501585 pValue 2.7624599564761706e-73
Epoch 2017: train_loss 0.024320008 val_loss 0.031609144
ttest: -19.66589476814492 pValue 4.628321894349566e-73
Epoch 2018: train_loss 0.024319436 val_loss 0.03160845
ttest: -19.63102246222226 pValue 7.597547828302703e-73
Epoch 2019: train_loss 0.024318863 val_loss 0.03160776
ttest: -19.597533466024746 pValue 1.2224073191332607e-72
Epoch 2020: train_loss 0.024318285 val_loss 0.031607054
ttest: -19.565554424111248 pValue 1.9243734246886857e-72
Epoch 2021: train_loss 0.024317712 val_loss 0.03160636
ttest: -19.53478854512677 pValue 2.976743926450584e-72
Epoch 2022: train_loss 0.024317136 val_loss 0.03160566
ttest: -19.505297003299596 pValue 4.5207599512382425e-72
Epoch 2023: train_loss 0.02431656 val_loss 0.031604953
ttest: -19.476924343637343 pValue 6.755716239183822e-72
Epoch 2024: train_loss 0.02431599 val_loss 0.031604253
ttest: -19.449524494817446 pValue 9.954810209332855e-72
Epoch 2025: train_loss 0.024315411 val_loss 0.031603552
ttest: -19.42280485978457 pValue 1.4524642631513544e-71
Epoch 2026: train_loss 0.024314841 val_loss 0.03160285
ttest: -19.39667945958221 pValue 2.1009948005911085e-71
Epoch 2027: train_loss 0.024314266 val_loss 0.031602133
ttest: -19.370906427916086 pValue 3.0233110757194862e-71
Epoch 2028: train_loss 0.02431369 val_loss 0.03160143
ttest: -19.34532405107563 pValue 4.337856476099871e-71
Epoch 2029: train_loss 0.02431312 val_loss 0.031600736
ttest: -19.319696521058543 pValue 6.226583504427285e-71
Epoch 2030: train_loss 0.024312546 val_loss 0.03160003
ttest: -19.29406460823518 pValue 8.936284133593207e-71
Epoch 2031: train_loss 0.024311977 val_loss 0.031599317
ttest: -19.267956413718792 pValue 1.2908788950002586e-70
Epoch 2032: train_loss 0.024311403 val_loss 0.031598594
ttest: -19.24151775512248 pValue 1.8730018651363783e-70
Epoch 2033: train_loss 0.024310827 val_loss 0.03159788
ttest: -19.214653689767733 pValue 2.7333285990223634e-70
Epoch 2034: train_loss 0.024310257 val_loss 0.03159716
ttest: -19.187207431320356 pValue 4.02067837963641e-70
Epoch 2035: train_loss 0.024309685 val_loss 0.031596445
ttest: -19.159280263685297 pValue 5.952990690690117e-70
Epoch 2036: train_loss 0.024309114 val_loss 0.031595733
ttest: -19.13071813965502 pValue 8.890644416286119e-70
Epoch 2037: train_loss 0.024308542 val_loss 0.031595014
ttest: -19.101507069022013 pValue 1.3395856275069334e-69
Epoch 2038: train_loss 0.024307968 val_loss 0.03159429
ttest: -19.071654481807204 pValue 2.0360710267529108e-69
Epoch 2039: train_loss 0.0243074 val_loss 0.031593572
ttest: -19.041301863279543 pValue 3.115518070450408e-69
Epoch 2040: train_loss 0.024306826 val_loss 0.03159285
ttest: -19.01037326589739 pValue 4.804379417338544e-69
Epoch 2041: train_loss 0.024306258 val_loss 0.031592127
ttest: -18.978863445981244 pValue 7.466848528135195e-69
Epoch 2042: train_loss 0.024305686 val_loss 0.031591397
ttest: -18.946777121422905 pValue 1.169480276331631e-68
Epoch 2043: train_loss 0.024305115 val_loss 0.03159067
ttest: -18.91424206991191 pValue 1.842550780138312e-68
Epoch 2044: train_loss 0.024304545 val_loss 0.031589948
ttest: -18.881215546054307 pValue 2.921924074234743e-68
Epoch 2045: train_loss 0.024303973 val_loss 0.031589214
ttest: -18.847885157447696 pValue 4.651536241273673e-68
Epoch 2046: train_loss 0.024303403 val_loss 0.031588472
ttest: -18.814023453306575 pValue 7.457161694145668e-68
Epoch 2047: train_loss 0.02430283 val_loss 0.03158773
ttest: -18.779814241040413 pValue 1.2008244119086461e-67
Epoch 2048: train_loss 0.024302263 val_loss 0.031586993
ttest: -18.745315053975364 pValue 1.9407085576979096e-67
Epoch 2049: train_loss 0.024301695 val_loss 0.03158626
ttest: -18.71045451459277 pValue 3.15095061255287e-67
Epoch 2050: train_loss 0.024301125 val_loss 0.031585526
ttest: -18.675153150203194 pValue 5.145127846720623e-67
Epoch 2051: train_loss 0.024300557 val_loss 0.03158479
ttest: -18.63963698276512 pValue 8.422744664471208e-67
Epoch 2052: train_loss 0.024299985 val_loss 0.03158405
ttest: -18.60387608994825 pValue 1.3828968712131585e-66
Epoch 2053: train_loss 0.024299417 val_loss 0.03158332
ttest: -18.567724376614226 pValue 2.2818057021960174e-66
Epoch 2054: train_loss 0.024298849 val_loss 0.031582575
ttest: -18.531463102420627 pValue 3.768965359918888e-66
Epoch 2055: train_loss 0.02429828 val_loss 0.03158185
ttest: -18.494947171961005 pValue 6.244372072805987e-66
Epoch 2056: train_loss 0.02429771 val_loss 0.031581104
ttest: -18.458276782467316 pValue 1.036267977362382e-65
Epoch 2057: train_loss 0.024297142 val_loss 0.031580362
ttest: -18.42118048223004 pValue 1.7290014293989388e-65
Epoch 2058: train_loss 0.024296576 val_loss 0.031579625
ttest: -18.384088866698793 pValue 2.88318068905149e-65
Epoch 2059: train_loss 0.024296006 val_loss 0.031578884
ttest: -18.346731310787803 pValue 4.823024210379039e-65
Epoch 2060: train_loss 0.024295438 val_loss 0.031578127
ttest: -18.309286282898476 pValue 8.073573693578352e-65
Epoch 2061: train_loss 0.02429487 val_loss 0.031577367
ttest: -18.27167683595341 pValue 1.3538410980901617e-64
Epoch 2062: train_loss 0.024294302 val_loss 0.031576604
ttest: -18.23395154190606 pValue 2.2726436951323413e-64
Epoch 2063: train_loss 0.024293734 val_loss 0.031575862
ttest: -18.196160371727967 pValue 3.816417849144228e-64
Epoch 2064: train_loss 0.02429317 val_loss 0.031575106
ttest: -18.158345044835084 pValue 6.407532348949893e-64
Epoch 2065: train_loss 0.024292601 val_loss 0.03157436
ttest: -18.120417823526342 pValue 1.0768522003056546e-63
Epoch 2066: train_loss 0.024292033 val_loss 0.031573612
ttest: -18.08254507728508 pValue 1.8074262997965765e-63
Epoch 2067: train_loss 0.024291467 val_loss 0.03157285
ttest: -18.044555235959628 pValue 3.0368414345446856e-63
Epoch 2068: train_loss 0.024290899 val_loss 0.03157208
ttest: -18.006541947321104 pValue 5.101317883282431e-63
Epoch 2069: train_loss 0.024290333 val_loss 0.031571325
ttest: -17.968519094482374 pValue 8.565599631800951e-63
Epoch 2070: train_loss 0.024289766 val_loss 0.031570565
ttest: -17.93056503688837 pValue 1.4360965996997296e-62
Epoch 2071: train_loss 0.0242892 val_loss 0.031569812
ttest: -17.8926194526725 pValue 2.4061168125960022e-62
Epoch 2072: train_loss 0.024288634 val_loss 0.031569052
ttest: -17.85461347851057 pValue 4.032389022382207e-62
Epoch 2073: train_loss 0.024288068 val_loss 0.031568296
ttest: -17.816536533206428 pValue 6.760535227842035e-62
Epoch 2074: train_loss 0.024287505 val_loss 0.031567536
ttest: -17.77865386728282 pValue 1.1298179960939716e-61
Epoch 2075: train_loss 0.024286937 val_loss 0.031566765
ttest: -17.740606815005535 pValue 1.891285046099028e-61
Epoch 2076: train_loss 0.02428637 val_loss 0.03156599
ttest: -17.70258525246222 pValue 3.1630559317682354e-61
Epoch 2077: train_loss 0.024285806 val_loss 0.031565223
ttest: -17.664496112142324 pValue 5.291805767802676e-61
Epoch 2078: train_loss 0.02428524 val_loss 0.03156445
ttest: -17.626391336730315 pValue 8.849958707876671e-61
Epoch 2079: train_loss 0.024284674 val_loss 0.031563684
ttest: -17.588318707018708 pValue 1.4785563812134815e-60
Epoch 2080: train_loss 0.02428411 val_loss 0.031562913
ttest: -17.550049202467687 pValue 2.475329580435003e-60
Epoch 2081: train_loss 0.024283545 val_loss 0.031562153
ttest: -17.51177359341508 pValue 4.1419727787974586e-60
Epoch 2082: train_loss 0.02428298 val_loss 0.031561375
ttest: -17.4734052890372 pValue 6.935291475825224e-60
Epoch 2083: train_loss 0.024282414 val_loss 0.03156059
ttest: -17.435003532201947 pValue 1.1610669628396945e-59
Epoch 2084: train_loss 0.024281852 val_loss 0.031559795
ttest: -17.396411274467944 pValue 1.9475900278334714e-59
Epoch 2085: train_loss 0.024281288 val_loss 0.03155902
ttest: -17.357762752540495 pValue 3.267383107662778e-59
Epoch 2086: train_loss 0.024280721 val_loss 0.031558245
ttest: -17.31912498917275 pValue 5.477385550817199e-59
Epoch 2087: train_loss 0.024280155 val_loss 0.03155748
ttest: -17.280350411240825 pValue 9.193325393339935e-59
Epoch 2088: train_loss 0.024279594 val_loss 0.031556707
ttest: -17.24150699366967 pValue 1.543477218208189e-58
Epoch 2089: train_loss 0.02427903 val_loss 0.03155593
ttest: -17.202301190996046 pValue 2.602270717604651e-58
Epoch 2090: train_loss 0.024278468 val_loss 0.031555146
ttest: -17.163175236293696 pValue 4.379901219500459e-58
Epoch 2091: train_loss 0.024277903 val_loss 0.03155436
ttest: -17.123981309704238 pValue 7.3737729215359e-58
Epoch 2092: train_loss 0.024277337 val_loss 0.031553574
ttest: -17.084648151873356 pValue 1.2429026618019081e-57
Epoch 2093: train_loss 0.024276776 val_loss 0.031552788
ttest: -17.04510325511597 pValue 2.0995193026471034e-57
Epoch 2094: train_loss 0.024276212 val_loss 0.031552006
ttest: -17.005572651335477 pValue 3.543498289209015e-57
Epoch 2095: train_loss 0.02427565 val_loss 0.031551227
ttest: -16.96591049028343 pValue 5.98702645087675e-57
Epoch 2096: train_loss 0.024275087 val_loss 0.031550437
ttest: -16.926117142653407 pValue 1.012629394650912e-56
Epoch 2097: train_loss 0.024274524 val_loss 0.031549644
ttest: -16.88627108659814 pValue 1.712760300997258e-56
Epoch 2098: train_loss 0.02427396 val_loss 0.03154885
ttest: -16.846298743323377 pValue 2.8997997910546443e-56
Epoch 2099: train_loss 0.024273397 val_loss 0.031548057
ttest: -16.806199908602636 pValue 4.914303689670352e-56
Epoch 2100: train_loss 0.024272835 val_loss 0.03154726
ttest: -16.765976853855634 pValue 8.336072619940375e-56
Epoch 2101: train_loss 0.024272272 val_loss 0.031546474
ttest: -16.725554962091962 pValue 1.4167322906927896e-55
Epoch 2102: train_loss 0.024271712 val_loss 0.031545687
ttest: -16.685166915341007 pValue 2.404975785164679e-55
Epoch 2103: train_loss 0.024271145 val_loss 0.031544894
ttest: -16.644585913109832 pValue 4.089959097891092e-55
Epoch 2104: train_loss 0.024270585 val_loss 0.031544104
ttest: -16.603968054830833 pValue 6.953776786724142e-55
Epoch 2105: train_loss 0.024270024 val_loss 0.031543314
ttest: -16.56316117199533 pValue 1.1843404788427773e-54
Epoch 2106: train_loss 0.024269463 val_loss 0.03154251
ttest: -16.52240949251043 pValue 2.014179342500528e-54
Epoch 2107: train_loss 0.024268901 val_loss 0.03154171
ttest: -16.481398177776974 pValue 3.4345139573668687e-54
Epoch 2108: train_loss 0.02426834 val_loss 0.03154091
ttest: -16.440292530476796 pValue 5.859187934661258e-54
Epoch 2109: train_loss 0.024267776 val_loss 0.03154011
ttest: -16.399100304858575 pValue 9.99924620311046e-54
Epoch 2110: train_loss 0.024267217 val_loss 0.031539313
ttest: -16.35790978672967 pValue 1.7051179213565392e-53
Epoch 2111: train_loss 0.024266655 val_loss 0.031538513
ttest: -16.31656724927463 pValue 2.911130649804745e-53
Epoch 2112: train_loss 0.024266094 val_loss 0.0315377
ttest: -16.274997201907897 pValue 4.980899223666599e-53
Epoch 2113: train_loss 0.024265531 val_loss 0.03153689
ttest: -16.23345294958137 pValue 8.512673570339734e-53
Epoch 2114: train_loss 0.024264973 val_loss 0.031536087
ttest: -16.191861907077275 pValue 1.454594264940648e-52
Epoch 2115: train_loss 0.02426441 val_loss 0.031535286
ttest: -16.15006529986696 pValue 2.4901191752664274e-52
Epoch 2116: train_loss 0.02426385 val_loss 0.03153447
ttest: -16.108235263417136 pValue 4.2612180052929335e-52
Epoch 2117: train_loss 0.024263289 val_loss 0.031533666
ttest: -16.0662119156482 pValue 7.304176972811934e-52
Epoch 2118: train_loss 0.024262726 val_loss 0.031532854
ttest: -16.02425357836757 pValue 1.249940699220447e-51
Epoch 2119: train_loss 0.024262168 val_loss 0.031532045
ttest: -15.982112942764353 pValue 2.1422098547396704e-51
Epoch 2120: train_loss 0.024261607 val_loss 0.031531226
ttest: -15.939879114344299 pValue 3.6727285269844204e-51
Epoch 2121: train_loss 0.02426105 val_loss 0.03153041
ttest: -15.897639167339234 pValue 6.291930306787968e-51
Epoch 2122: train_loss 0.024260486 val_loss 0.031529583
ttest: -15.855228163693127 pValue 1.0793379705752363e-50
Epoch 2123: train_loss 0.024259927 val_loss 0.03152876
ttest: -15.812733966478708 pValue 1.851904731394622e-50
Epoch 2124: train_loss 0.024259366 val_loss 0.031527948
ttest: -15.770158025554624 pValue 3.1780142388548483e-50
Epoch 2125: train_loss 0.024258807 val_loss 0.03152713
ttest: -15.727676396835674 pValue 5.442478752540914e-50
Epoch 2126: train_loss 0.024258249 val_loss 0.03152632
ttest: -15.685031762236003 pValue 9.331595776138559e-50
Epoch 2127: train_loss 0.024257688 val_loss 0.0315255
ttest: -15.642137659552445 pValue 1.603620930607692e-49
Epoch 2128: train_loss 0.024257127 val_loss 0.031524673
ttest: -15.59934490863061 pValue 2.7498251377204866e-49
Epoch 2129: train_loss 0.024256568 val_loss 0.031523835
ttest: -15.556391680677285 pValue 4.720613751161692e-49
Epoch 2130: train_loss 0.024256008 val_loss 0.031523015
ttest: -15.513189990650488 pValue 8.12187379422445e-49
Epoch 2131: train_loss 0.02425545 val_loss 0.031522196
ttest: -15.470185788300736 pValue 1.3926451887118465e-48
Epoch 2132: train_loss 0.024254888 val_loss 0.031521372
ttest: -15.426936602356907 pValue 2.3931045124014645e-48
Epoch 2133: train_loss 0.024254331 val_loss 0.03152054
ttest: -15.383622635268338 pValue 4.111809856304753e-48
Epoch 2134: train_loss 0.024253769 val_loss 0.03151971
ttest: -15.340243881610473 pValue 7.064008675703297e-48
Epoch 2135: train_loss 0.024253212 val_loss 0.031518877
ttest: -15.2968071522352 pValue 1.21332192024806e-47
Epoch 2136: train_loss 0.024252655 val_loss 0.03151805
ttest: -15.253219412073904 pValue 2.0859643107023432e-47
Epoch 2137: train_loss 0.024252096 val_loss 0.031517208
ttest: -15.209576855923876 pValue 3.585247329135129e-47
Epoch 2138: train_loss 0.024251536 val_loss 0.031516384
ttest: -15.165695280749661 pValue 6.17449156859144e-47
Epoch 2139: train_loss 0.024250977 val_loss 0.031515554
ttest: -15.122043568800091 pValue 1.059315384660728e-46
Epoch 2140: train_loss 0.024250418 val_loss 0.03151472
ttest: -15.077970674260433 pValue 1.825106672879225e-46
Epoch 2141: train_loss 0.02424986 val_loss 0.03151389
ttest: -15.034132977772348 pValue 3.132291486228529e-46
Epoch 2142: train_loss 0.024249302 val_loss 0.031513054
ttest: -14.989973061588236 pValue 5.3917670110064055e-46
Epoch 2143: train_loss 0.024248742 val_loss 0.031512216
ttest: -14.94586757172167 pValue 9.26562843499935e-46
Epoch 2144: train_loss 0.024248186 val_loss 0.031511374
ttest: -14.901439967576811 pValue 1.5969756174980193e-45
Epoch 2145: train_loss 0.024247626 val_loss 0.03151054
ttest: -14.85735976054442 pValue 2.7379908677673574e-45
Epoch 2146: train_loss 0.024247069 val_loss 0.031509697
ttest: -14.812864971909637 pValue 4.713317206928373e-45
Epoch 2147: train_loss 0.024246512 val_loss 0.031508856
ttest: -14.768244585804183 pValue 8.117796576211864e-45
Epoch 2148: train_loss 0.024245953 val_loss 0.031508017
ttest: -14.723595185427572 pValue 1.3971743879009403e-44
Epoch 2149: train_loss 0.024245394 val_loss 0.031507183
ttest: -14.678922303876313 pValue 2.402880887185851e-44
Epoch 2150: train_loss 0.024244836 val_loss 0.031506345
ttest: -14.634228165901112 pValue 4.129228370855688e-44
Epoch 2151: train_loss 0.024244279 val_loss 0.0315055
ttest: -14.58941660839638 pValue 7.098448292082963e-44
Epoch 2152: train_loss 0.024243722 val_loss 0.03150465
ttest: -14.54449032195853 pValue 1.2206646170935058e-43
Epoch 2153: train_loss 0.024243165 val_loss 0.031503804
ttest: -14.49935274370442 pValue 2.1021696216272714e-43
Epoch 2154: train_loss 0.024242608 val_loss 0.031502955
ttest: -14.454302472544695 pValue 3.612515831138284e-43
Epoch 2155: train_loss 0.024242051 val_loss 0.031502098
ttest: -14.409046122519507 pValue 6.216594361576501e-43
Epoch 2156: train_loss 0.024241492 val_loss 0.031501234
ttest: -14.363684309895596 pValue 1.0699522733810488e-42
Epoch 2157: train_loss 0.024240935 val_loss 0.031500388
ttest: -14.31842096780384 pValue 1.83730667781419e-42
Epoch 2158: train_loss 0.024240378 val_loss 0.03149954
ttest: -14.27295870327221 pValue 3.1589753039159925e-42
Epoch 2159: train_loss 0.024239821 val_loss 0.031498678
ttest: -14.22750268090425 pValue 5.424869533394259e-42
Epoch 2160: train_loss 0.024239264 val_loss 0.03149783
ttest: -14.181747710327755 pValue 9.338642018178244e-42
Epoch 2161: train_loss 0.024238707 val_loss 0.031496968
ttest: -14.136006519061578 pValue 1.6054914783350218e-41
Epoch 2162: train_loss 0.024238149 val_loss 0.03149611
ttest: -14.090281131547213 pValue 2.7564513693115523e-41
Epoch 2163: train_loss 0.024237595 val_loss 0.031495254
ttest: -14.044473117624756 pValue 4.731664594377945e-41
Epoch 2164: train_loss 0.024237039 val_loss 0.031494386
ttest: -13.998585671299722 pValue 8.120409647055722e-41
Epoch 2165: train_loss 0.024236482 val_loss 0.031493522
ttest: -13.952409030953827 pValue 1.3967171208815474e-40
Epoch 2166: train_loss 0.024235925 val_loss 0.03149267
ttest: -13.906262758080842 pValue 2.398647849052831e-40
Epoch 2167: train_loss 0.024235366 val_loss 0.03149181
ttest: -13.859937987439896 pValue 4.12299247459599e-40
Epoch 2168: train_loss 0.02423481 val_loss 0.03149095
ttest: -13.813863963900554 pValue 7.0576844886246004e-40
Epoch 2169: train_loss 0.024234254 val_loss 0.03149009
ttest: -13.767295112421083 pValue 1.2136517517604881e-39
Epoch 2170: train_loss 0.024233697 val_loss 0.031489227
ttest: -13.720877962890526 pValue 2.0807783851802787e-39
Epoch 2171: train_loss 0.024233142 val_loss 0.03148835
ttest: -13.674508476506016 pValue 3.561104013844442e-39
Epoch 2172: train_loss 0.024232585 val_loss 0.03148748
ttest: -13.62775521751212 pValue 6.114178665997969e-39
Epoch 2173: train_loss 0.02423203 val_loss 0.031486608
ttest: -13.581055169648042 pValue 1.0478047111957282e-38
Epoch 2174: train_loss 0.024231471 val_loss 0.03148575
ttest: -13.53430208575388 pValue 1.794496534892246e-38
Epoch 2175: train_loss 0.024230918 val_loss 0.031484887
ttest: -13.487389627817706 pValue 3.075046649229059e-38
Epoch 2176: train_loss 0.024230361 val_loss 0.03148402
ttest: -13.44031843142106 pValue 5.272260190708636e-38
Epoch 2177: train_loss 0.024229806 val_loss 0.031483147
ttest: -13.39331306037156 pValue 9.021010356883259e-38
Epoch 2178: train_loss 0.024229249 val_loss 0.03148227
ttest: -13.346153256899491 pValue 1.5442523409262742e-37
Epoch 2179: train_loss 0.024228696 val_loss 0.031481385
ttest: -13.298953194727172 pValue 2.641278909909191e-37
Epoch 2180: train_loss 0.024228139 val_loss 0.031480517
ttest: -13.251604306090082 pValue 4.5193396027662734e-37
Epoch 2181: train_loss 0.024227582 val_loss 0.03147964
ttest: -13.204219959592205 pValue 7.725657123960772e-37
Epoch 2182: train_loss 0.024227027 val_loss 0.031478774
ttest: -13.156804984154222 pValue 1.3193760682010344e-36
Epoch 2183: train_loss 0.024226472 val_loss 0.031477906
ttest: -13.10936147713414 pValue 2.250925208404541e-36
Epoch 2184: train_loss 0.024225915 val_loss 0.031477038
ttest: -13.061779659157907 pValue 3.841011184431209e-36
Epoch 2185: train_loss 0.024225362 val_loss 0.03147617
ttest: -13.014057940306486 pValue 6.555754793082182e-36
Epoch 2186: train_loss 0.024224805 val_loss 0.031475287
ttest: -12.966200473352822 pValue 1.1190898754216045e-35
Epoch 2187: train_loss 0.02422425 val_loss 0.031474408
ttest: -12.918325171640404 pValue 1.9080687998246122e-35
Epoch 2188: train_loss 0.024223695 val_loss 0.03147353
ttest: -12.87031743792353 pValue 3.2535747714481807e-35
Epoch 2189: train_loss 0.024223141 val_loss 0.03147264
ttest: -12.822533939905723 pValue 5.526362719811369e-35
Epoch 2190: train_loss 0.024222584 val_loss 0.03147175
ttest: -12.774150927077468 pValue 9.436159569327615e-35
Epoch 2191: train_loss 0.024222028 val_loss 0.031470876
ttest: -12.725997829090055 pValue 1.6048251029811077e-34
Epoch 2192: train_loss 0.024221474 val_loss 0.031470004
ttest: -12.67772121494054 pValue 2.7291847535289562e-34
Epoch 2193: train_loss 0.024220921 val_loss 0.031469118
ttest: -12.62932400520424 pValue 4.640770592510239e-34
Epoch 2194: train_loss 0.024220364 val_loss 0.031468228
ttest: -12.580928291737118 pValue 7.879738963818423e-34
Epoch 2195: train_loss 0.02421981 val_loss 0.03146732
ttest: -12.532418121613258 pValue 1.3376601348410842e-33
Epoch 2196: train_loss 0.024219254 val_loss 0.03146642
ttest: -12.483914726956005 pValue 2.2673161761997855e-33
Epoch 2197: train_loss 0.024218699 val_loss 0.03146553
ttest: -12.435177170926055 pValue 3.8472014167930096e-33
Epoch 2198: train_loss 0.024218146 val_loss 0.031464648
ttest: -12.386329256194541 pValue 6.526054691204401e-33
Epoch 2199: train_loss 0.02421759 val_loss 0.031463765
ttest: -12.337621763938698 pValue 1.1036896714499266e-32
Epoch 2200: train_loss 0.024217036 val_loss 0.03146288
ttest: -12.288560058432628 pValue 1.8708964732170443e-32
Epoch 2201: train_loss 0.024216482 val_loss 0.03146199
ttest: -12.239519870949835 pValue 3.165854103007527e-32
Epoch 2202: train_loss 0.024215924 val_loss 0.031461094
ttest: -12.190505567126838 pValue 5.347474416678571e-32
Epoch 2203: train_loss 0.024215372 val_loss 0.031460192
ttest: -12.141266434766383 pValue 9.04032454539149e-32
Epoch 2204: train_loss 0.024214815 val_loss 0.031459294
ttest: -12.092058052450913 pValue 1.5254722456957466e-31
Epoch 2205: train_loss 0.024214264 val_loss 0.031458393
ttest: -12.04275758352666 pValue 2.572613371769045e-31
Epoch 2206: train_loss 0.024213707 val_loss 0.031457502
ttest: -11.993365833623077 pValue 4.335956506725616e-31
Epoch 2207: train_loss 0.024213154 val_loss 0.031456612
ttest: -11.943757318767945 pValue 7.31314373753226e-31
Epoch 2208: train_loss 0.024212599 val_loss 0.03145572
ttest: -11.894322087195082 pValue 1.2292489505695395e-30
Epoch 2209: train_loss 0.024212044 val_loss 0.031454824
ttest: -11.844411793425413 pValue 2.0732337024921657e-30
Epoch 2210: train_loss 0.02421149 val_loss 0.031453907
ttest: -11.794680897814533 pValue 3.484473550944076e-30
Epoch 2211: train_loss 0.024210935 val_loss 0.031453
ttest: -11.74500481623897 pValue 5.843531898641153e-30
Epoch 2212: train_loss 0.024210382 val_loss 0.031452097
ttest: -11.694855530487823 pValue 9.83204975494479e-30
Epoch 2213: train_loss 0.024209827 val_loss 0.03145119
ttest: -11.645030578126729 pValue 1.646021579717137e-29
Epoch 2214: train_loss 0.024209274 val_loss 0.03145029
ttest: -11.595001136240922 pValue 2.756962664788894e-29
Epoch 2215: train_loss 0.02420872 val_loss 0.031449392
ttest: -11.54463541462015 pValue 4.6260207440128657e-29
Epoch 2216: train_loss 0.024208166 val_loss 0.0314485
ttest: -11.494472106882068 pValue 7.733044350539753e-29
Epoch 2217: train_loss 0.024207612 val_loss 0.031447586
ttest: -11.444108579119684 pValue 1.2931610351766014e-28
Epoch 2218: train_loss 0.02420706 val_loss 0.031446666
ttest: -11.393822346582297 pValue 2.1571254364035535e-28
Epoch 2219: train_loss 0.024206504 val_loss 0.031445745
ttest: -11.343340772097175 pValue 3.599329623819554e-28
Epoch 2220: train_loss 0.02420595 val_loss 0.031444833
ttest: -11.292802841761269 pValue 5.9988731296115695e-28
Epoch 2221: train_loss 0.024205396 val_loss 0.031443924
ttest: -11.242210810832582 pValue 9.986304193608388e-28
Epoch 2222: train_loss 0.024204843 val_loss 0.03144301
ttest: -11.19143016781491 pValue 1.6626812454271328e-27
Epoch 2223: train_loss 0.02420429 val_loss 0.031442102
ttest: -11.140740651271747 pValue 2.7609278206632745e-27
Epoch 2224: train_loss 0.024203736 val_loss 0.03144119
ttest: -11.089724598851582 pValue 4.591498057162622e-27
Epoch 2225: train_loss 0.024203183 val_loss 0.031440265
ttest: -11.038948534087316 pValue 7.604025158498489e-27
Epoch 2226: train_loss 0.024202628 val_loss 0.031439334
ttest: -10.9878480839479 pValue 1.261128340757328e-26
Epoch 2227: train_loss 0.024202073 val_loss 0.031438414
ttest: -10.936853219334184 pValue 2.0856472943324339e-26
Epoch 2228: train_loss 0.024201522 val_loss 0.0314375
ttest: -10.885536438795313 pValue 3.453957589137704e-26
Epoch 2229: train_loss 0.024200967 val_loss 0.031436585
ttest: -10.834186716091406 pValue 5.711369546008359e-26
Epoch 2230: train_loss 0.024200413 val_loss 0.03143567
ttest: -10.782805556469066 pValue 9.429751171963024e-26
Epoch 2231: train_loss 0.02419986 val_loss 0.03143476
ttest: -10.73139979080244 pValue 1.5544046624979972e-25
Epoch 2232: train_loss 0.024199305 val_loss 0.031433832
ttest: -10.679821706448177 pValue 2.5618279777207815e-25
Epoch 2233: train_loss 0.024198754 val_loss 0.031432915
ttest: -10.628222294308637 pValue 4.21515936053866e-25
Epoch 2234: train_loss 0.024198199 val_loss 0.03143199
ttest: -10.576604278654694 pValue 6.92374806452343e-25
Epoch 2235: train_loss 0.024197645 val_loss 0.031431068
ttest: -10.524822697917042 pValue 1.1369246349803098e-24
Epoch 2236: train_loss 0.024197094 val_loss 0.03143014
ttest: -10.473027674100905 pValue 1.863599024212588e-24
Epoch 2237: train_loss 0.02419654 val_loss 0.031429224
ttest: -10.421072400673738 pValue 3.053562461418162e-24
Epoch 2238: train_loss 0.024195986 val_loss 0.0314283
ttest: -10.369262760361423 pValue 4.986846136860855e-24
Epoch 2239: train_loss 0.024195435 val_loss 0.03142736
ttest: -10.317144537666566 pValue 8.152217474039749e-24
Epoch 2240: train_loss 0.02419488 val_loss 0.031426426
ttest: -10.264718635057077 pValue 1.3339356752123218e-23
Epoch 2241: train_loss 0.024194326 val_loss 0.031425487
ttest: -10.212600189419014 pValue 2.1721263563977385e-23
Epoch 2242: train_loss 0.024193773 val_loss 0.031424563
ttest: -10.160331464074378 pValue 3.535022168192266e-23
Epoch 2243: train_loss 0.02419322 val_loss 0.031423636
ttest: -10.108072210002168 pValue 5.741189850679011e-23
Epoch 2244: train_loss 0.024192667 val_loss 0.03142271
ttest: -10.055510439463236 pValue 9.331793444983112e-23
Epoch 2245: train_loss 0.024192113 val_loss 0.031421784
ttest: -10.003119463300084 pValue 1.5113719038692537e-22
Epoch 2246: train_loss 0.02419156 val_loss 0.03142086
ttest: -9.950429416969785 pValue 2.4496140240113743e-22
Epoch 2247: train_loss 0.024191007 val_loss 0.03141993
ttest: -9.89775917633213 pValue 3.9615205635132267e-22
Epoch 2248: train_loss 0.024190454 val_loss 0.03141899
ttest: -9.845111658770751 pValue 6.392209296384265e-22
Epoch 2249: train_loss 0.0241899 val_loss 0.031418055
ttest: -9.792168817939356 pValue 1.032081146603882e-21
Epoch 2250: train_loss 0.024189347 val_loss 0.031417128
ttest: -9.739093353514956 pValue 1.664929761038935e-21
Epoch 2251: train_loss 0.024188796 val_loss 0.03141619
ttest: -9.686211790008885 pValue 2.6755628240194176e-21
Epoch 2252: train_loss 0.024188241 val_loss 0.03141525
ttest: -9.633040428395937 pValue 4.3018547487281705e-21
Epoch 2253: train_loss 0.024187688 val_loss 0.03141432
ttest: -9.58007168878505 pValue 6.889661860518614e-21
Epoch 2254: train_loss 0.024187136 val_loss 0.03141337
ttest: -9.526815610239748 pValue 1.1039100769741893e-20
Epoch 2255: train_loss 0.024186581 val_loss 0.031412434
ttest: -9.473438563004846 pValue 1.766886373850571e-20
Epoch 2256: train_loss 0.024186028 val_loss 0.031411488
ttest: -9.420111048670625 pValue 2.820745776718637e-20
Epoch 2257: train_loss 0.024185475 val_loss 0.03141055
ttest: -9.366667302148178 pValue 4.4980927232865754e-20
Epoch 2258: train_loss 0.024184924 val_loss 0.03140961
ttest: -9.31311052590874 pValue 7.16442110449029e-20
Epoch 2259: train_loss 0.024184372 val_loss 0.031408668
ttest: -9.259612874420291 pValue 1.1380634761486479e-19
Epoch 2260: train_loss 0.02418382 val_loss 0.031407733
ttest: -9.206007729801973 pValue 1.8055444699056337e-19
Epoch 2261: train_loss 0.024183264 val_loss 0.031406794
ttest: -9.152126188304909 pValue 2.8650122101470984e-19
Epoch 2262: train_loss 0.024182713 val_loss 0.031405848
ttest: -9.098484410146735 pValue 4.526806329032099e-19
Epoch 2263: train_loss 0.02418216 val_loss 0.031404898
ttest: -9.044396337022505 pValue 7.163802761583669e-19
Epoch 2264: train_loss 0.024181606 val_loss 0.031403944
ttest: -8.990383644249548 pValue 1.1304236642774126e-18
Epoch 2265: train_loss 0.024181053 val_loss 0.031402994
ttest: -8.936275148767164 pValue 1.7811929278211864e-18
Epoch 2266: train_loss 0.0241805 val_loss 0.031402044
ttest: -8.882249344914833 pValue 2.7983154960810108e-18
Epoch 2267: train_loss 0.024179948 val_loss 0.03140109
ttest: -8.82795716582633 pValue 4.396038202402325e-18
Epoch 2268: train_loss 0.024179393 val_loss 0.031400144
ttest: -8.773933911831692 pValue 6.8748574698264365e-18
Epoch 2269: train_loss 0.024178842 val_loss 0.031399198
ttest: -8.71946909905892 pValue 1.076605251323775e-17
Epoch 2270: train_loss 0.024178287 val_loss 0.031398244
ttest: -8.665101912967506 pValue 1.6807161308559082e-17
Epoch 2271: train_loss 0.024177736 val_loss 0.031397294
ttest: -8.610838229801498 pValue 2.615527386918988e-17
Epoch 2272: train_loss 0.024177184 val_loss 0.03139634
ttest: -8.55613544841249 pValue 4.0753463576860755e-17
Epoch 2273: train_loss 0.02417663 val_loss 0.031395383
ttest: -8.501541359592169 pValue 6.329430029650202e-17
Epoch 2274: train_loss 0.024176076 val_loss 0.03139443
ttest: -8.44724730439269 pValue 9.783551630162275e-17
Epoch 2275: train_loss 0.024175525 val_loss 0.03139348
ttest: -8.392146482013253 pValue 1.5184780928043073e-16
Epoch 2276: train_loss 0.024174973 val_loss 0.031392537
ttest: -8.337352399171353 pValue 2.34539568456944e-16
Epoch 2277: train_loss 0.02417442 val_loss 0.03139158
ttest: -8.282684910464393 pValue 3.610366209592797e-16
Epoch 2278: train_loss 0.024173867 val_loss 0.031390622
ttest: -8.227584687396863 pValue 5.563190763238964e-16
Epoch 2279: train_loss 0.024173314 val_loss 0.031389657
ttest: -8.17261667714372 pValue 8.542656582546394e-16
Epoch 2280: train_loss 0.02417276 val_loss 0.03138869
ttest: -8.117788959531516 pValue 1.3071817310070042e-15
Epoch 2281: train_loss 0.024172207 val_loss 0.031387724
ttest: -8.062531222991234 pValue 2.002019805130447e-15
Epoch 2282: train_loss 0.024171656 val_loss 0.03138676
ttest: -8.007612635821067 pValue 3.050715079259452e-15
Epoch 2283: train_loss 0.024171103 val_loss 0.0313858
ttest: -7.952072822975197 pValue 4.6594182032412e-15
Epoch 2284: train_loss 0.02417055 val_loss 0.031384848
ttest: -7.896882226166967 pValue 7.079888410112775e-15
Epoch 2285: train_loss 0.024169998 val_loss 0.031383898
ttest: -7.841264998339888 pValue 1.0765673036490333e-14
Epoch 2286: train_loss 0.024169443 val_loss 0.031382937
ttest: -7.7860073554692635 pValue 1.6285169840627938e-14
Epoch 2287: train_loss 0.024168892 val_loss 0.031381976
ttest: -7.730526077543709 pValue 2.4614034658520234e-14
Epoch 2288: train_loss 0.024168339 val_loss 0.031380996
ttest: -7.675020208803232 pValue 3.711569905691086e-14
Epoch 2289: train_loss 0.024167785 val_loss 0.031380028
ttest: -7.619694164582112 pValue 5.5751989815332973e-14
Epoch 2290: train_loss 0.024167234 val_loss 0.031379063
ttest: -7.563950437212932 pValue 8.379036930003845e-14
Epoch 2291: train_loss 0.02416668 val_loss 0.0313781
ttest: -7.508191984723919 pValue 1.2562073249688707e-13
Epoch 2292: train_loss 0.024166128 val_loss 0.03137713
ttest: -7.452422712273932 pValue 1.878651797607617e-13
Epoch 2293: train_loss 0.024165576 val_loss 0.03137616
ttest: -7.396648052626019 pValue 2.802391950376046e-13
Epoch 2294: train_loss 0.024165023 val_loss 0.031375177
ttest: -7.340870477279512 pValue 4.1696396827321544e-13
Epoch 2295: train_loss 0.024164468 val_loss 0.0313742
ttest: -7.284679893837985 pValue 6.205998730935026e-13
Epoch 2296: train_loss 0.024163917 val_loss 0.031373233
ttest: -7.228699939557902 pValue 9.198954088410032e-13
Epoch 2297: train_loss 0.024163362 val_loss 0.03137226
ttest: -7.1729400163804655 pValue 1.3578777981073072e-12
Epoch 2298: train_loss 0.024162808 val_loss 0.031371295
ttest: -7.1167726579881885 pValue 2.0048297126624274e-12
Epoch 2299: train_loss 0.024162259 val_loss 0.03137033
ttest: -7.060621774331655 pValue 2.9518507625291834e-12
Epoch 2300: train_loss 0.024161706 val_loss 0.031369343
ttest: -7.00449212315338 pValue 4.334068584518006e-12
Epoch 2301: train_loss 0.024161153 val_loss 0.031368356
ttest: -6.948172877725783 pValue 6.354798631463923e-12
Epoch 2302: train_loss 0.0241606 val_loss 0.03136738
ttest: -6.892099264452332 pValue 9.277363090674866e-12
Epoch 2303: train_loss 0.024160048 val_loss 0.031366397
ttest: -6.835843658389621 pValue 1.352444341265009e-11
Epoch 2304: train_loss 0.024159495 val_loss 0.031365424
ttest: -6.779628289427868 pValue 1.9657644234038812e-11
Epoch 2305: train_loss 0.02415894 val_loss 0.031364452
ttest: -6.723016448405996 pValue 2.857008347986177e-11
Epoch 2306: train_loss 0.024158388 val_loss 0.031363472
ttest: -6.666892889554999 pValue 4.127784480548748e-11
Epoch 2307: train_loss 0.024157835 val_loss 0.0313625
ttest: -6.61037916866112 pValue 5.962866856447138e-11
Epoch 2308: train_loss 0.024157284 val_loss 0.031361517
ttest: -6.553922211467479 pValue 8.587111431168207e-11
Epoch 2309: train_loss 0.024156732 val_loss 0.031360537
ttest: -6.497301150592076 pValue 1.2345483891802718e-10
Epoch 2310: train_loss 0.024156177 val_loss 0.03135955
ttest: -6.440745715181471 pValue 1.769260361183829e-10
Epoch 2311: train_loss 0.024155624 val_loss 0.03135857
ttest: -6.384261472117158 pValue 2.527461918518704e-10
Epoch 2312: train_loss 0.024155075 val_loss 0.031357586
ttest: -6.327625346642329 pValue 3.604083036243809e-10
Epoch 2313: train_loss 0.024154518 val_loss 0.0313566
ttest: -6.270839495747383 pValue 5.129847343416568e-10
Epoch 2314: train_loss 0.024153965 val_loss 0.03135562
ttest: -6.214601658780986 pValue 7.25666023015983e-10
Epoch 2315: train_loss 0.024153411 val_loss 0.03135462
ttest: -6.157760551130759 pValue 1.0274894253700864e-09
Epoch 2316: train_loss 0.02415286 val_loss 0.031353626
ttest: -6.101483208875421 pValue 1.4458255354363301e-09
Epoch 2317: train_loss 0.024152309 val_loss 0.03135264
ttest: -6.044605411177183 pValue 2.0362139615971735e-09
Epoch 2318: train_loss 0.024151754 val_loss 0.03135165
ttest: -5.987832865211248 pValue 2.857810269221333e-09
Epoch 2319: train_loss 0.0241512 val_loss 0.03135067
ttest: -5.931172558767176 pValue 3.996985920469329e-09
Epoch 2320: train_loss 0.02415065 val_loss 0.03134969
ttest: -5.874632570197086 pValue 5.570648338577277e-09
Epoch 2321: train_loss 0.024150094 val_loss 0.0313487
ttest: -5.817735756422726 pValue 7.758215639229783e-09
Epoch 2322: train_loss 0.024149543 val_loss 0.031347718
ttest: -5.760725148143529 pValue 1.0781222800929776e-08
Epoch 2323: train_loss 0.02414899 val_loss 0.031346723
ttest: -5.704579945981061 pValue 1.4865794330101145e-08
Epoch 2324: train_loss 0.024148438 val_loss 0.03134573
ttest: -5.647355774491873 pValue 2.0566154885265222e-08
Epoch 2325: train_loss 0.024147883 val_loss 0.031344727
ttest: -5.590768447444556 pValue 2.8269166400956892e-08
Epoch 2326: train_loss 0.024147332 val_loss 0.031343743
ttest: -5.534088590309078 pValue 3.876747317579716e-08
Epoch 2327: train_loss 0.024146777 val_loss 0.03134276
ttest: -5.477320098962809 pValue 5.303949148694933e-08
Epoch 2328: train_loss 0.024146223 val_loss 0.031341765
ttest: -5.420718259080667 pValue 7.229298854009492e-08
Epoch 2329: train_loss 0.02414567 val_loss 0.031340767
ttest: -5.364037954344929 pValue 9.829749564615783e-08
Epoch 2330: train_loss 0.024145119 val_loss 0.03133975
ttest: -5.3072840092092175 pValue 1.333275945183857e-07
Epoch 2331: train_loss 0.024144566 val_loss 0.031338748
ttest: -5.250716268832268 pValue 1.80145736276081e-07
Epoch 2332: train_loss 0.024144012 val_loss 0.031337745
ttest: -5.193828059154549 pValue 2.431199644628522e-07
Epoch 2333: train_loss 0.02414346 val_loss 0.031336747
ttest: -5.137655437842666 pValue 3.2594685603810225e-07
Epoch 2334: train_loss 0.024142906 val_loss 0.031335756
ttest: -5.0809146508587775 pValue 4.3703866764229474e-07
Epoch 2335: train_loss 0.024142353 val_loss 0.031334784
ttest: -5.024124631621616 pValue 5.8445537163844e-07
Epoch 2336: train_loss 0.024141798 val_loss 0.03133379
ttest: -4.967817533721738 pValue 7.774449898046586e-07
Epoch 2337: train_loss 0.024141246 val_loss 0.0313328
ttest: -4.91121180631239 pValue 1.032765726203478e-06
Epoch 2338: train_loss 0.024140693 val_loss 0.031331774
ttest: -4.8548413337688405 pValue 1.3664053163637735e-06
Epoch 2339: train_loss 0.024140138 val_loss 0.03133077
ttest: -4.798446690504274 pValue 1.8028880974819777e-06
Epoch 2340: train_loss 0.024139587 val_loss 0.031329777
ttest: -4.742303922201458 pValue 2.369112634938896e-06
Epoch 2341: train_loss 0.024139034 val_loss 0.03132878
ttest: -4.685610265110114 pValue 3.1125573393898224e-06
Epoch 2342: train_loss 0.02413848 val_loss 0.03132779
ttest: -4.62972354935698 pValue 4.061895404600547e-06
Epoch 2343: train_loss 0.024137927 val_loss 0.031326782
ttest: -4.573568145867607 pValue 5.292542145858218e-06
Epoch 2344: train_loss 0.024137372 val_loss 0.031325765
ttest: -4.517419953353597 pValue 6.876208847739596e-06
Epoch 2345: train_loss 0.024136819 val_loss 0.03132474
ttest: -4.461285736324964 pValue 8.907767896852762e-06
Epoch 2346: train_loss 0.024136268 val_loss 0.031323735
ttest: -4.4054504990792545 pValue 1.1491105672604226e-05
Epoch 2347: train_loss 0.02413571 val_loss 0.031322733
ttest: -4.349364307305789 pValue 1.4798542810072464e-05
Epoch 2348: train_loss 0.02413516 val_loss 0.031321727
ttest: -4.293593222095971 pValue 1.8977178134479455e-05
Epoch 2349: train_loss 0.024134606 val_loss 0.03132073
ttest: -4.2378648318932175 pValue 2.4262694474154967e-05
Epoch 2350: train_loss 0.024134051 val_loss 0.031319726
ttest: -4.1821863344652055 pValue 3.092642070023037e-05
Epoch 2351: train_loss 0.024133498 val_loss 0.0313187
ttest: -4.126851889934878 pValue 3.9251700402176585e-05
Epoch 2352: train_loss 0.024132943 val_loss 0.03131769
ttest: -4.071295198771091 pValue 4.972667588715021e-05
Epoch 2353: train_loss 0.024132393 val_loss 0.03131667
ttest: -4.01580925187049 pValue 6.28021771899726e-05
Epoch 2354: train_loss 0.024131838 val_loss 0.031315666
ttest: -3.9609864632394785 pValue 7.887773271446683e-05
Epoch 2355: train_loss 0.024131283 val_loss 0.031314667
ttest: -3.9056689148066583 pValue 9.89979313275087e-05
Epoch 2356: train_loss 0.02413073 val_loss 0.03131366
ttest: -3.8501491542467003 pValue 0.000124006461199644
Epoch 2357: train_loss 0.024130179 val_loss 0.03131264
ttest: -3.7956195768408425 pValue 0.00015428791364625142
Epoch 2358: train_loss 0.024129624 val_loss 0.031311624
ttest: -3.7409074317971402 pValue 0.0001915831973330867
Epoch 2359: train_loss 0.02412907 val_loss 0.031310603
ttest: -3.6860154627329953 pValue 0.000237412834070726
Epoch 2360: train_loss 0.024128515 val_loss 0.03130958
ttest: -3.6321572089947107 pValue 0.0002922395047389521
Epoch 2361: train_loss 0.02412796 val_loss 0.031308565
ttest: -3.577531449192609 pValue 0.00035982412745625596
Epoch 2362: train_loss 0.024127409 val_loss 0.03130755
ttest: -3.5230495894731826 pValue 0.00044160057231855745
Epoch 2363: train_loss 0.024126854 val_loss 0.031306542
ttest: -3.4693360726489253 pValue 0.0005389722327359042
Epoch 2364: train_loss 0.0241263 val_loss 0.031305537
ttest: -3.415482248792519 pValue 0.0006564260663364261
Epoch 2365: train_loss 0.024125746 val_loss 0.031304516
ttest: -3.362114388398571 pValue 0.0007959812389020555
Epoch 2366: train_loss 0.024125192 val_loss 0.031303484
ttest: -3.308308744073442 pValue 0.0009642048089487242
Epoch 2367: train_loss 0.02412464 val_loss 0.03130246
ttest: -3.2553229736762987 pValue 0.0011615829292556288
Epoch 2368: train_loss 0.024124082 val_loss 0.031301435
ttest: -3.2022283225688524 pValue 0.0013963269479337205
Epoch 2369: train_loss 0.024123529 val_loss 0.031300418
ttest: -3.149029970129841 pValue 0.0016748119159103422
Epoch 2370: train_loss 0.024122976 val_loss 0.03129941
ttest: -3.0966928767401845 pValue 0.0019979192582471
Epoch 2371: train_loss 0.024122423 val_loss 0.03129839
ttest: -3.044272832528847 pValue 0.0023781028358233506
Epoch 2372: train_loss 0.024121866 val_loss 0.031297367
ttest: -2.992098514655539 pValue 0.0028213393350972307
Epoch 2373: train_loss 0.024121314 val_loss 0.03129634
ttest: -2.939855201841295 pValue 0.0033396907818286146
Epoch 2374: train_loss 0.02412076 val_loss 0.03129531
ttest: -2.8882030422521154 pValue 0.003936174286161662
Epoch 2375: train_loss 0.024120206 val_loss 0.031294283
ttest: -2.83650076262991 pValue 0.004628747593703514
Epoch 2376: train_loss 0.02411965 val_loss 0.031293258
ttest: -2.7854161193202533 pValue 0.005419790900406033
Epoch 2377: train_loss 0.024119098 val_loss 0.031292245
ttest: -2.7343007348320665 pValue 0.006331699279329175
Epoch 2378: train_loss 0.024118543 val_loss 0.031291217
ttest: -2.683495440318775 pValue 0.0073728563540108065
Epoch 2379: train_loss 0.024117988 val_loss 0.031290192
ttest: -2.6330122938622638 pValue 0.00855723407464314
Epoch 2380: train_loss 0.024117433 val_loss 0.031289153
ttest: -2.5825255627380277 pValue 0.009909267955221235
Epoch 2381: train_loss 0.024116877 val_loss 0.031288117
ttest: -2.533062003987641 pValue 0.011415513122005857
Epoch 2382: train_loss 0.024116322 val_loss 0.03128709
ttest: -2.483277647526933 pValue 0.013133706009472775
Epoch 2383: train_loss 0.02411577 val_loss 0.031286057
ttest: -2.434206030280725 pValue 0.015047535068088001
Epoch 2384: train_loss 0.024115212 val_loss 0.031285033
ttest: -2.385174015417354 pValue 0.017201387942797738
Epoch 2385: train_loss 0.024114659 val_loss 0.031284012
ttest: -2.336537023403523 pValue 0.01960082609378755
Epoch 2386: train_loss 0.024114106 val_loss 0.031282976
ttest: -2.2879588292152353 pValue 0.022284592756833203
Epoch 2387: train_loss 0.02411355 val_loss 0.03128194
ttest: -2.2405015599459985 pValue 0.025209852529431292
Epoch 2388: train_loss 0.024112998 val_loss 0.031280912
ttest: -2.192777142313117 pValue 0.02848156071336073
Epoch 2389: train_loss 0.024112443 val_loss 0.031279873
ttest: -2.1451432858306982 pValue 0.03210589697712567
Epoch 2390: train_loss 0.024111887 val_loss 0.031278837
ttest: -2.098322526369705 pValue 0.03604712671587269
Epoch 2391: train_loss 0.024111332 val_loss 0.031277806
ttest: -2.0519760207299282 pValue 0.040347995508065386
Epoch 2392: train_loss 0.024110777 val_loss 0.031276777
ttest: -2.006118045228056 pValue 0.04502473545193866
Epoch 2393: train_loss 0.024110222 val_loss 0.03127574
ttest: -1.960037665077948 pValue 0.050177132161434754
Epoch 2394: train_loss 0.024109667 val_loss 0.03127469
ttest: -1.914832361276005 pValue 0.05570370725544381
Epoch 2395: train_loss 0.024109114 val_loss 0.03127365
ttest: -1.8694231634500207 pValue 0.06175736675596311
Epoch 2396: train_loss 0.024108559 val_loss 0.03127261
ttest: -1.8252873297477743 pValue 0.06815399758669101
Epoch 2397: train_loss 0.024108 val_loss 0.031271573
ttest: -1.7813407131438022 pValue 0.07505546341815747
Epoch 2398: train_loss 0.024107447 val_loss 0.031270538
ttest: -1.7379653813355929 pValue 0.08241729336451525
Epoch 2399: train_loss 0.024106892 val_loss 0.03126951
ttest: -1.6951769508797683 pValue 0.0902433447156336
Epoch 2400: train_loss 0.024106335 val_loss 0.03126847
ttest: -1.6522391849733988 pValue 0.09868781131246868
Epoch 2401: train_loss 0.024105784 val_loss 0.031267427
ttest: -1.6102903400482786 pValue 0.10753639863094042
#################################################################
Target Domain Transfer
Test RMSE: 9.208791
Using TensorFlow backend.
2022-07-22 14:19:30.656175: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2
2022-07-22 14:19:30.799211: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties:
name: GeForce RTX 2080 Ti major: 7 minor: 5 memoryClockRate(GHz): 1.755
pciBusID: 0000:2e:00.0
totalMemory: 11.00GiB freeMemory: 9.90GiB
2022-07-22 14:19:30.799601: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0
2022-07-22 14:19:31.078804: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:
2022-07-22 14:19:31.079020: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0
2022-07-22 14:19:31.079992: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N
2022-07-22 14:19:31.080556: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9542 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:2e:00.0, compute capability: 7.5)
WARNING:tensorflow:From C:\Users\USER\.conda\envs\POI\lib\site-packages\tensorflow\python\ops\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
WARNING:tensorflow:From C:\Users\USER\.conda\envs\POI\lib\site-packages\tensorflow\python\ops\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.cast instead.
2022-07-22 14:19:32.099073: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0
2022-07-22 14:19:32.099236: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:
2022-07-22 14:19:32.100201: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0
2022-07-22 14:19:32.100644: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N
2022-07-22 14:19:32.101302: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9542 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:2e:00.0, compute capability: 7.5)
#################################################################
Source Domain information
DiDi Chengdu
source closeness shape: (1272, 256, 24, 1)
source test_y shape: (144, 256, 1)
Number of trainable variables 61921
Number of training samples 1272
#################################################################
Target Domain information
DiDi Xian
target closeness shape: (168, 253, 24, 1)
target test_y shape: (144, 253, 1)
Number of trainable variables 61921
Number of training samples 168
pretrain model not found. start training...
WARNING:tensorflow:From C:\Users\USER\.conda\envs\POI\lib\site-packages\tensorflow\python\training\saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
Found model in disk
Model converged, stop training
Not loading model from disk
Running Operation ('train_op',)
2022-07-22 14:19:33.939722: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library cublas64_100.dll locally
Epoch 0: train_loss 0.04723818 val_loss 0.04051413
Epoch 1: train_loss 0.047090262 val_loss 0.040124536
Epoch 2: train_loss 0.04688156 val_loss 0.039742447
Epoch 3: train_loss 0.046702884 val_loss 0.039476242
Epoch 4: train_loss 0.046593685 val_loss 0.039363984
Epoch 5: train_loss 0.046536185 val_loss 0.03938606
Epoch 6: train_loss 0.046486147 val_loss 0.03950201
Epoch 7: train_loss 0.04641147 val_loss 0.03967905
Epoch 8: train_loss 0.046309907 val_loss 0.039895926
Epoch 9: train_loss 0.046200603 val_loss 0.040131256
Epoch 10: train_loss 0.046104133 val_loss 0.040354103
Epoch 11: train_loss 0.04602787 val_loss 0.04052644
Epoch 12: train_loss 0.045964416 val_loss 0.040615562
Epoch 13: train_loss 0.045900412 val_loss 0.04060667
Epoch 14: train_loss 0.04582674 val_loss 0.040507387
Epoch 15: train_loss 0.045742985 val_loss 0.040342625
Epoch 16: train_loss 0.04565481 val_loss 0.040144272
Epoch 17: train_loss 0.045568377 val_loss 0.03994139
Epoch 18: train_loss 0.045486294 val_loss 0.039754722
Epoch 19: train_loss 0.045407366 val_loss 0.039595813
Epoch 20: train_loss 0.045328937 val_loss 0.039468654
Epoch 21: train_loss 0.045249403 val_loss 0.03937175
Epoch 22: train_loss 0.04516909 val_loss 0.039299574
WARNING:tensorflow:From C:\Users\USER\.conda\envs\POI\lib\site-packages\tensorflow\python\training\saver.py:966: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to delete files with this prefix.
Epoch 23: train_loss 0.04508942 val_loss 0.039243836
Epoch 24: train_loss 0.045011558 val_loss 0.03919506
Epoch 25: train_loss 0.044935606 val_loss 0.039144717
Epoch 26: train_loss 0.044860803 val_loss 0.03908719
Epoch 27: train_loss 0.04478626 val_loss 0.039020818
Epoch 28: train_loss 0.044711553 val_loss 0.038947623
Epoch 29: train_loss 0.044636823 val_loss 0.038871918
Epoch 30: train_loss 0.04456246 val_loss 0.038798586
Epoch 31: train_loss 0.044488743 val_loss 0.0387317
Epoch 32: train_loss 0.044415694 val_loss 0.038673557
Epoch 33: train_loss 0.04434317 val_loss 0.038624506
Epoch 34: train_loss 0.044271044 val_loss 0.038583197
Epoch 35: train_loss 0.044199307 val_loss 0.038546983
Epoch 36: train_loss 0.044128045 val_loss 0.03851262
Epoch 37: train_loss 0.044057354 val_loss 0.03847689
Epoch 38: train_loss 0.043987233 val_loss 0.03843724
Epoch 39: train_loss 0.043917626 val_loss 0.03839217
Epoch 40: train_loss 0.04384844 val_loss 0.038341463
Epoch 41: train_loss 0.04377962 val_loss 0.038285937
Epoch 42: train_loss 0.04371114 val_loss 0.038227182
Epoch 43: train_loss 0.043643024 val_loss 0.03816705
Epoch 44: train_loss 0.04357528 val_loss 0.038107235
Epoch 45: train_loss 0.043507896 val_loss 0.038048998
Epoch 46: train_loss 0.04344087 val_loss 0.037993006
Epoch 47: train_loss 0.043374192 val_loss 0.03793932
Epoch 48: train_loss 0.043307856 val_loss 0.037887547
Epoch 49: train_loss 0.04324186 val_loss 0.037837043
Epoch 50: train_loss 0.043176208 val_loss 0.037787106
Epoch 51: train_loss 0.043110892 val_loss 0.037737153
Epoch 52: train_loss 0.043045886 val_loss 0.03768688
Epoch 53: train_loss 0.042981192 val_loss 0.037636228
Epoch 54: train_loss 0.042916775 val_loss 0.037585407
Epoch 55: train_loss 0.042852633 val_loss 0.037534747
Epoch 56: train_loss 0.042788763 val_loss 0.03748461
Epoch 57: train_loss 0.04272515 val_loss 0.037435267
Epoch 58: train_loss 0.042661794 val_loss 0.03738687
Epoch 59: train_loss 0.042598687 val_loss 0.037339382
Epoch 60: train_loss 0.042535827 val_loss 0.03729264
Epoch 61: train_loss 0.04247321 val_loss 0.0372464
Epoch 62: train_loss 0.042410824 val_loss 0.037200354
Epoch 63: train_loss 0.042348668 val_loss 0.03715429
Epoch 64: train_loss 0.04228674 val_loss 0.037108038
Epoch 65: train_loss 0.04222502 val_loss 0.037061535
Epoch 66: train_loss 0.0421635 val_loss 0.037014816
Epoch 67: train_loss 0.042102188 val_loss 0.036968
Epoch 68: train_loss 0.042041063 val_loss 0.036921196
Epoch 69: train_loss 0.04198012 val_loss 0.036874566
Epoch 70: train_loss 0.041919366 val_loss 0.036828205
Epoch 71: train_loss 0.041858785 val_loss 0.036782146
Epoch 72: train_loss 0.041798376 val_loss 0.03673639
Epoch 73: train_loss 0.04173813 val_loss 0.036690895
Epoch 74: train_loss 0.041678052 val_loss 0.0366456
Epoch 75: train_loss 0.04161813 val_loss 0.036600452
Epoch 76: train_loss 0.04155837 val_loss 0.036555406
Epoch 77: train_loss 0.04149875 val_loss 0.03651045
Epoch 78: train_loss 0.041439272 val_loss 0.036465604
Epoch 79: train_loss 0.04137994 val_loss 0.036420863
Epoch 80: train_loss 0.04132074 val_loss 0.03637628
Epoch 81: train_loss 0.041261666 val_loss 0.036331855
Epoch 82: train_loss 0.041202724 val_loss 0.03628761
Epoch 83: train_loss 0.0411439 val_loss 0.03624355
Epoch 84: train_loss 0.0410852 val_loss 0.036199663
Epoch 85: train_loss 0.041026615 val_loss 0.03615593
Epoch 86: train_loss 0.040968142 val_loss 0.03611232
Epoch 87: train_loss 0.040909782 val_loss 0.036068805
Epoch 88: train_loss 0.040851526 val_loss 0.036025364
Epoch 89: train_loss 0.040793374 val_loss 0.035982
Epoch 90: train_loss 0.04073532 val_loss 0.035938706
Epoch 91: train_loss 0.040677357 val_loss 0.035895504
Epoch 92: train_loss 0.040619493 val_loss 0.035852376
Epoch 93: train_loss 0.04056172 val_loss 0.03580935
Epoch 94: train_loss 0.04050403 val_loss 0.035766415
Epoch 95: train_loss 0.040446438 val_loss 0.03572359
Epoch 96: train_loss 0.04038892 val_loss 0.03568086
Epoch 97: train_loss 0.040331483 val_loss 0.03563822
Epoch 98: train_loss 0.04027412 val_loss 0.035595667
Epoch 99: train_loss 0.04021684 val_loss 0.035553202
Epoch 100: train_loss 0.040159628 val_loss 0.035510823
Epoch 101: train_loss 0.040102493 val_loss 0.035468515
Epoch 102: train_loss 0.040045425 val_loss 0.03542629
Epoch 103: train_loss 0.039988425 val_loss 0.035384145
Epoch 104: train_loss 0.03993149 val_loss 0.03534207
Epoch 105: train_loss 0.039874617 val_loss 0.035300083
Epoch 106: train_loss 0.03981781 val_loss 0.03525817
Epoch 107: train_loss 0.03976106 val_loss 0.035216328
Epoch 108: train_loss 0.039704368 val_loss 0.03517456
Epoch 109: train_loss 0.039647736 val_loss 0.03513287
Epoch 110: train_loss 0.039591156 val_loss 0.03509125
Epoch 111: train_loss 0.039534632 val_loss 0.03504969
Epoch 112: train_loss 0.03947816 val_loss 0.035008203
Epoch 113: train_loss 0.039421745 val_loss 0.03496677
Epoch 114: train_loss 0.03936537 val_loss 0.034925397
Epoch 115: train_loss 0.03930905 val_loss 0.034884103
Epoch 116: train_loss 0.039252773 val_loss 0.03484286
Epoch 117: train_loss 0.03919655 val_loss 0.034801677
Epoch 118: train_loss 0.039140366 val_loss 0.03476056
Epoch 119: train_loss 0.039084226 val_loss 0.0347195
Epoch 120: train_loss 0.03902813 val_loss 0.0346785
Epoch 121: train_loss 0.03897208 val_loss 0.03463756
Epoch 122: train_loss 0.038916066 val_loss 0.034596674
Epoch 123: train_loss 0.038860098 val_loss 0.034555852
Epoch 124: train_loss 0.038804166 val_loss 0.03451509
Epoch 125: train_loss 0.038748275 val_loss 0.03447438
Epoch 126: train_loss 0.038692422 val_loss 0.034433722
Epoch 127: train_loss 0.03863661 val_loss 0.034393128
Epoch 128: train_loss 0.03858083 val_loss 0.034352582
Epoch 129: train_loss 0.03852509 val_loss 0.03431209
Epoch 130: train_loss 0.038469385 val_loss 0.03427166
Epoch 131: train_loss 0.03841372 val_loss 0.03423129
Epoch 132: train_loss 0.038358085 val_loss 0.034190964
Epoch 133: train_loss 0.03830249 val_loss 0.034150694
Epoch 134: train_loss 0.038246922 val_loss 0.034110475
Epoch 135: train_loss 0.038191393 val_loss 0.03407031
Epoch 136: train_loss 0.0381359 val_loss 0.034030203
Epoch 137: train_loss 0.03808043 val_loss 0.033990137
Epoch 138: train_loss 0.038025007 val_loss 0.033950128
Epoch 139: train_loss 0.03796961 val_loss 0.033910166
Epoch 140: train_loss 0.037914246 val_loss 0.033870265
Epoch 141: train_loss 0.03785892 val_loss 0.03383042
Epoch 142: train_loss 0.03780362 val_loss 0.03379062
Epoch 143: train_loss 0.03774836 val_loss 0.03375087
Epoch 144: train_loss 0.037693128 val_loss 0.03371118
Epoch 145: train_loss 0.03763793 val_loss 0.033671543
Epoch 146: train_loss 0.037582766 val_loss 0.033631958
Epoch 147: train_loss 0.037527632 val_loss 0.03359243
Epoch 148: train_loss 0.03747253 val_loss 0.033552945
Epoch 149: train_loss 0.037417464 val_loss 0.033513512
Epoch 150: train_loss 0.037362427 val_loss 0.033474132
Epoch 151: train_loss 0.03730743 val_loss 0.033434793
Epoch 152: train_loss 0.037252463 val_loss 0.03339552
Epoch 153: train_loss 0.03719753 val_loss 0.033356305
Epoch 154: train_loss 0.037142627 val_loss 0.03331715
Epoch 155: train_loss 0.037087765 val_loss 0.033278037
Epoch 156: train_loss 0.037032932 val_loss 0.033238977
Epoch 157: train_loss 0.03697814 val_loss 0.033199977
Epoch 158: train_loss 0.03692338 val_loss 0.03316104
Epoch 159: train_loss 0.036868654 val_loss 0.03312215
Epoch 160: train_loss 0.036813967 val_loss 0.033083316
Epoch 161: train_loss 0.036759317 val_loss 0.033044543
Epoch 162: train_loss 0.036704697 val_loss 0.033005822
Epoch 163: train_loss 0.03665012 val_loss 0.03296715
Epoch 164: train_loss 0.03659559 val_loss 0.032928538
Epoch 165: train_loss 0.03654109 val_loss 0.03288998
Epoch 166: train_loss 0.036486626 val_loss 0.032851487
Epoch 167: train_loss 0.036432207 val_loss 0.032813054
Epoch 168: train_loss 0.03637783 val_loss 0.032774683
Epoch 169: train_loss 0.036323488 val_loss 0.03273637
Epoch 170: train_loss 0.036269195 val_loss 0.032698125
Epoch 171: train_loss 0.03621494 val_loss 0.03265993
Epoch 172: train_loss 0.03616073 val_loss 0.03262181
Epoch 173: train_loss 0.036106564 val_loss 0.032583743
Epoch 174: train_loss 0.036052443 val_loss 0.032545734
Epoch 175: train_loss 0.035998367 val_loss 0.032507792
Epoch 176: train_loss 0.035944343 val_loss 0.03246991
Epoch 177: train_loss 0.035890356 val_loss 0.032432094
Epoch 178: train_loss 0.03583642 val_loss 0.03239434
Epoch 179: train_loss 0.03578254 val_loss 0.03235665
Epoch 180: train_loss 0.0357287 val_loss 0.032319024
Epoch 181: train_loss 0.035674915 val_loss 0.032281466
Epoch 182: train_loss 0.03562118 val_loss 0.03224398
Epoch 183: train_loss 0.0355675 val_loss 0.03220657
Epoch 184: train_loss 0.035513874 val_loss 0.032169234
Epoch 185: train_loss 0.0354603 val_loss 0.03213196
Epoch 186: train_loss 0.035406787 val_loss 0.032094743
Epoch 187: train_loss 0.035353325 val_loss 0.0320576
Epoch 188: train_loss 0.03529992 val_loss 0.03202053
Epoch 189: train_loss 0.035246577 val_loss 0.031983536
Epoch 190: train_loss 0.035193294 val_loss 0.031946614
Epoch 191: train_loss 0.03514007 val_loss 0.03190976
Epoch 192: train_loss 0.035086907 val_loss 0.031872995
Epoch 193: train_loss 0.035033807 val_loss 0.0318363
Epoch 194: train_loss 0.034980778 val_loss 0.031799693
Epoch 195: train_loss 0.034927808 val_loss 0.03176315
Epoch 196: train_loss 0.03487491 val_loss 0.03172669
Epoch 197: train_loss 0.034822073 val_loss 0.031690307
Epoch 198: train_loss 0.03476931 val_loss 0.031654008
Epoch 199: train_loss 0.034716617 val_loss 0.0316178
Epoch 200: train_loss 0.034663998 val_loss 0.031581674
Epoch 201: train_loss 0.03461145 val_loss 0.031545624
Epoch 202: train_loss 0.034558974 val_loss 0.03150965
Epoch 203: train_loss 0.034506578 val_loss 0.031473767
Epoch 204: train_loss 0.034454256 val_loss 0.031437963
Epoch 205: train_loss 0.034402017 val_loss 0.03140224
Epoch 206: train_loss 0.034349855 val_loss 0.031366613
Epoch 207: train_loss 0.034297775 val_loss 0.031331066
Epoch 208: train_loss 0.03424578 val_loss 0.031295627
Epoch 209: train_loss 0.034193866 val_loss 0.03126027
Epoch 210: train_loss 0.03414204 val_loss 0.031225001
Epoch 211: train_loss 0.0340903 val_loss 0.03118983
Epoch 212: train_loss 0.034038648 val_loss 0.031154757
Epoch 213: train_loss 0.03398709 val_loss 0.031119777
Epoch 214: train_loss 0.033935618 val_loss 0.031084891
Epoch 215: train_loss 0.033884242 val_loss 0.031050095
Epoch 216: train_loss 0.033832964 val_loss 0.031015405
Epoch 217: train_loss 0.033781778 val_loss 0.030980814
Epoch 218: train_loss 0.033730693 val_loss 0.03094632
Epoch 219: train_loss 0.03367971 val_loss 0.030911934
Epoch 220: train_loss 0.03362882 val_loss 0.030877635
Epoch 221: train_loss 0.03357804 val_loss 0.03084344
Epoch 222: train_loss 0.033527363 val_loss 0.030809354
Epoch 223: train_loss 0.03347679 val_loss 0.030775366
Epoch 224: train_loss 0.033426326 val_loss 0.030741492
Epoch 225: train_loss 0.033375967 val_loss 0.03070772
Epoch 226: train_loss 0.033325724 val_loss 0.03067406
Epoch 227: train_loss 0.033275593 val_loss 0.030640507
Epoch 228: train_loss 0.033225577 val_loss 0.030607067
Epoch 229: train_loss 0.033175677 val_loss 0.03057374
Epoch 230: train_loss 0.033125896 val_loss 0.030540526
Epoch 231: train_loss 0.03307623 val_loss 0.030507425
Epoch 232: train_loss 0.033026688 val_loss 0.03047444
Epoch 233: train_loss 0.032977268 val_loss 0.030441567
Epoch 234: train_loss 0.032927975 val_loss 0.030408815
Epoch 235: train_loss 0.03287881 val_loss 0.030376181
Epoch 236: train_loss 0.03282977 val_loss 0.030343667
Epoch 237: train_loss 0.032780863 val_loss 0.03031127
Epoch 238: train_loss 0.032732084 val_loss 0.03027899
Epoch 239: train_loss 0.032683443 val_loss 0.03024683
Epoch 240: train_loss 0.032634936 val_loss 0.030214798
Epoch 241: train_loss 0.03258657 val_loss 0.030182887
Epoch 242: train_loss 0.03253834 val_loss 0.030151099
Epoch 243: train_loss 0.032490253 val_loss 0.030119443
Epoch 244: train_loss 0.0324423 val_loss 0.03008792
Epoch 245: train_loss 0.032394506 val_loss 0.030056523
Epoch 246: train_loss 0.032346852 val_loss 0.030025253
Epoch 247: train_loss 0.032299343 val_loss 0.029994113
Epoch 248: train_loss 0.03225199 val_loss 0.029963097
Epoch 249: train_loss 0.032204784 val_loss 0.029932221
Epoch 250: train_loss 0.032157734 val_loss 0.029901488
Epoch 251: train_loss 0.032110844 val_loss 0.029870877
Epoch 252: train_loss 0.032064103 val_loss 0.029840406
Epoch 253: train_loss 0.03201753 val_loss 0.029810075
Epoch 254: train_loss 0.031971112 val_loss 0.029779881
Epoch 255: train_loss 0.03192486 val_loss 0.029749827
Epoch 256: train_loss 0.031878773 val_loss 0.029719917
Epoch 257: train_loss 0.03183285 val_loss 0.029690137
Epoch 258: train_loss 0.031787097 val_loss 0.029660499
Epoch 259: train_loss 0.031741515 val_loss 0.029631007
Epoch 260: train_loss 0.031696104 val_loss 0.029601658
Epoch 261: train_loss 0.03165087 val_loss 0.029572459
Epoch 262: train_loss 0.03160581 val_loss 0.029543404
Epoch 263: train_loss 0.03156093 val_loss 0.029514495
Epoch 264: train_loss 0.031516224 val_loss 0.029485743
Epoch 265: train_loss 0.031471707 val_loss 0.029457131
Epoch 266: train_loss 0.031427365 val_loss 0.029428678
Epoch 267: train_loss 0.03138321 val_loss 0.029400367
Epoch 268: train_loss 0.03133924 val_loss 0.029372212
Epoch 269: train_loss 0.03129546 val_loss 0.029344201
Epoch 270: train_loss 0.03125187 val_loss 0.029316342
Epoch 271: train_loss 0.031208467 val_loss 0.029288644
Epoch 272: train_loss 0.03116526 val_loss 0.029261103
Epoch 273: train_loss 0.031122252 val_loss 0.029233713
Epoch 274: train_loss 0.031079438 val_loss 0.029206483
Epoch 275: train_loss 0.031036818 val_loss 0.029179415
Epoch 276: train_loss 0.0309944 val_loss 0.029152505
Epoch 277: train_loss 0.030952185 val_loss 0.029125744
Epoch 278: train_loss 0.030910172 val_loss 0.029099144
Epoch 279: train_loss 0.030868366 val_loss 0.0290727
Epoch 280: train_loss 0.030826762 val_loss 0.029046422
Epoch 281: train_loss 0.030785369 val_loss 0.029020298
Epoch 282: train_loss 0.030744184 val_loss 0.028994352
Epoch 283: train_loss 0.03070321 val_loss 0.028968567
Epoch 284: train_loss 0.030662447 val_loss 0.028942939
Epoch 285: train_loss 0.030621897 val_loss 0.02891748
Epoch 286: train_loss 0.030581564 val_loss 0.028892187
Epoch 287: train_loss 0.030541448 val_loss 0.028867055
Epoch 288: train_loss 0.030501548 val_loss 0.028842093
Epoch 289: train_loss 0.03046187 val_loss 0.028817292
Epoch 290: train_loss 0.030422408 val_loss 0.02879267
Epoch 291: train_loss 0.03038317 val_loss 0.02876822
Epoch 292: train_loss 0.030344158 val_loss 0.028743934
Epoch 293: train_loss 0.030305367 val_loss 0.028719813
Epoch 294: train_loss 0.030266803 val_loss 0.02869586
Epoch 295: train_loss 0.030228466 val_loss 0.028672075
Epoch 296: train_loss 0.030190356 val_loss 0.028648455
Epoch 297: train_loss 0.030152474 val_loss 0.028625004
Epoch 298: train_loss 0.030114828 val_loss 0.028601723
Epoch 299: train_loss 0.030077405 val_loss 0.028578622
Epoch 300: train_loss 0.030040223 val_loss 0.02855568
Epoch 301: train_loss 0.030003268 val_loss 0.028532913
Epoch 302: train_loss 0.029966548 val_loss 0.028510327
Epoch 303: train_loss 0.029930066 val_loss 0.028487913
Epoch 304: train_loss 0.02989382 val_loss 0.028465668
Epoch 305: train_loss 0.02985781 val_loss 0.028443597
Epoch 306: train_loss 0.029822037 val_loss 0.028421704
Epoch 307: train_loss 0.029786505 val_loss 0.028399985
Epoch 308: train_loss 0.029751211 val_loss 0.02837844
Epoch 309: train_loss 0.02971616 val_loss 0.028357064
Epoch 310: train_loss 0.029681347 val_loss 0.028335854
Epoch 311: train_loss 0.029646777 val_loss 0.02831482
Epoch 312: train_loss 0.029612448 val_loss 0.028293958
Epoch 313: train_loss 0.029578364 val_loss 0.028273271
Epoch 314: train_loss 0.029544521 val_loss 0.028252752
Epoch 315: train_loss 0.029510925 val_loss 0.028232414
Epoch 316: train_loss 0.029477572 val_loss 0.028212246
Epoch 317: train_loss 0.029444464 val_loss 0.028192254
Epoch 318: train_loss 0.029411599 val_loss 0.028172439
Epoch 319: train_loss 0.029378984 val_loss 0.028152796
Epoch 320: train_loss 0.029346611 val_loss 0.028133323
Epoch 321: train_loss 0.029314486 val_loss 0.028114023
Epoch 322: train_loss 0.02928261 val_loss 0.028094893
Epoch 323: train_loss 0.02925098 val_loss 0.028075935
Epoch 324: train_loss 0.029219594 val_loss 0.028057156
Epoch 325: train_loss 0.029188458 val_loss 0.028038539
Epoch 326: train_loss 0.029157568 val_loss 0.028020097
Epoch 327: train_loss 0.029126927 val_loss 0.02800183
Epoch 328: train_loss 0.029096529 val_loss 0.027983729
Epoch 329: train_loss 0.029066382 val_loss 0.027965793
Epoch 330: train_loss 0.029036481 val_loss 0.027948031
Epoch 331: train_loss 0.029006828 val_loss 0.027930442
Epoch 332: train_loss 0.02897742 val_loss 0.02791302
Epoch 333: train_loss 0.02894826 val_loss 0.027895771
Epoch 334: train_loss 0.028919347 val_loss 0.027878687
Epoch 335: train_loss 0.02889068 val_loss 0.027861765
Epoch 336: train_loss 0.028862257 val_loss 0.027845021
Epoch 337: train_loss 0.028834078 val_loss 0.027828444
Epoch 338: train_loss 0.02880615 val_loss 0.027812026
Epoch 339: train_loss 0.028778464 val_loss 0.027795779
Epoch 340: train_loss 0.028751023 val_loss 0.027779687
Epoch 341: train_loss 0.028723825 val_loss 0.027763769
Epoch 342: train_loss 0.028696872 val_loss 0.027748007
Epoch 343: train_loss 0.028670158 val_loss 0.02773241
Epoch 344: train_loss 0.02864369 val_loss 0.02771698
Epoch 345: train_loss 0.028617462 val_loss 0.027701702
Epoch 346: train_loss 0.028591476 val_loss 0.027686581
Epoch 347: train_loss 0.028565727 val_loss 0.027671631
Epoch 348: train_loss 0.02854022 val_loss 0.02765684
Epoch 349: train_loss 0.028514951 val_loss 0.027642205
Epoch 350: train_loss 0.028489921 val_loss 0.027627723
Epoch 351: train_loss 0.028465126 val_loss 0.027613407
Epoch 352: train_loss 0.028440567 val_loss 0.027599238
Epoch 353: train_loss 0.028416242 val_loss 0.02758523
Epoch 354: train_loss 0.028392155 val_loss 0.02757137
Epoch 355: train_loss 0.028368298 val_loss 0.027557656
Epoch 356: train_loss 0.028344672 val_loss 0.027544105
Epoch 357: train_loss 0.02832128 val_loss 0.027530693
Epoch 358: train_loss 0.028298113 val_loss 0.027517444
Epoch 359: train_loss 0.02827518 val_loss 0.027504336
Epoch 360: train_loss 0.028252471 val_loss 0.027491381
Epoch 361: train_loss 0.028229991 val_loss 0.027478578
Epoch 362: train_loss 0.028207734 val_loss 0.027465919
Epoch 363: train_loss 0.028185703 val_loss 0.027453415
Epoch 364: train_loss 0.028163895 val_loss 0.027441049
Epoch 365: train_loss 0.028142307 val_loss 0.027428823
Epoch 366: train_loss 0.028120942 val_loss 0.027416741
Epoch 367: train_loss 0.028099794 val_loss 0.02740479
Epoch 368: train_loss 0.028078863 val_loss 0.02739298
Epoch 369: train_loss 0.028058149 val_loss 0.027381312
Epoch 370: train_loss 0.028037649 val_loss 0.027369779
Epoch 371: train_loss 0.02801736 val_loss 0.027358392
Epoch 372: train_loss 0.027997289 val_loss 0.027347144
Epoch 373: train_loss 0.027977422 val_loss 0.027336026
Epoch 374: train_loss 0.027957767 val_loss 0.027325032
Epoch 375: train_loss 0.02793832 val_loss 0.027314173
Epoch 376: train_loss 0.027919076 val_loss 0.027303452
Epoch 377: train_loss 0.02790004 val_loss 0.027292853
Epoch 378: train_loss 0.027881201 val_loss 0.027282378
Epoch 379: train_loss 0.027862567 val_loss 0.027272038
Epoch 380: train_loss 0.027844135 val_loss 0.027261827
Epoch 381: train_loss 0.027825896 val_loss 0.02725174
Epoch 382: train_loss 0.027807854 val_loss 0.027241787
Epoch 383: train_loss 0.027790006 val_loss 0.027231947
Epoch 384: train_loss 0.027772354 val_loss 0.027222227
Epoch 385: train_loss 0.027754892 val_loss 0.02721263
Epoch 386: train_loss 0.027737617 val_loss 0.027203145
Epoch 387: train_loss 0.027720533 val_loss 0.027193785
Epoch 388: train_loss 0.027703635 val_loss 0.027184535
Epoch 389: train_loss 0.027686916 val_loss 0.027175402
Epoch 390: train_loss 0.027670387 val_loss 0.027166376
Epoch 391: train_loss 0.027654035 val_loss 0.027157478
Epoch 392: train_loss 0.027637862 val_loss 0.027148684
Epoch 393: train_loss 0.027621869 val_loss 0.027140003
Epoch 394: train_loss 0.027606051 val_loss 0.027131433
Epoch 395: train_loss 0.027590405 val_loss 0.02712297
Epoch 396: train_loss 0.02757493 val_loss 0.027114617
Epoch 397: train_loss 0.02755963 val_loss 0.027106363
Epoch 398: train_loss 0.027544495 val_loss 0.027098233
Epoch 399: train_loss 0.02752953 val_loss 0.027090197
Epoch 400: train_loss 0.02751473 val_loss 0.027082259
Epoch 401: train_loss 0.027500091 val_loss 0.027074425
Epoch 402: train_loss 0.027485615 val_loss 0.027066683
Epoch 403: train_loss 0.027471298 val_loss 0.027059043
Epoch 404: train_loss 0.027457139 val_loss 0.027051494
Epoch 405: train_loss 0.027443137 val_loss 0.027044041
Epoch 406: train_loss 0.027429288 val_loss 0.027036691
Epoch 407: train_loss 0.027415592 val_loss 0.027029436
Epoch 408: train_loss 0.027402049 val_loss 0.027022274
Epoch 409: train_loss 0.027388655 val_loss 0.027015217
Epoch 410: train_loss 0.027375406 val_loss 0.027008245
Epoch 411: train_loss 0.027362304 val_loss 0.027001355
Epoch 412: train_loss 0.027349345 val_loss 0.026994552
Epoch 413: train_loss 0.02733653 val_loss 0.026987838
Epoch 414: train_loss 0.027323855 val_loss 0.02698121
Epoch 415: train_loss 0.027311318 val_loss 0.026974663
Epoch 416: train_loss 0.02729892 val_loss 0.026968196
Epoch 417: train_loss 0.027286654 val_loss 0.026961816
Epoch 418: train_loss 0.027274523 val_loss 0.026955526
Epoch 419: train_loss 0.027262524 val_loss 0.026949318
Epoch 420: train_loss 0.027250655 val_loss 0.026943186
Epoch 421: train_loss 0.027238917 val_loss 0.026937129
Epoch 422: train_loss 0.027227301 val_loss 0.02693115
Epoch 423: train_loss 0.027215812 val_loss 0.026925247
Epoch 424: train_loss 0.02720445 val_loss 0.026919411
Epoch 425: train_loss 0.027193204 val_loss 0.02691366
Epoch 426: train_loss 0.027182084 val_loss 0.02690798
Epoch 427: train_loss 0.027171077 val_loss 0.026902368
Epoch 428: train_loss 0.02716019 val_loss 0.026896829
Epoch 429: train_loss 0.027149417 val_loss 0.026891362
Epoch 430: train_loss 0.02713876 val_loss 0.026885962
Epoch 431: train_loss 0.027128214 val_loss 0.026880639
Epoch 432: train_loss 0.02711778 val_loss 0.026875375
Epoch 433: train_loss 0.027107453 val_loss 0.026870178
Epoch 434: train_loss 0.027097233 val_loss 0.026865045
Epoch 435: train_loss 0.027087122 val_loss 0.026859984
Epoch 436: train_loss 0.027077112 val_loss 0.026854983
Epoch 437: train_loss 0.02706721 val_loss 0.026850037
Epoch 438: train_loss 0.027057406 val_loss 0.026845153
Epoch 439: train_loss 0.027047701 val_loss 0.02684034
Epoch 440: train_loss 0.027038097 val_loss 0.026835583
Epoch 441: train_loss 0.02702859 val_loss 0.02683088
Epoch 442: train_loss 0.027019177 val_loss 0.026826236
Epoch 443: train_loss 0.027009862 val_loss 0.026821656
Epoch 444: train_loss 0.027000636 val_loss 0.026817124
Epoch 445: train_loss 0.026991501 val_loss 0.026812654
Epoch 446: train_loss 0.02698246 val_loss 0.02680824
Epoch 447: train_loss 0.026973508 val_loss 0.026803879
Epoch 448: train_loss 0.026964642 val_loss 0.026799573
Epoch 449: train_loss 0.026955862 val_loss 0.026795313
Epoch 450: train_loss 0.026947167 val_loss 0.026791107
Epoch 451: train_loss 0.026938554 val_loss 0.026786951
Epoch 452: train_loss 0.026930025 val_loss 0.026782839
Epoch 453: train_loss 0.026921578 val_loss 0.026778782
Epoch 454: train_loss 0.02691321 val_loss 0.026774766
Epoch 455: train_loss 0.02690492 val_loss 0.026770804
Epoch 456: train_loss 0.026896708 val_loss 0.026766885
Epoch 457: train_loss 0.026888574 val_loss 0.026763024
Epoch 458: train_loss 0.026880514 val_loss 0.0267592
Epoch 459: train_loss 0.026872527 val_loss 0.02675543
Epoch 460: train_loss 0.02686461 val_loss 0.026751706
Epoch 461: train_loss 0.02685677 val_loss 0.026748018
Epoch 462: train_loss 0.026848998 val_loss 0.02674437
Epoch 463: train_loss 0.026841294 val_loss 0.026740761
Epoch 464: train_loss 0.02683366 val_loss 0.02673719
Epoch 465: train_loss 0.026826095 val_loss 0.026733669
Epoch 466: train_loss 0.026818594 val_loss 0.02673019
Epoch 467: train_loss 0.02681116 val_loss 0.026726754
Epoch 468: train_loss 0.026803788 val_loss 0.026723355
Epoch 469: train_loss 0.02679648 val_loss 0.026719995
Epoch 470: train_loss 0.026789233 val_loss 0.026716668
Epoch 471: train_loss 0.026782049 val_loss 0.02671338
Epoch 472: train_loss 0.026774924 val_loss 0.026710134
Epoch 473: train_loss 0.02676786 val_loss 0.02670692
Epoch 474: train_loss 0.026760852 val_loss 0.02670374
Epoch 475: train_loss 0.026753902 val_loss 0.026700595
Epoch 476: train_loss 0.026747009 val_loss 0.026697483
Epoch 477: train_loss 0.026740171 val_loss 0.026694408
Epoch 478: train_loss 0.02673339 val_loss 0.02669137
Epoch 479: train_loss 0.02672666 val_loss 0.026688362
Epoch 480: train_loss 0.026719984 val_loss 0.026685392
Epoch 481: train_loss 0.02671336 val_loss 0.026682453
Epoch 482: train_loss 0.026706785 val_loss 0.026679555
Epoch 483: train_loss 0.026700266 val_loss 0.026676673
Epoch 484: train_loss 0.026693791 val_loss 0.026673824
Epoch 485: train_loss 0.026687369 val_loss 0.026671011
Epoch 486: train_loss 0.026680993 val_loss 0.026668226
Epoch 487: train_loss 0.026674666 val_loss 0.026665468
Epoch 488: train_loss 0.026668385 val_loss 0.026662745
Epoch 489: train_loss 0.026662149 val_loss 0.02666005
Epoch 490: train_loss 0.026655959 val_loss 0.026657382
Epoch 491: train_loss 0.026649814 val_loss 0.026654746
Epoch 492: train_loss 0.026643712 val_loss 0.026652131
Epoch 493: train_loss 0.026637655 val_loss 0.026649546
Epoch 494: train_loss 0.026631638 val_loss 0.026646987
Epoch 495: train_loss 0.026625663 val_loss 0.026644455
Epoch 496: train_loss 0.026619729 val_loss 0.026641948
Epoch 497: train_loss 0.026613839 val_loss 0.026639469
Epoch 498: train_loss 0.026607987 val_loss 0.026637018
Epoch 499: train_loss 0.026602171 val_loss 0.026634587
Epoch 500: train_loss 0.026596397 val_loss 0.02663218
Epoch 501: train_loss 0.02659066 val_loss 0.026629796
Epoch 502: train_loss 0.02658496 val_loss 0.026627446
Epoch 503: train_loss 0.026579298 val_loss 0.026625117
Epoch 504: train_loss 0.026573673 val_loss 0.026622815
Epoch 505: train_loss 0.026568085 val_loss 0.02662053
Epoch 506: train_loss 0.026562529 val_loss 0.026618272
Epoch 507: train_loss 0.02655701 val_loss 0.026616031
Epoch 508: train_loss 0.026551522 val_loss 0.026613815
Epoch 509: train_loss 0.026546072 val_loss 0.026611624
Epoch 510: train_loss 0.026540654 val_loss 0.026609452
Epoch 511: train_loss 0.026535269 val_loss 0.0266073
Epoch 512: train_loss 0.026529916 val_loss 0.026605168
Epoch 513: train_loss 0.026524596 val_loss 0.026603056
Epoch 514: train_loss 0.026519302 val_loss 0.02660097
Epoch 515: train_loss 0.026514046 val_loss 0.026598904
Epoch 516: train_loss 0.026508816 val_loss 0.026596861
Epoch 517: train_loss 0.026503619 val_loss 0.026594838
Epoch 518: train_loss 0.02649845 val_loss 0.026592836
Epoch 519: train_loss 0.02649331 val_loss 0.02659085
Epoch 520: train_loss 0.0264882 val_loss 0.02658888
Epoch 521: train_loss 0.026483119 val_loss 0.026586933
Epoch 522: train_loss 0.026478063 val_loss 0.02658501
Epoch 523: train_loss 0.026473038 val_loss 0.026583098
Epoch 524: train_loss 0.026468039 val_loss 0.026581205
Epoch 525: train_loss 0.026463065 val_loss 0.026579335
Epoch 526: train_loss 0.026458118 val_loss 0.026577486
Epoch 527: train_loss 0.026453199 val_loss 0.026575653
Epoch 528: train_loss 0.026448306 val_loss 0.02657384
Epoch 529: train_loss 0.026443437 val_loss 0.026572037
Epoch 530: train_loss 0.026438594 val_loss 0.026570255
Epoch 531: train_loss 0.026433773 val_loss 0.0265685
Epoch 532: train_loss 0.02642898 val_loss 0.026566768
Epoch 533: train_loss 0.026424209 val_loss 0.026565041
Epoch 534: train_loss 0.026419464 val_loss 0.026563326
Epoch 535: train_loss 0.026414745 val_loss 0.026561629
Epoch 536: train_loss 0.026410043 val_loss 0.026559949
Epoch 537: train_loss 0.026405368 val_loss 0.02655828
Epoch 538: train_loss 0.026400715 val_loss 0.026556635
Epoch 539: train_loss 0.026396085 val_loss 0.026555007
Epoch 540: train_loss 0.026391476 val_loss 0.026553398
Epoch 541: train_loss 0.026386889 val_loss 0.0265518
Epoch 542: train_loss 0.026382323 val_loss 0.026550222
Epoch 543: train_loss 0.026377779 val_loss 0.02654866
Epoch 544: train_loss 0.026373256 val_loss 0.026547104
Epoch 545: train_loss 0.026368754 val_loss 0.026545567
Epoch 546: train_loss 0.026364274 val_loss 0.026544053
Epoch 547: train_loss 0.026359811 val_loss 0.026542548
Epoch 548: train_loss 0.02635537 val_loss 0.026541067
Epoch 549: train_loss 0.02635095 val_loss 0.026539596
Epoch 550: train_loss 0.026346551 val_loss 0.02653814
Epoch 551: train_loss 0.026342168 val_loss 0.026536694
Epoch 552: train_loss 0.026337806 val_loss 0.026535265
Epoch 553: train_loss 0.026333462 val_loss 0.026533846
Epoch 554: train_loss 0.02632914 val_loss 0.026532441
Epoch 555: train_loss 0.026324835 val_loss 0.026531056
Epoch 556: train_loss 0.026320545 val_loss 0.026529687
Epoch 557: train_loss 0.026316278 val_loss 0.026528336
Epoch 558: train_loss 0.026312027 val_loss 0.026526997
Epoch 559: train_loss 0.026307793 val_loss 0.02652567
Epoch 560: train_loss 0.026303578 val_loss 0.026524354
Epoch 561: train_loss 0.026299382 val_loss 0.02652304
Epoch 562: train_loss 0.0262952 val_loss 0.026521752
Epoch 563: train_loss 0.026291039 val_loss 0.026520472
Epoch 564: train_loss 0.026286893 val_loss 0.026519205
Epoch 565: train_loss 0.026282761 val_loss 0.026517957
Epoch 566: train_loss 0.02627865 val_loss 0.026516728
Epoch 567: train_loss 0.026274554 val_loss 0.026515514
Epoch 568: train_loss 0.026270472 val_loss 0.026514307
Epoch 569: train_loss 0.026266411 val_loss 0.026513113
Epoch 570: train_loss 0.026262365 val_loss 0.026511922
Epoch 571: train_loss 0.02625833 val_loss 0.026510749
Epoch 572: train_loss 0.026254315 val_loss 0.026509594
Epoch 573: train_loss 0.026250314 val_loss 0.026508452
Epoch 574: train_loss 0.026246332 val_loss 0.026507318
Epoch 575: train_loss 0.026242364 val_loss 0.026506197
Epoch 576: train_loss 0.026238408 val_loss 0.026505094
Epoch 577: train_loss 0.02623447 val_loss 0.026503999
Epoch 578: train_loss 0.026230548 val_loss 0.026502915
Epoch 579: train_loss 0.02622664 val_loss 0.026501851
Epoch 580: train_loss 0.026222747 val_loss 0.026500793
Epoch 581: train_loss 0.026218867 val_loss 0.026499754
Epoch 582: train_loss 0.026215 val_loss 0.026498716
Epoch 583: train_loss 0.026211152 val_loss 0.026497692
Epoch 584: train_loss 0.026207317 val_loss 0.02649668
Epoch 585: train_loss 0.026203493 val_loss 0.026495682
Epoch 586: train_loss 0.026199687 val_loss 0.026494697
Epoch 587: train_loss 0.026195891 val_loss 0.026493723
Epoch 588: train_loss 0.026192112 val_loss 0.026492758
Epoch 589: train_loss 0.026188347 val_loss 0.026491806
Epoch 590: train_loss 0.026184592 val_loss 0.026490869
Epoch 591: train_loss 0.026180854 val_loss 0.026489936
Epoch 592: train_loss 0.026177129 val_loss 0.026489021
Epoch 593: train_loss 0.026173417 val_loss 0.026488116
Epoch 594: train_loss 0.026169717 val_loss 0.026487231
Epoch 595: train_loss 0.026166031 val_loss 0.026486346
Epoch 596: train_loss 0.02616236 val_loss 0.026485475
Epoch 597: train_loss 0.026158698 val_loss 0.026484618
Epoch 598: train_loss 0.026155053 val_loss 0.02648377
Epoch 599: train_loss 0.026151419 val_loss 0.02648293
Epoch 600: train_loss 0.026147798 val_loss 0.026482102
Epoch 601: train_loss 0.02614419 val_loss 0.026481282
Epoch 602: train_loss 0.026140593 val_loss 0.026480477
Epoch 603: train_loss 0.026137007 val_loss 0.026479686
Epoch 604: train_loss 0.026133437 val_loss 0.026478903
Epoch 605: train_loss 0.026129877 val_loss 0.026478132
Epoch 606: train_loss 0.026126333 val_loss 0.026477376
Epoch 607: train_loss 0.026122797 val_loss 0.026476623
Epoch 608: train_loss 0.026119277 val_loss 0.026475882
Epoch 609: train_loss 0.026115764 val_loss 0.026475152
Epoch 610: train_loss 0.026112266 val_loss 0.026474433
Epoch 611: train_loss 0.026108779 val_loss 0.026473727
Epoch 612: train_loss 0.026105303 val_loss 0.026473029
Epoch 613: train_loss 0.026101839 val_loss 0.026472336
Epoch 614: train_loss 0.02609839 val_loss 0.026471656
Epoch 615: train_loss 0.026094947 val_loss 0.026470989
Epoch 616: train_loss 0.026091522 val_loss 0.026470333
Epoch 617: train_loss 0.026088104 val_loss 0.026469687
Epoch 618: train_loss 0.026084699 val_loss 0.02646905
Epoch 619: train_loss 0.026081303 val_loss 0.026468422
Epoch 620: train_loss 0.02607792 val_loss 0.026467804
Epoch 621: train_loss 0.02607455 val_loss 0.026467193
Epoch 622: train_loss 0.026071187 val_loss 0.026466593
Epoch 623: train_loss 0.026067838 val_loss 0.026466006
Epoch 624: train_loss 0.0260645 val_loss 0.026465433
Epoch 625: train_loss 0.02606117 val_loss 0.026464865
Epoch 626: train_loss 0.026057854 val_loss 0.026464306
Epoch 627: train_loss 0.02605455 val_loss 0.026463754
Epoch 628: train_loss 0.026051253 val_loss 0.026463209
Epoch 629: train_loss 0.02604797 val_loss 0.026462676
Epoch 630: train_loss 0.026044693 val_loss 0.026462153
Epoch 631: train_loss 0.02604143 val_loss 0.026461635
Epoch 632: train_loss 0.026038177 val_loss 0.026461137
Epoch 633: train_loss 0.026034936 val_loss 0.026460646
Epoch 634: train_loss 0.026031705 val_loss 0.026460163
Epoch 635: train_loss 0.026028482 val_loss 0.026459692
Epoch 636: train_loss 0.02602527 val_loss 0.026459234
Epoch 637: train_loss 0.02602207 val_loss 0.026458776
Epoch 638: train_loss 0.026018877 val_loss 0.026458332
Epoch 639: train_loss 0.026015695 val_loss 0.02645789
Epoch 640: train_loss 0.026012529 val_loss 0.026457455
Epoch 641: train_loss 0.026009366 val_loss 0.026457036
Epoch 642: train_loss 0.026006216 val_loss 0.026456624
Epoch 643: train_loss 0.026003078 val_loss 0.026456214
Epoch 644: train_loss 0.025999947 val_loss 0.026455829
Epoch 645: train_loss 0.025996827 val_loss 0.026455453
Epoch 646: train_loss 0.025993714 val_loss 0.02645508
Epoch 647: train_loss 0.025990613 val_loss 0.026454717
Epoch 648: train_loss 0.025987525 val_loss 0.026454352
Epoch 649: train_loss 0.025984442 val_loss 0.026453994
Epoch 650: train_loss 0.025981368 val_loss 0.026453655
Epoch 651: train_loss 0.025978308 val_loss 0.02645332
Epoch 652: train_loss 0.025975253 val_loss 0.026452992
Epoch 653: train_loss 0.02597221 val_loss 0.026452674
Epoch 654: train_loss 0.025969177 val_loss 0.026452374
Epoch 655: train_loss 0.025966153 val_loss 0.02645207
Epoch 656: train_loss 0.025963139 val_loss 0.026451783
Epoch 657: train_loss 0.025960132 val_loss 0.0264515
Epoch 658: train_loss 0.025957137 val_loss 0.02645122
Epoch 659: train_loss 0.02595415 val_loss 0.026450953
Epoch 660: train_loss 0.025951171 val_loss 0.026450695
Epoch 661: train_loss 0.025948204 val_loss 0.02645045
Epoch 662: train_loss 0.025945244 val_loss 0.02645021
Epoch 663: train_loss 0.025942292 val_loss 0.026449976
Epoch 664: train_loss 0.025939353 val_loss 0.026449747
Epoch 665: train_loss 0.025936421 val_loss 0.026449531
Epoch 666: train_loss 0.025933497 val_loss 0.026449323
Epoch 667: train_loss 0.025930583 val_loss 0.026449116
Epoch 668: train_loss 0.025927676 val_loss 0.02644892
Epoch 669: train_loss 0.025924781 val_loss 0.026448727
Epoch 670: train_loss 0.025921892 val_loss 0.02644855
Epoch 671: train_loss 0.025919013 val_loss 0.026448382
Epoch 672: train_loss 0.025916142 val_loss 0.026448214
Epoch 673: train_loss 0.025913281 val_loss 0.026448058
Epoch 674: train_loss 0.025910428 val_loss 0.026447905
Epoch 675: train_loss 0.025907584 val_loss 0.026447764
Epoch 676: train_loss 0.025904749 val_loss 0.026447624
Epoch 677: train_loss 0.025901921 val_loss 0.0264475
Epoch 678: train_loss 0.025899101 val_loss 0.026447382
Epoch 679: train_loss 0.025896292 val_loss 0.026447276
Epoch 680: train_loss 0.02589349 val_loss 0.026447171
Epoch 681: train_loss 0.025890695 val_loss 0.02644707
Epoch 682: train_loss 0.02588791 val_loss 0.02644698
Epoch 683: train_loss 0.025885135 val_loss 0.026446892
Epoch 684: train_loss 0.025882367 val_loss 0.026446817
Epoch 685: train_loss 0.025879603 val_loss 0.026446749
Epoch 686: train_loss 0.025876854 val_loss 0.026446685
Epoch 687: train_loss 0.025874108 val_loss 0.02644663
Epoch 688: train_loss 0.025871374 val_loss 0.026446579
Epoch 689: train_loss 0.025868647 val_loss 0.026446538
Epoch 690: train_loss 0.025865927 val_loss 0.026446497
Epoch 691: train_loss 0.025863215 val_loss 0.026446471
Epoch 692: train_loss 0.02586051 val_loss 0.026446447
Epoch 693: train_loss 0.025857817 val_loss 0.026446426
Epoch 694: train_loss 0.02585513 val_loss 0.026446415
Epoch 695: train_loss 0.02585245 val_loss 0.026446419
Epoch 696: train_loss 0.025849776 val_loss 0.026446428
Epoch 697: train_loss 0.025847113 val_loss 0.026446441
Epoch 698: train_loss 0.025844458 val_loss 0.026446447
Epoch 699: train_loss 0.025841808 val_loss 0.026446464
Epoch 700: train_loss 0.025839169 val_loss 0.026446495
Epoch 701: train_loss 0.025836535 val_loss 0.02644653
Epoch 702: train_loss 0.025833908 val_loss 0.026446572
Epoch 703: train_loss 0.025831291 val_loss 0.02644662
Epoch 704: train_loss 0.025828682 val_loss 0.026446676
Epoch 705: train_loss 0.025826078 val_loss 0.026446735
Epoch 706: train_loss 0.025823483 val_loss 0.026446803
Epoch 707: train_loss 0.025820896 val_loss 0.026446875
Epoch 708: train_loss 0.025818314 val_loss 0.026446946
Epoch 709: train_loss 0.025815742 val_loss 0.026447035
Epoch 710: train_loss 0.025813177 val_loss 0.026447123
Epoch 711: train_loss 0.02581062 val_loss 0.026447216
Epoch 712: train_loss 0.025808068 val_loss 0.026447324
Epoch 713: train_loss 0.025805525 val_loss 0.026447432
Epoch 714: train_loss 0.025802989 val_loss 0.026447553
Epoch 715: train_loss 0.02580046 val_loss 0.026447669
Epoch 716: train_loss 0.025797939 val_loss 0.02644779
Epoch 717: train_loss 0.025795422 val_loss 0.026447915
Epoch 718: train_loss 0.02579292 val_loss 0.026448052
Epoch 719: train_loss 0.025790416 val_loss 0.026448192
Epoch 720: train_loss 0.025787923 val_loss 0.026448345
Epoch 721: train_loss 0.025785439 val_loss 0.0264485
Epoch 722: train_loss 0.025782958 val_loss 0.026448654
Epoch 723: train_loss 0.025780486 val_loss 0.026448822
Epoch 724: train_loss 0.025778022 val_loss 0.026448982
Epoch 725: train_loss 0.025775563 val_loss 0.026449155
Epoch 726: train_loss 0.025773112 val_loss 0.026449334
Epoch 727: train_loss 0.025770668 val_loss 0.026449515
Epoch 728: train_loss 0.025768232 val_loss 0.02644971
Epoch 729: train_loss 0.025765799 val_loss 0.026449906
Epoch 730: train_loss 0.025763378 val_loss 0.026450103
Epoch 731: train_loss 0.025760958 val_loss 0.026450302
Epoch 732: train_loss 0.025758546 val_loss 0.02645051
Epoch 733: train_loss 0.025756143 val_loss 0.026450727
Epoch 734: train_loss 0.025753746 val_loss 0.026450943
Epoch 735: train_loss 0.025751356 val_loss 0.026451163
Epoch 736: train_loss 0.025748972 val_loss 0.026451394
Epoch 737: train_loss 0.025746593 val_loss 0.02645163
Epoch 738: train_loss 0.025744222 val_loss 0.026451873
Epoch 739: train_loss 0.02574186 val_loss 0.026452115
Epoch 740: train_loss 0.0257395 val_loss 0.02645236
Epoch 741: train_loss 0.02573715 val_loss 0.026452601
Epoch 742: train_loss 0.025734803 val_loss 0.02645285
Epoch 743: train_loss 0.025732465 val_loss 0.026453113
Epoch 744: train_loss 0.025730133 val_loss 0.026453385
Epoch 745: train_loss 0.025727807 val_loss 0.026453657
Epoch 746: train_loss 0.025725488 val_loss 0.026453938
Epoch 747: train_loss 0.025723174 val_loss 0.026454212
Epoch 748: train_loss 0.025720865 val_loss 0.026454492
Epoch 749: train_loss 0.025718566 val_loss 0.026454777
Epoch 750: train_loss 0.025716271 val_loss 0.026455069
Epoch 751: train_loss 0.025713984 val_loss 0.026455356
Epoch 752: train_loss 0.0257117 val_loss 0.026455658
Epoch 753: train_loss 0.025709424 val_loss 0.02645597
Epoch 754: train_loss 0.025707152 val_loss 0.026456283
Epoch 755: train_loss 0.02570489 val_loss 0.026456598
Epoch 756: train_loss 0.02570263 val_loss 0.026456911
Epoch 757: train_loss 0.02570038 val_loss 0.026457224
Epoch 758: train_loss 0.025698131 val_loss 0.02645755
Epoch 759: train_loss 0.02569589 val_loss 0.026457876
Epoch 760: train_loss 0.025693655 val_loss 0.02645821
Epoch 761: train_loss 0.025691427 val_loss 0.026458541
Epoch 762: train_loss 0.025689203 val_loss 0.02645888
Epoch 763: train_loss 0.025686987 val_loss 0.026459217
Epoch 764: train_loss 0.025684778 val_loss 0.026459564
Epoch 765: train_loss 0.025682572 val_loss 0.026459921
Epoch 766: train_loss 0.025680372 val_loss 0.026460275
Epoch 767: train_loss 0.025678176 val_loss 0.026460625
Epoch 768: train_loss 0.02567599 val_loss 0.02646099
Epoch 769: train_loss 0.025673807 val_loss 0.026461363
Epoch 770: train_loss 0.025671631 val_loss 0.026461726
Epoch 771: train_loss 0.02566946 val_loss 0.026462095
Epoch 772: train_loss 0.025667293 val_loss 0.02646247
Epoch 773: train_loss 0.025665134 val_loss 0.02646284
Epoch 774: train_loss 0.025662977 val_loss 0.026463218
Epoch 775: train_loss 0.02566083 val_loss 0.026463607
Epoch 776: train_loss 0.025658686 val_loss 0.026463998
Epoch 777: train_loss 0.025656547 val_loss 0.02646439
Epoch 778: train_loss 0.025654417 val_loss 0.026464785
Epoch 779: train_loss 0.025652288 val_loss 0.026465174
Epoch 780: train_loss 0.025650164 val_loss 0.026465574
Epoch 781: train_loss 0.02564805 val_loss 0.026465978
Epoch 782: train_loss 0.025645938 val_loss 0.02646638
Epoch 783: train_loss 0.025643831 val_loss 0.026466792
Epoch 784: train_loss 0.02564173 val_loss 0.026467202
Epoch 785: train_loss 0.025639635 val_loss 0.026467614
Epoch 786: train_loss 0.025637543 val_loss 0.026468039
Epoch 787: train_loss 0.025635459 val_loss 0.026468461
Epoch 788: train_loss 0.02563338 val_loss 0.02646888
Epoch 789: train_loss 0.025631305 val_loss 0.026469301
Epoch 790: train_loss 0.025629234 val_loss 0.026469728
Epoch 791: train_loss 0.02562717 val_loss 0.026470164
Epoch 792: train_loss 0.02562511 val_loss 0.026470605
Epoch 793: train_loss 0.025623055 val_loss 0.026471047
Epoch 794: train_loss 0.025621004 val_loss 0.026471484
Epoch 795: train_loss 0.025618961 val_loss 0.026471924
Epoch 796: train_loss 0.025616921 val_loss 0.026472367
Epoch 797: train_loss 0.025614887 val_loss 0.026472814
Epoch 798: train_loss 0.025612857 val_loss 0.026473261
Epoch 799: train_loss 0.02561083 val_loss 0.026473705
Epoch 800: train_loss 0.025608812 val_loss 0.026474157
Epoch 801: train_loss 0.025606796 val_loss 0.02647462
Epoch 802: train_loss 0.025604788 val_loss 0.026475081
Epoch 803: train_loss 0.02560278 val_loss 0.026475547
Epoch 804: train_loss 0.02560078 val_loss 0.026476009
Epoch 805: train_loss 0.025598783 val_loss 0.026476465
Epoch 806: train_loss 0.02559679 val_loss 0.026476927
Epoch 807: train_loss 0.025594804 val_loss 0.026477393
Epoch 808: train_loss 0.025592823 val_loss 0.026477866
Epoch 809: train_loss 0.025590844 val_loss 0.026478345
Epoch 810: train_loss 0.025588874 val_loss 0.026478823
Epoch 811: train_loss 0.025586905 val_loss 0.026479308
Epoch 812: train_loss 0.02558494 val_loss 0.026479784
Epoch 813: train_loss 0.025582984 val_loss 0.026480256
Epoch 814: train_loss 0.025581028 val_loss 0.02648074
Epoch 815: train_loss 0.025579076 val_loss 0.026481224
Epoch 816: train_loss 0.025577132 val_loss 0.026481705
Epoch 817: train_loss 0.02557519 val_loss 0.026482193
Epoch 818: train_loss 0.025573255 val_loss 0.02648269
Epoch 819: train_loss 0.02557132 val_loss 0.026483191
Epoch 820: train_loss 0.025569394 val_loss 0.02648369
Epoch 821: train_loss 0.025567472 val_loss 0.026484182
Epoch 822: train_loss 0.025565553 val_loss 0.026484676
Epoch 823: train_loss 0.025563637 val_loss 0.026485166
Epoch 824: train_loss 0.025561728 val_loss 0.026485663
Epoch 825: train_loss 0.025559818 val_loss 0.026486164
Epoch 826: train_loss 0.025557918 val_loss 0.02648667
Epoch 827: train_loss 0.025556022 val_loss 0.026487183
Epoch 828: train_loss 0.025554128 val_loss 0.026487688
Epoch 829: train_loss 0.025552237 val_loss 0.026488192
Epoch 830: train_loss 0.025550352 val_loss 0.026488703
Epoch 831: train_loss 0.025548473 val_loss 0.026489213
Epoch 832: train_loss 0.025546597 val_loss 0.026489723
Epoch 833: train_loss 0.025544725 val_loss 0.026490236
Epoch 834: train_loss 0.025542855 val_loss 0.02649075
Epoch 835: train_loss 0.025540989 val_loss 0.026491271
Epoch 836: train_loss 0.025539128 val_loss 0.026491797
Epoch 837: train_loss 0.025537273 val_loss 0.026492318
Epoch 838: train_loss 0.02553542 val_loss 0.02649284
Epoch 839: train_loss 0.025533572 val_loss 0.026493357
Epoch 840: train_loss 0.025531728 val_loss 0.026493872
Epoch 841: train_loss 0.025529888 val_loss 0.026494386
Epoch 842: train_loss 0.02552805 val_loss 0.026494913
Epoch 843: train_loss 0.025526216 val_loss 0.026495442
Epoch 844: train_loss 0.025524385 val_loss 0.026495976
Epoch 845: train_loss 0.025522562 val_loss 0.02649651
Epoch 846: train_loss 0.025520742 val_loss 0.02649704
Epoch 847: train_loss 0.025518924 val_loss 0.026497565
Epoch 848: train_loss 0.02551711 val_loss 0.026498085
Epoch 849: train_loss 0.0255153 val_loss 0.02649861
Epoch 850: train_loss 0.025513494 val_loss 0.026499143
Epoch 851: train_loss 0.02551169 val_loss 0.026499683
Epoch 852: train_loss 0.025509892 val_loss 0.026500225
Epoch 853: train_loss 0.025508096 val_loss 0.026500767
Epoch 854: train_loss 0.025506305 val_loss 0.026501296
Epoch 855: train_loss 0.025504516 val_loss 0.026501827
Epoch 856: train_loss 0.02550273 val_loss 0.026502358
Epoch 857: train_loss 0.02550095 val_loss 0.026502898
Epoch 858: train_loss 0.025499173 val_loss 0.026503433
Epoch 859: train_loss 0.025497401 val_loss 0.026503965
Epoch 860: train_loss 0.02549563 val_loss 0.026504504
Epoch 861: train_loss 0.025493864 val_loss 0.026505047
Epoch 862: train_loss 0.0254921 val_loss 0.026505599
Epoch 863: train_loss 0.02549034 val_loss 0.026506132
Epoch 864: train_loss 0.025488585 val_loss 0.026506662
Epoch 865: train_loss 0.02548683 val_loss 0.026507204
Epoch 866: train_loss 0.02548508 val_loss 0.026507748
Epoch 867: train_loss 0.025483336 val_loss 0.026508287
Epoch 868: train_loss 0.025481593 val_loss 0.026508832
Epoch 869: train_loss 0.025479853 val_loss 0.026509376
Epoch 870: train_loss 0.025478119 val_loss 0.026509924
Epoch 871: train_loss 0.025476387 val_loss 0.026510464
Epoch 872: train_loss 0.025474656 val_loss 0.026511
Epoch 873: train_loss 0.025472932 val_loss 0.026511546
Epoch 874: train_loss 0.025471207 val_loss 0.026512085
Epoch 875: train_loss 0.02546949 val_loss 0.026512634
Epoch 876: train_loss 0.025467772 val_loss 0.026513176
Epoch 877: train_loss 0.02546606 val_loss 0.02651372
Epoch 878: train_loss 0.025464348 val_loss 0.026514264
Epoch 879: train_loss 0.025462642 val_loss 0.026514808
Epoch 880: train_loss 0.025460938 val_loss 0.026515352
Epoch 881: train_loss 0.02545924 val_loss 0.026515897
Epoch 882: train_loss 0.025457539 val_loss 0.026516438
Epoch 883: train_loss 0.025455847 val_loss 0.026516976
Epoch 884: train_loss 0.025454156 val_loss 0.026517527
Epoch 885: train_loss 0.025452469 val_loss 0.026518077
Epoch 886: train_loss 0.025450783 val_loss 0.02651862
Epoch 887: train_loss 0.0254491 val_loss 0.026519164
Epoch 888: train_loss 0.02544742 val_loss 0.026519699
Epoch 889: train_loss 0.025445744 val_loss 0.026520245
Epoch 890: train_loss 0.025444072 val_loss 0.026520792
Epoch 891: train_loss 0.025442401 val_loss 0.026521336
Epoch 892: train_loss 0.025440734 val_loss 0.026521882
Epoch 893: train_loss 0.02543907 val_loss 0.026522426
Epoch 894: train_loss 0.025437409 val_loss 0.026522968
Epoch 895: train_loss 0.02543575 val_loss 0.02652352
Epoch 896: train_loss 0.025434095 val_loss 0.02652406
Epoch 897: train_loss 0.025432441 val_loss 0.026524596
Epoch 898: train_loss 0.025430791 val_loss 0.026525136
Epoch 899: train_loss 0.025429145 val_loss 0.026525682
Epoch 900: train_loss 0.0254275 val_loss 0.026526231
Epoch 901: train_loss 0.025425859 val_loss 0.026526775
Epoch 902: train_loss 0.02542422 val_loss 0.026527304
Epoch 903: train_loss 0.025422584 val_loss 0.026527843
Epoch 904: train_loss 0.025420953 val_loss 0.026528394
Epoch 905: train_loss 0.025419319 val_loss 0.026528938
Epoch 906: train_loss 0.025417691 val_loss 0.02652948
Epoch 907: train_loss 0.025416065 val_loss 0.026530012
Epoch 908: train_loss 0.025414445 val_loss 0.02653055
Epoch 909: train_loss 0.025412824 val_loss 0.026531085
Epoch 910: train_loss 0.025411207 val_loss 0.026531622
Epoch 911: train_loss 0.02540959 val_loss 0.02653216
Epoch 912: train_loss 0.02540798 val_loss 0.026532698
Epoch 913: train_loss 0.025406368 val_loss 0.026533242
Epoch 914: train_loss 0.025404762 val_loss 0.026533782
Epoch 915: train_loss 0.025403157 val_loss 0.026534313
Epoch 916: train_loss 0.025401555 val_loss 0.026534839
Epoch 917: train_loss 0.025399957 val_loss 0.026535364
Epoch 918: train_loss 0.025398359 val_loss 0.026535893
Epoch 919: train_loss 0.025396764 val_loss 0.026536431
Epoch 920: train_loss 0.025395174 val_loss 0.02653697
Epoch 921: train_loss 0.025393583 val_loss 0.026537502
Epoch 922: train_loss 0.025391996 val_loss 0.026538033
Epoch 923: train_loss 0.025390413 val_loss 0.026538558
Epoch 924: train_loss 0.02538883 val_loss 0.026539082
Epoch 925: train_loss 0.02538725 val_loss 0.026539614
Epoch 926: train_loss 0.025385672 val_loss 0.026540136
Epoch 927: train_loss 0.025384096 val_loss 0.026540658
Epoch 928: train_loss 0.025382524 val_loss 0.026541183
Epoch 929: train_loss 0.025380954 val_loss 0.026541712
Epoch 930: train_loss 0.025379384 val_loss 0.026542243
Epoch 931: train_loss 0.025377817 val_loss 0.026542768
Epoch 932: train_loss 0.025376257 val_loss 0.026543288
Epoch 933: train_loss 0.025374694 val_loss 0.0265438
Epoch 934: train_loss 0.025373135 val_loss 0.026544316
Epoch 935: train_loss 0.025371578 val_loss 0.026544828
Epoch 936: train_loss 0.025370024 val_loss 0.026545355
Epoch 937: train_loss 0.02536847 val_loss 0.02654588
Epoch 938: train_loss 0.025366921 val_loss 0.026546396
Epoch 939: train_loss 0.025365371 val_loss 0.026546903
Epoch 940: train_loss 0.025363825 val_loss 0.026547417
Epoch 941: train_loss 0.025362283 val_loss 0.026547926
Epoch 942: train_loss 0.02536074 val_loss 0.026548434
Epoch 943: train_loss 0.025359202 val_loss 0.026548946
Epoch 944: train_loss 0.025357664 val_loss 0.026549455
Epoch 945: train_loss 0.025356129 val_loss 0.026549961
Epoch 946: train_loss 0.025354596 val_loss 0.026550472
Epoch 947: train_loss 0.025353065 val_loss 0.026550977
Epoch 948: train_loss 0.025351536 val_loss 0.026551485
Epoch 949: train_loss 0.025350008 val_loss 0.02655199
Epoch 950: train_loss 0.025348485 val_loss 0.026552478
Epoch 951: train_loss 0.02534696 val_loss 0.026552971
Epoch 952: train_loss 0.025345441 val_loss 0.02655347
Epoch 953: train_loss 0.025343921 val_loss 0.026553972
Epoch 954: train_loss 0.025342405 val_loss 0.026554475
Epoch 955: train_loss 0.025340889 val_loss 0.026554976
Epoch 956: train_loss 0.025339376 val_loss 0.026555466
Epoch 957: train_loss 0.025337866 val_loss 0.026555954
Epoch 958: train_loss 0.025336355 val_loss 0.026556442
Epoch 959: train_loss 0.02533485 val_loss 0.026556926
Epoch 960: train_loss 0.025333343 val_loss 0.026557406
Epoch 961: train_loss 0.02533184 val_loss 0.026557898
Epoch 962: train_loss 0.025330337 val_loss 0.026558388
Epoch 963: train_loss 0.025328837 val_loss 0.026558878
Epoch 964: train_loss 0.02532734 val_loss 0.026559368
Epoch 965: train_loss 0.025325846 val_loss 0.026559854
Epoch 966: train_loss 0.025324348 val_loss 0.026560325
Epoch 967: train_loss 0.025322858 val_loss 0.0265608
Epoch 968: train_loss 0.025321366 val_loss 0.026561275
Epoch 969: train_loss 0.025319878 val_loss 0.02656176
Epoch 970: train_loss 0.025318388 val_loss 0.026562234
Epoch 971: train_loss 0.025316905 val_loss 0.026562711
Epoch 972: train_loss 0.02531542 val_loss 0.02656318
Epoch 973: train_loss 0.02531394 val_loss 0.026563643
Epoch 974: train_loss 0.02531246 val_loss 0.026564106
Epoch 975: train_loss 0.02531098 val_loss 0.026564574
Epoch 976: train_loss 0.025309503 val_loss 0.026565043
Epoch 977: train_loss 0.02530803 val_loss 0.02656552
Epoch 978: train_loss 0.025306556 val_loss 0.026565976
Epoch 979: train_loss 0.025305085 val_loss 0.026566438
Epoch 980: train_loss 0.025303613 val_loss 0.026566893
Epoch 981: train_loss 0.025302146 val_loss 0.026567345
Epoch 982: train_loss 0.025300678 val_loss 0.026567806
Epoch 983: train_loss 0.025299214 val_loss 0.026568264
Epoch 984: train_loss 0.02529775 val_loss 0.026568707
Epoch 985: train_loss 0.02529629 val_loss 0.026569165
Epoch 986: train_loss 0.02529483 val_loss 0.02656961
Epoch 987: train_loss 0.025293367 val_loss 0.026570072
Epoch 988: train_loss 0.02529191 val_loss 0.026570521
Epoch 989: train_loss 0.025290456 val_loss 0.026570963
Epoch 990: train_loss 0.025289 val_loss 0.026571406
Epoch 991: train_loss 0.025287546 val_loss 0.026571838
Epoch 992: train_loss 0.025286097 val_loss 0.026572285
Epoch 993: train_loss 0.025284646 val_loss 0.02657273
Epoch 994: train_loss 0.025283199 val_loss 0.026573172
Epoch 995: train_loss 0.02528175 val_loss 0.026573606
Epoch 996: train_loss 0.025280306 val_loss 0.026574032
Epoch 997: train_loss 0.025278863 val_loss 0.026574463
Epoch 998: train_loss 0.02527742 val_loss 0.026574891
Epoch 999: train_loss 0.02527598 val_loss 0.02657532
Epoch 1000: train_loss 0.025274541 val_loss 0.026575739
Epoch 1001: train_loss 0.025273103 val_loss 0.026576169
Epoch 1002: train_loss 0.025271665 val_loss 0.026576588
Epoch 1003: train_loss 0.02527023 val_loss 0.026577014
Epoch 1004: train_loss 0.025268797 val_loss 0.026577432
Epoch 1005: train_loss 0.025267364 val_loss 0.026577849
Epoch 1006: train_loss 0.025265932 val_loss 0.026578264
Epoch 1007: train_loss 0.025264502 val_loss 0.026578674
Epoch 1008: train_loss 0.025263075 val_loss 0.026579086
Epoch 1009: train_loss 0.025261646 val_loss 0.026579497
Epoch 1010: train_loss 0.025260221 val_loss 0.026579907
Epoch 1011: train_loss 0.025258798 val_loss 0.026580317
Epoch 1012: train_loss 0.025257373 val_loss 0.026580727
Epoch 1013: train_loss 0.025255952 val_loss 0.026581127
Epoch 1014: train_loss 0.02525453 val_loss 0.02658152
Epoch 1015: train_loss 0.025253111 val_loss 0.026581911
Epoch 1016: train_loss 0.025251694 val_loss 0.026582316
Epoch 1017: train_loss 0.025250278 val_loss 0.026582714
Epoch 1018: train_loss 0.025248863 val_loss 0.026583113
Epoch 1019: train_loss 0.025247447 val_loss 0.02658351
Epoch 1020: train_loss 0.025246035 val_loss 0.026583899
Epoch 1021: train_loss 0.025244623 val_loss 0.026584288
Epoch 1022: train_loss 0.025243212 val_loss 0.026584668
Epoch 1023: train_loss 0.025241803 val_loss 0.026585046
Epoch 1024: train_loss 0.025240395 val_loss 0.026585428
Epoch 1025: train_loss 0.025238987 val_loss 0.026585817
Epoch 1026: train_loss 0.025237583 val_loss 0.0265862
Epoch 1027: train_loss 0.025236176 val_loss 0.026586575
Epoch 1028: train_loss 0.025234774 val_loss 0.02658695
Epoch 1029: train_loss 0.02523337 val_loss 0.026587315
Epoch 1030: train_loss 0.025231969 val_loss 0.026587682
Epoch 1031: train_loss 0.025230568 val_loss 0.026588056
Epoch 1032: train_loss 0.025229169 val_loss 0.026588423
Epoch 1033: train_loss 0.02522777 val_loss 0.026588786
Epoch 1034: train_loss 0.025226375 val_loss 0.02658915
Epoch 1035: train_loss 0.02522498 val_loss 0.026589511
Epoch 1036: train_loss 0.025223583 val_loss 0.026589857
Epoch 1037: train_loss 0.02522219 val_loss 0.026590217
Epoch 1038: train_loss 0.025220798 val_loss 0.02659057
Epoch 1039: train_loss 0.025219407 val_loss 0.026590923
Epoch 1040: train_loss 0.025218016 val_loss 0.026591271
Epoch 1041: train_loss 0.025216626 val_loss 0.026591627
Epoch 1042: train_loss 0.025215238 val_loss 0.026591979
Epoch 1043: train_loss 0.02521385 val_loss 0.026592324
Epoch 1044: train_loss 0.025212463 val_loss 0.026592657
Epoch 1045: train_loss 0.025211079 val_loss 0.026592992
Epoch 1046: train_loss 0.025209695 val_loss 0.026593331
Epoch 1047: train_loss 0.025208311 val_loss 0.026593674
Epoch 1048: train_loss 0.02520693 val_loss 0.026594004
Epoch 1049: train_loss 0.025205549 val_loss 0.026594337
Epoch 1050: train_loss 0.025204167 val_loss 0.026594667
Epoch 1051: train_loss 0.025202787 val_loss 0.026594998
Epoch 1052: train_loss 0.02520141 val_loss 0.026595324
Epoch 1053: train_loss 0.025200032 val_loss 0.026595648
Epoch 1054: train_loss 0.025198653 val_loss 0.026595967
Epoch 1055: train_loss 0.025197279 val_loss 0.026596287
Epoch 1056: train_loss 0.025195904 val_loss 0.026596604
Epoch 1057: train_loss 0.02519453 val_loss 0.026596917
Epoch 1058: train_loss 0.025193159 val_loss 0.026597235
Epoch 1059: train_loss 0.025191786 val_loss 0.026597545
Epoch 1060: train_loss 0.025190413 val_loss 0.026597861
Epoch 1061: train_loss 0.025189042 val_loss 0.026598169
Epoch 1062: train_loss 0.025187675 val_loss 0.026598472
Epoch 1063: train_loss 0.025186306 val_loss 0.026598768
Epoch 1064: train_loss 0.025184937 val_loss 0.026599068
Epoch 1065: train_loss 0.02518357 val_loss 0.026599364
Epoch 1066: train_loss 0.025182204 val_loss 0.026599659
Epoch 1067: train_loss 0.025180839 val_loss 0.026599962
Epoch 1068: train_loss 0.025179476 val_loss 0.026600258
Epoch 1069: train_loss 0.025178112 val_loss 0.02660055
Epoch 1070: train_loss 0.025176749 val_loss 0.026600836
Epoch 1071: train_loss 0.025175387 val_loss 0.02660112
Epoch 1072: train_loss 0.025174025 val_loss 0.026601389
Epoch 1073: train_loss 0.025172666 val_loss 0.026601668
Epoch 1074: train_loss 0.025171306 val_loss 0.026601959
Epoch 1075: train_loss 0.025169946 val_loss 0.026602242
Epoch 1076: train_loss 0.025168587 val_loss 0.026602512
Epoch 1077: train_loss 0.02516723 val_loss 0.026602786
Epoch 1078: train_loss 0.025165875 val_loss 0.026603052
Epoch 1079: train_loss 0.025164519 val_loss 0.026603326
Epoch 1080: train_loss 0.025163162 val_loss 0.026603585
Epoch 1081: train_loss 0.025161808 val_loss 0.026603846
Epoch 1082: train_loss 0.025160456 val_loss 0.026604105
Epoch 1083: train_loss 0.025159102 val_loss 0.026604367
Epoch 1084: train_loss 0.02515775 val_loss 0.026604628
Epoch 1085: train_loss 0.0251564 val_loss 0.02660488
Epoch 1086: train_loss 0.025155047 val_loss 0.026605131
Epoch 1087: train_loss 0.025153698 val_loss 0.026605383
Epoch 1088: train_loss 0.025152348 val_loss 0.026605636
Epoch 1089: train_loss 0.025151 val_loss 0.026605893
Epoch 1090: train_loss 0.02514965 val_loss 0.026606128
Epoch 1091: train_loss 0.025148306 val_loss 0.026606372
Epoch 1092: train_loss 0.025146957 val_loss 0.026606612
Epoch 1093: train_loss 0.025145613 val_loss 0.026606848
Epoch 1094: train_loss 0.025144268 val_loss 0.02660709
Epoch 1095: train_loss 0.025142921 val_loss 0.026607323
Epoch 1096: train_loss 0.025141578 val_loss 0.02660756
Epoch 1097: train_loss 0.025140233 val_loss 0.026607782
Epoch 1098: train_loss 0.02513889 val_loss 0.026608005
Epoch 1099: train_loss 0.02513755 val_loss 0.02660823
Epoch 1100: train_loss 0.025136206 val_loss 0.026608454
Epoch 1101: train_loss 0.025134865 val_loss 0.026608683
Epoch 1102: train_loss 0.025133526 val_loss 0.0266089
Epoch 1103: train_loss 0.025132187 val_loss 0.026609117
Epoch 1104: train_loss 0.02513085 val_loss 0.026609333
Epoch 1105: train_loss 0.025129508 val_loss 0.026609546
Epoch 1106: train_loss 0.02512817 val_loss 0.026609752
Epoch 1107: train_loss 0.025126833 val_loss 0.026609955
Epoch 1108: train_loss 0.025125496 val_loss 0.026610153
Epoch 1109: train_loss 0.02512416 val_loss 0.026610363
Epoch 1110: train_loss 0.025122825 val_loss 0.026610566
Epoch 1111: train_loss 0.02512149 val_loss 0.026610764
Epoch 1112: train_loss 0.025120154 val_loss 0.026610967
Epoch 1113: train_loss 0.02511882 val_loss 0.026611157
Epoch 1114: train_loss 0.025117487 val_loss 0.026611343
Epoch 1115: train_loss 0.025116153 val_loss 0.02661154
Epoch 1116: train_loss 0.02511482 val_loss 0.026611727
Epoch 1117: train_loss 0.025113488 val_loss 0.026611913
Epoch 1118: train_loss 0.025112158 val_loss 0.026612094
Epoch 1119: train_loss 0.025110826 val_loss 0.026612276
Epoch 1120: train_loss 0.025109496 val_loss 0.026612455
Epoch 1121: train_loss 0.025108166 val_loss 0.026612632
Epoch 1122: train_loss 0.025106836 val_loss 0.026612801
Epoch 1123: train_loss 0.025105506 val_loss 0.026612973
Epoch 1124: train_loss 0.025104178 val_loss 0.026613144
Epoch 1125: train_loss 0.02510285 val_loss 0.026613316
Epoch 1126: train_loss 0.02510152 val_loss 0.026613481
Epoch 1127: train_loss 0.025100194 val_loss 0.026613636
Epoch 1128: train_loss 0.025098868 val_loss 0.026613798
Epoch 1129: train_loss 0.02509754 val_loss 0.026613966
Epoch 1130: train_loss 0.025096215 val_loss 0.026614122
Epoch 1131: train_loss 0.02509489 val_loss 0.026614275
Epoch 1132: train_loss 0.025093567 val_loss 0.026614424
Epoch 1133: train_loss 0.02509224 val_loss 0.026614567
Epoch 1134: train_loss 0.025090916 val_loss 0.026614716
Epoch 1135: train_loss 0.025089592 val_loss 0.026614867
Epoch 1136: train_loss 0.025088267 val_loss 0.026615024
Epoch 1137: train_loss 0.025086945 val_loss 0.026615165
Epoch 1138: train_loss 0.025085622 val_loss 0.026615294
Epoch 1139: train_loss 0.025084302 val_loss 0.026615424
Epoch 1140: train_loss 0.02508298 val_loss 0.026615556
Epoch 1141: train_loss 0.025081659 val_loss 0.026615694
Epoch 1142: train_loss 0.025080338 val_loss 0.026615832
Epoch 1143: train_loss 0.025079016 val_loss 0.026615966
Epoch 1144: train_loss 0.025077695 val_loss 0.02661609
Epoch 1145: train_loss 0.025076378 val_loss 0.026616208
Epoch 1146: train_loss 0.025075056 val_loss 0.026616324
Epoch 1147: train_loss 0.025073737 val_loss 0.026616441
Epoch 1148: train_loss 0.025072418 val_loss 0.026616555
Epoch 1149: train_loss 0.0250711 val_loss 0.02661668
Epoch 1150: train_loss 0.025069783 val_loss 0.026616804
Epoch 1151: train_loss 0.025068466 val_loss 0.026616918
Epoch 1152: train_loss 0.025067147 val_loss 0.02661702
Epoch 1153: train_loss 0.02506583 val_loss 0.026617121
Epoch 1154: train_loss 0.025064515 val_loss 0.026617216
Epoch 1155: train_loss 0.025063198 val_loss 0.026617322
Epoch 1156: train_loss 0.025061883 val_loss 0.026617428
Epoch 1157: train_loss 0.025060566 val_loss 0.026617527
Epoch 1158: train_loss 0.025059251 val_loss 0.026617626
Epoch 1159: train_loss 0.025057936 val_loss 0.026617717
Epoch 1160: train_loss 0.025056621 val_loss 0.026617808
Epoch 1161: train_loss 0.025055308 val_loss 0.026617894
Epoch 1162: train_loss 0.025053993 val_loss 0.02661798
Epoch 1163: train_loss 0.02505268 val_loss 0.026618062
Epoch 1164: train_loss 0.025051367 val_loss 0.026618145
Epoch 1165: train_loss 0.025050052 val_loss 0.026618227
Epoch 1166: train_loss 0.025048738 val_loss 0.02661831
Epoch 1167: train_loss 0.025047425 val_loss 0.02661839
Epoch 1168: train_loss 0.025046114 val_loss 0.026618458
Epoch 1169: train_loss 0.025044803 val_loss 0.026618525
Epoch 1170: train_loss 0.025043491 val_loss 0.026618596
Epoch 1171: train_loss 0.025042178 val_loss 0.026618669
Epoch 1172: train_loss 0.025040867 val_loss 0.026618738
Epoch 1173: train_loss 0.025039557 val_loss 0.026618803
Epoch 1174: train_loss 0.025038242 val_loss 0.026618866
Epoch 1175: train_loss 0.025036935 val_loss 0.026618924
Epoch 1176: train_loss 0.025035623 val_loss 0.026618978
Epoch 1177: train_loss 0.025034312 val_loss 0.026619032
Epoch 1178: train_loss 0.025033005 val_loss 0.026619088
Epoch 1179: train_loss 0.025031693 val_loss 0.026619134
Epoch 1180: train_loss 0.025030386 val_loss 0.026619183
Epoch 1181: train_loss 0.025029074 val_loss 0.02661923
Epoch 1182: train_loss 0.025027767 val_loss 0.026619276
Epoch 1183: train_loss 0.025026457 val_loss 0.02661932
Epoch 1184: train_loss 0.025025148 val_loss 0.026619362
Epoch 1185: train_loss 0.02502384 val_loss 0.026619395
Epoch 1186: train_loss 0.025022533 val_loss 0.026619427
Epoch 1187: train_loss 0.025021225 val_loss 0.026619466
Epoch 1188: train_loss 0.025019918 val_loss 0.026619496
Epoch 1189: train_loss 0.025018608 val_loss 0.026619524
Epoch 1190: train_loss 0.0250173 val_loss 0.02661955
Epoch 1191: train_loss 0.025015995 val_loss 0.026619576
Epoch 1192: train_loss 0.025014687 val_loss 0.0266196
Epoch 1193: train_loss 0.02501338 val_loss 0.02661962
Epoch 1194: train_loss 0.025012072 val_loss 0.02661964
Epoch 1195: train_loss 0.025010766 val_loss 0.026619662
Epoch 1196: train_loss 0.02500946 val_loss 0.02661967
Epoch 1197: train_loss 0.025008153 val_loss 0.026619677
Epoch 1198: train_loss 0.02500685 val_loss 0.026619686
Epoch 1199: train_loss 0.025005542 val_loss 0.0266197
Epoch 1200: train_loss 0.025004238 val_loss 0.02661971
Epoch 1201: train_loss 0.02500293 val_loss 0.026619716
Epoch 1202: train_loss 0.025001626 val_loss 0.026619717
Epoch 1203: train_loss 0.025000319 val_loss 0.02661972
Epoch 1204: train_loss 0.024999015 val_loss 0.026619716
Epoch 1205: train_loss 0.02499771 val_loss 0.0266197
Epoch 1206: train_loss 0.024996407 val_loss 0.026619695
Epoch 1207: train_loss 0.0249951 val_loss 0.026619695
Epoch 1208: train_loss 0.024993796 val_loss 0.026619688
Epoch 1209: train_loss 0.024992492 val_loss 0.026619667
Epoch 1210: train_loss 0.024991186 val_loss 0.026619645
Epoch 1211: train_loss 0.02498988 val_loss 0.026619628
Epoch 1212: train_loss 0.024988577 val_loss 0.026619624
Epoch 1213: train_loss 0.024987273 val_loss 0.026619602
Epoch 1214: train_loss 0.02498597 val_loss 0.02661957
Epoch 1215: train_loss 0.024984665 val_loss 0.026619539
Epoch 1216: train_loss 0.024983361 val_loss 0.026619514
Epoch 1217: train_loss 0.024982058 val_loss 0.026619488
Epoch 1218: train_loss 0.024980756 val_loss 0.026619462
Epoch 1219: train_loss 0.024979454 val_loss 0.02661942
Epoch 1220: train_loss 0.024978148 val_loss 0.02661939
Epoch 1221: train_loss 0.024976846 val_loss 0.026619356
Epoch 1222: train_loss 0.024975542 val_loss 0.026619317
Epoch 1223: train_loss 0.024974238 val_loss 0.026619267
Epoch 1224: train_loss 0.024972934 val_loss 0.026619224
Epoch 1225: train_loss 0.024971634 val_loss 0.026619181
Epoch 1226: train_loss 0.02497033 val_loss 0.026619133
Epoch 1227: train_loss 0.024969026 val_loss 0.026619084
Epoch 1228: train_loss 0.024967723 val_loss 0.026619036
Epoch 1229: train_loss 0.024966422 val_loss 0.026618984
Epoch 1230: train_loss 0.02496512 val_loss 0.02661893
Epoch 1231: train_loss 0.024963818 val_loss 0.026618872
Epoch 1232: train_loss 0.024962515 val_loss 0.026618807
Epoch 1233: train_loss 0.02496121 val_loss 0.026618741
Epoch 1234: train_loss 0.024959909 val_loss 0.02661868
Epoch 1235: train_loss 0.024958607 val_loss 0.026618619
Epoch 1236: train_loss 0.024957305 val_loss 0.026618553
Epoch 1237: train_loss 0.024956003 val_loss 0.026618483
Epoch 1238: train_loss 0.024954699 val_loss 0.02661842
Epoch 1239: train_loss 0.024953397 val_loss 0.026618348
Epoch 1240: train_loss 0.024952095 val_loss 0.02661827
Epoch 1241: train_loss 0.024950795 val_loss 0.026618188
Epoch 1242: train_loss 0.024949493 val_loss 0.026618114
Epoch 1243: train_loss 0.02494819 val_loss 0.026618034
Epoch 1244: train_loss 0.024946887 val_loss 0.02661795
Epoch 1245: train_loss 0.024945585 val_loss 0.026617868
Epoch 1246: train_loss 0.024944283 val_loss 0.026617792
Epoch 1247: train_loss 0.024942983 val_loss 0.026617715
Epoch 1248: train_loss 0.02494168 val_loss 0.02661762
Epoch 1249: train_loss 0.024940377 val_loss 0.026617516
Epoch 1250: train_loss 0.024939075 val_loss 0.026617417
Epoch 1251: train_loss 0.024937775 val_loss 0.026617322
Epoch 1252: train_loss 0.024936471 val_loss 0.02661723
Epoch 1253: train_loss 0.02493517 val_loss 0.026617132
Epoch 1254: train_loss 0.024933867 val_loss 0.02661703
Epoch 1255: train_loss 0.024932565 val_loss 0.026616927
Epoch 1256: train_loss 0.024931263 val_loss 0.02661682
Epoch 1257: train_loss 0.024929963 val_loss 0.026616713
Epoch 1258: train_loss 0.024928661 val_loss 0.026616609
Epoch 1259: train_loss 0.02492736 val_loss 0.026616493
Epoch 1260: train_loss 0.024926055 val_loss 0.02661638
Epoch 1261: train_loss 0.024924755 val_loss 0.026616268
Epoch 1262: train_loss 0.024923451 val_loss 0.026616149
Epoch 1263: train_loss 0.02492215 val_loss 0.026616031
Epoch 1264: train_loss 0.02492085 val_loss 0.026615907
Epoch 1265: train_loss 0.024919545 val_loss 0.026615782
Epoch 1266: train_loss 0.024918243 val_loss 0.02661566
Epoch 1267: train_loss 0.02491694 val_loss 0.026615543
Epoch 1268: train_loss 0.02491564 val_loss 0.026615411
Epoch 1269: train_loss 0.024914335 val_loss 0.026615279
Epoch 1270: train_loss 0.024913035 val_loss 0.026615135
Epoch 1271: train_loss 0.024911733 val_loss 0.026615005
Epoch 1272: train_loss 0.02491043 val_loss 0.026614873
Epoch 1273: train_loss 0.02490913 val_loss 0.026614739
Epoch 1274: train_loss 0.024907824 val_loss 0.026614591
Epoch 1275: train_loss 0.024906524 val_loss 0.026614457
Epoch 1276: train_loss 0.02490522 val_loss 0.026614321
Epoch 1277: train_loss 0.024903918 val_loss 0.026614176
Epoch 1278: train_loss 0.024902616 val_loss 0.026614018
Epoch 1279: train_loss 0.024901312 val_loss 0.02661387
Epoch 1280: train_loss 0.024900008 val_loss 0.026613723
Epoch 1281: train_loss 0.024898706 val_loss 0.026613576
Epoch 1282: train_loss 0.024897402 val_loss 0.026613427
Epoch 1283: train_loss 0.0248961 val_loss 0.02661327
Epoch 1284: train_loss 0.024894796 val_loss 0.02661311
Epoch 1285: train_loss 0.024893494 val_loss 0.026612952
Epoch 1286: train_loss 0.024892189 val_loss 0.026612788
Epoch 1287: train_loss 0.024890887 val_loss 0.026612623
Epoch 1288: train_loss 0.024889585 val_loss 0.026612455
Epoch 1289: train_loss 0.02488828 val_loss 0.026612297
Epoch 1290: train_loss 0.024886977 val_loss 0.026612129
Epoch 1291: train_loss 0.024885673 val_loss 0.026611958
Epoch 1292: train_loss 0.02488437 val_loss 0.02661179
Epoch 1293: train_loss 0.024883065 val_loss 0.026611615
Epoch 1294: train_loss 0.024881762 val_loss 0.026611434
Epoch 1295: train_loss 0.024880458 val_loss 0.026611255
Epoch 1296: train_loss 0.024879154 val_loss 0.026611082
Epoch 1297: train_loss 0.02487785 val_loss 0.0266109
Epoch 1298: train_loss 0.024876544 val_loss 0.026610717
Epoch 1299: train_loss 0.02487524 val_loss 0.026610533
Epoch 1300: train_loss 0.024873935 val_loss 0.026610343
Epoch 1301: train_loss 0.02487263 val_loss 0.026610164
Epoch 1302: train_loss 0.024871327 val_loss 0.026609976
Epoch 1303: train_loss 0.024870023 val_loss 0.026609777
Epoch 1304: train_loss 0.024868717 val_loss 0.026609585
Epoch 1305: train_loss 0.024867412 val_loss 0.026609398
Epoch 1306: train_loss 0.024866108 val_loss 0.026609207
Epoch 1307: train_loss 0.024864804 val_loss 0.026609004
Epoch 1308: train_loss 0.024863496 val_loss 0.026608797
Epoch 1309: train_loss 0.024862193 val_loss 0.026608596
Epoch 1310: train_loss 0.024860885 val_loss 0.0266084
Epoch 1311: train_loss 0.024859581 val_loss 0.026608193
Epoch 1312: train_loss 0.024858274 val_loss 0.026607987
Epoch 1313: train_loss 0.024856968 val_loss 0.026607776
Epoch 1314: train_loss 0.024855664 val_loss 0.02660757
Epoch 1315: train_loss 0.024854355 val_loss 0.026607366
Epoch 1316: train_loss 0.024853049 val_loss 0.026607143
Epoch 1317: train_loss 0.024851743 val_loss 0.026606927
Epoch 1318: train_loss 0.024850436 val_loss 0.026606705
Epoch 1319: train_loss 0.024849128 val_loss 0.026606489
Epoch 1320: train_loss 0.02484782 val_loss 0.026606271
Epoch 1321: train_loss 0.024846517 val_loss 0.026606051
Epoch 1322: train_loss 0.024845209 val_loss 0.026605831
Epoch 1323: train_loss 0.0248439 val_loss 0.02660561
Epoch 1324: train_loss 0.024842594 val_loss 0.026605379
Epoch 1325: train_loss 0.024841286 val_loss 0.026605155
Epoch 1326: train_loss 0.024839979 val_loss 0.026604922
Epoch 1327: train_loss 0.024838671 val_loss 0.026604693
Epoch 1328: train_loss 0.024837364 val_loss 0.026604459
Epoch 1329: train_loss 0.024836056 val_loss 0.026604222
Epoch 1330: train_loss 0.024834745 val_loss 0.02660399
Epoch 1331: train_loss 0.024833437 val_loss 0.026603749
Epoch 1332: train_loss 0.024832128 val_loss 0.026603507
Epoch 1333: train_loss 0.02483082 val_loss 0.026603263
Epoch 1334: train_loss 0.024829512 val_loss 0.026603028
Epoch 1335: train_loss 0.024828203 val_loss 0.026602797
Epoch 1336: train_loss 0.024826892 val_loss 0.026602551
Epoch 1337: train_loss 0.024825582 val_loss 0.02660229
Epoch 1338: train_loss 0.024824273 val_loss 0.026602037
Epoch 1339: train_loss 0.024822963 val_loss 0.026601788
Epoch 1340: train_loss 0.024821652 val_loss 0.026601538
Epoch 1341: train_loss 0.02482034 val_loss 0.026601285
Epoch 1342: train_loss 0.024819031 val_loss 0.026601035
Epoch 1343: train_loss 0.02481772 val_loss 0.026600791
Epoch 1344: train_loss 0.02481641 val_loss 0.026600536
Epoch 1345: train_loss 0.0248151 val_loss 0.026600268
Epoch 1346: train_loss 0.024813788 val_loss 0.026599996
Epoch 1347: train_loss 0.024812477 val_loss 0.026599728
Epoch 1348: train_loss 0.024811165 val_loss 0.02659947
Epoch 1349: train_loss 0.024809854 val_loss 0.026599206
Epoch 1350: train_loss 0.024808541 val_loss 0.02659894
Epoch 1351: train_loss 0.02480723 val_loss 0.026598662
Epoch 1352: train_loss 0.024805918 val_loss 0.026598394
Epoch 1353: train_loss 0.024804605 val_loss 0.02659812
Epoch 1354: train_loss 0.024803292 val_loss 0.026597844
Epoch 1355: train_loss 0.02480198 val_loss 0.026597569
Epoch 1356: train_loss 0.024800666 val_loss 0.026597288
Epoch 1357: train_loss 0.024799354 val_loss 0.026597012
Epoch 1358: train_loss 0.02479804 val_loss 0.02659673
Epoch 1359: train_loss 0.024796726 val_loss 0.026596447
Epoch 1360: train_loss 0.024795413 val_loss 0.026596159
Epoch 1361: train_loss 0.024794098 val_loss 0.026595877
Epoch 1362: train_loss 0.024792783 val_loss 0.026595596
Epoch 1363: train_loss 0.024791472 val_loss 0.026595308
Epoch 1364: train_loss 0.024790153 val_loss 0.02659501
Epoch 1365: train_loss 0.024788842 val_loss 0.02659472
Epoch 1366: train_loss 0.024787525 val_loss 0.02659443
Epoch 1367: train_loss 0.02478621 val_loss 0.026594145
Epoch 1368: train_loss 0.024784893 val_loss 0.026593847
Epoch 1369: train_loss 0.024783578 val_loss 0.026593544
Epoch 1370: train_loss 0.024782263 val_loss 0.026593257
Epoch 1371: train_loss 0.024780946 val_loss 0.026592962
Epoch 1372: train_loss 0.024779629 val_loss 0.02659266
Epoch 1373: train_loss 0.024778314 val_loss 0.026592351
Epoch 1374: train_loss 0.024776995 val_loss 0.026592042
Epoch 1375: train_loss 0.02477568 val_loss 0.026591746
Epoch 1376: train_loss 0.024774363 val_loss 0.02659144
Epoch 1377: train_loss 0.024773046 val_loss 0.026591126
Epoch 1378: train_loss 0.024771728 val_loss 0.026590811
Epoch 1379: train_loss 0.024770409 val_loss 0.026590506
Epoch 1380: train_loss 0.02476909 val_loss 0.026590193
Epoch 1381: train_loss 0.024767775 val_loss 0.026589876
Epoch 1382: train_loss 0.024766453 val_loss 0.026589556
Epoch 1383: train_loss 0.024765136 val_loss 0.026589243
Epoch 1384: train_loss 0.024763815 val_loss 0.026588922
Epoch 1385: train_loss 0.024762496 val_loss 0.026588606
Epoch 1386: train_loss 0.024761176 val_loss 0.02658828
Epoch 1387: train_loss 0.024759857 val_loss 0.026587954
Epoch 1388: train_loss 0.024758538 val_loss 0.02658763
Epoch 1389: train_loss 0.024757218 val_loss 0.026587306
Epoch 1390: train_loss 0.024755895 val_loss 0.026586983
Epoch 1391: train_loss 0.024754576 val_loss 0.026586656
Epoch 1392: train_loss 0.024753254 val_loss 0.026586322
Epoch 1393: train_loss 0.024751933 val_loss 0.026585985
Epoch 1394: train_loss 0.024750613 val_loss 0.02658565
Epoch 1395: train_loss 0.02474929 val_loss 0.02658533
Epoch 1396: train_loss 0.024747968 val_loss 0.026584996
Epoch 1397: train_loss 0.024746647 val_loss 0.02658465
Epoch 1398: train_loss 0.024745323 val_loss 0.02658431
Epoch 1399: train_loss 0.024744 val_loss 0.026583973
Epoch 1400: train_loss 0.024742678 val_loss 0.026583642
Epoch 1401: train_loss 0.024741355 val_loss 0.02658329
Epoch 1402: train_loss 0.024740031 val_loss 0.026582941
Epoch 1403: train_loss 0.024738707 val_loss 0.026582608
Epoch 1404: train_loss 0.024737384 val_loss 0.026582269
Epoch 1405: train_loss 0.024736058 val_loss 0.026581911
Epoch 1406: train_loss 0.024734735 val_loss 0.026581557
Epoch 1407: train_loss 0.02473341 val_loss 0.026581207
Epoch 1408: train_loss 0.024732083 val_loss 0.026580865
Epoch 1409: train_loss 0.024730759 val_loss 0.026580513
Epoch 1410: train_loss 0.024729433 val_loss 0.026580151
Epoch 1411: train_loss 0.024728108 val_loss 0.02657979
Epoch 1412: train_loss 0.024726782 val_loss 0.026579442
Epoch 1413: train_loss 0.024725452 val_loss 0.026579088
Epoch 1414: train_loss 0.02472413 val_loss 0.026578711
Epoch 1415: train_loss 0.0247228 val_loss 0.026578352
Epoch 1416: train_loss 0.024721473 val_loss 0.026578002
Epoch 1417: train_loss 0.024720145 val_loss 0.026577644
Epoch 1418: train_loss 0.024718817 val_loss 0.026577264
Epoch 1419: train_loss 0.02471749 val_loss 0.026576897
Epoch 1420: train_loss 0.024716161 val_loss 0.026576536
Epoch 1421: train_loss 0.024714831 val_loss 0.026576173
Epoch 1422: train_loss 0.024713503 val_loss 0.026575798
Epoch 1423: train_loss 0.024712173 val_loss 0.026575424
Epoch 1424: train_loss 0.024710843 val_loss 0.026575057
Epoch 1425: train_loss 0.024709515 val_loss 0.026574673
Epoch 1426: train_loss 0.024708185 val_loss 0.026574291
Epoch 1427: train_loss 0.024706852 val_loss 0.02657392
Epoch 1428: train_loss 0.024705522 val_loss 0.02657355
Epoch 1429: train_loss 0.024704192 val_loss 0.026573172
Epoch 1430: train_loss 0.024702862 val_loss 0.026572784
Epoch 1431: train_loss 0.02470153 val_loss 0.026572395
Epoch 1432: train_loss 0.024700198 val_loss 0.026572015
Epoch 1433: train_loss 0.024698865 val_loss 0.02657164
Epoch 1434: train_loss 0.024697533 val_loss 0.026571257
Epoch 1435: train_loss 0.0246962 val_loss 0.02657087
Epoch 1436: train_loss 0.024694867 val_loss 0.026570478
Epoch 1437: train_loss 0.024693534 val_loss 0.026570085
Epoch 1438: train_loss 0.0246922 val_loss 0.0265697
Epoch 1439: train_loss 0.024690866 val_loss 0.02656931
Epoch 1440: train_loss 0.024689533 val_loss 0.026568921
Epoch 1441: train_loss 0.0246882 val_loss 0.026568526
Epoch 1442: train_loss 0.024686866 val_loss 0.02656813
Epoch 1443: train_loss 0.024685528 val_loss 0.026567727
Epoch 1444: train_loss 0.024684193 val_loss 0.026567325
Epoch 1445: train_loss 0.024682857 val_loss 0.026566925
Epoch 1446: train_loss 0.02468152 val_loss 0.026566532
Epoch 1447: train_loss 0.024680186 val_loss 0.026566133
Epoch 1448: train_loss 0.024678849 val_loss 0.026565734
Epoch 1449: train_loss 0.024677511 val_loss 0.026565325
Epoch 1450: train_loss 0.024676176 val_loss 0.026564911
Epoch 1451: train_loss 0.024674837 val_loss 0.02656451
Epoch 1452: train_loss 0.0246735 val_loss 0.026564106
Epoch 1453: train_loss 0.024672162 val_loss 0.026563697
Epoch 1454: train_loss 0.02467082 val_loss 0.02656329
Epoch 1455: train_loss 0.024669483 val_loss 0.02656288
Epoch 1456: train_loss 0.024668144 val_loss 0.02656246
Epoch 1457: train_loss 0.024666805 val_loss 0.026562052
Epoch 1458: train_loss 0.024665464 val_loss 0.02656164
Epoch 1459: train_loss 0.024664126 val_loss 0.026561225
Epoch 1460: train_loss 0.024662785 val_loss 0.026560813
Epoch 1461: train_loss 0.024661444 val_loss 0.026560394
Epoch 1462: train_loss 0.024660103 val_loss 0.02655997
Epoch 1463: train_loss 0.024658762 val_loss 0.026559552
Epoch 1464: train_loss 0.024657419 val_loss 0.026559131
Epoch 1465: train_loss 0.024656078 val_loss 0.026558718
Epoch 1466: train_loss 0.024654735 val_loss 0.026558293
Epoch 1467: train_loss 0.024653394 val_loss 0.026557857
Epoch 1468: train_loss 0.024652049 val_loss 0.02655743
Epoch 1469: train_loss 0.024650706 val_loss 0.02655701
Epoch 1470: train_loss 0.024649363 val_loss 0.026556587
Epoch 1471: train_loss 0.024648018 val_loss 0.026556157
Epoch 1472: train_loss 0.024646673 val_loss 0.026555724
Epoch 1473: train_loss 0.024645329 val_loss 0.026555294
Epoch 1474: train_loss 0.024643984 val_loss 0.026554858
Epoch 1475: train_loss 0.024642639 val_loss 0.026554426
Epoch 1476: train_loss 0.024641294 val_loss 0.026553992
Epoch 1477: train_loss 0.024639945 val_loss 0.026553556
Epoch 1478: train_loss 0.0246386 val_loss 0.026553117
Epoch 1479: train_loss 0.024637256 val_loss 0.026552685
Epoch 1480: train_loss 0.024635907 val_loss 0.026552249
Epoch 1481: train_loss 0.02463456 val_loss 0.026551807
Epoch 1482: train_loss 0.024633212 val_loss 0.026551357
Epoch 1483: train_loss 0.024631863 val_loss 0.026550911
Epoch 1484: train_loss 0.024630515 val_loss 0.026550474
Epoch 1485: train_loss 0.024629164 val_loss 0.026550036
Epoch 1486: train_loss 0.024627818 val_loss 0.026549587
Epoch 1487: train_loss 0.024626467 val_loss 0.026549146
Epoch 1488: train_loss 0.024625119 val_loss 0.026548699
Epoch 1489: train_loss 0.024623768 val_loss 0.026548244
Epoch 1490: train_loss 0.024622418 val_loss 0.02654779
Epoch 1491: train_loss 0.024621066 val_loss 0.026547346
Epoch 1492: train_loss 0.024619713 val_loss 0.026546905
Epoch 1493: train_loss 0.024618365 val_loss 0.026546447
Epoch 1494: train_loss 0.024617013 val_loss 0.026545987
Epoch 1495: train_loss 0.02461566 val_loss 0.026545528
Epoch 1496: train_loss 0.024614308 val_loss 0.026545074
Epoch 1497: train_loss 0.024612954 val_loss 0.026544623
Epoch 1498: train_loss 0.024611602 val_loss 0.026544167
Epoch 1499: train_loss 0.024610247 val_loss 0.026543705
Epoch 1500: train_loss 0.024608893 val_loss 0.026543247
Epoch 1501: train_loss 0.02460754 val_loss 0.026542779
Epoch 1502: train_loss 0.024606183 val_loss 0.026542319
Epoch 1503: train_loss 0.024604829 val_loss 0.026541859
Epoch 1504: train_loss 0.024603475 val_loss 0.0265414
Epoch 1505: train_loss 0.024602119 val_loss 0.026540935
Epoch 1506: train_loss 0.024600763 val_loss 0.026540462
Epoch 1507: train_loss 0.024599405 val_loss 0.026539994
Epoch 1508: train_loss 0.02459805 val_loss 0.026539527
Epoch 1509: train_loss 0.02459669 val_loss 0.026539065
Epoch 1510: train_loss 0.024595331 val_loss 0.026538588
Epoch 1511: train_loss 0.024593975 val_loss 0.026538113
Epoch 1512: train_loss 0.024592616 val_loss 0.026537646
Epoch 1513: train_loss 0.024591258 val_loss 0.026537172
Epoch 1514: train_loss 0.0245899 val_loss 0.026536692
Epoch 1515: train_loss 0.024588538 val_loss 0.026536224
Epoch 1516: train_loss 0.024587177 val_loss 0.026535751
Epoch 1517: train_loss 0.02458582 val_loss 0.02653526
Epoch 1518: train_loss 0.024584457 val_loss 0.026534777
Epoch 1519: train_loss 0.024583098 val_loss 0.02653431
Epoch 1520: train_loss 0.024581738 val_loss 0.026533833
Epoch 1521: train_loss 0.024580374 val_loss 0.026533352
Epoch 1522: train_loss 0.02457901 val_loss 0.026532864
Epoch 1523: train_loss 0.024577651 val_loss 0.026532378
Epoch 1524: train_loss 0.024576286 val_loss 0.026531897
Epoch 1525: train_loss 0.02457492 val_loss 0.026531415
Epoch 1526: train_loss 0.024573559 val_loss 0.02653093
Epoch 1527: train_loss 0.024572194 val_loss 0.026530448
Epoch 1528: train_loss 0.02457083 val_loss 0.026529958
Epoch 1529: train_loss 0.024569467 val_loss 0.02652947
Epoch 1530: train_loss 0.0245681 val_loss 0.026528982
Epoch 1531: train_loss 0.024566732 val_loss 0.026528494
Epoch 1532: train_loss 0.024565369 val_loss 0.026528003
Epoch 1533: train_loss 0.024564002 val_loss 0.026527505
Epoch 1534: train_loss 0.024562635 val_loss 0.026527017
Epoch 1535: train_loss 0.024561267 val_loss 0.02652653
Epoch 1536: train_loss 0.0245599 val_loss 0.026526026
Epoch 1537: train_loss 0.024558533 val_loss 0.02652553
Epoch 1538: train_loss 0.024557164 val_loss 0.026525032
Epoch 1539: train_loss 0.024555795 val_loss 0.02652453
Epoch 1540: train_loss 0.024554426 val_loss 0.026524037
Epoch 1541: train_loss 0.024553057 val_loss 0.026523545
Epoch 1542: train_loss 0.024551686 val_loss 0.02652304
Epoch 1543: train_loss 0.024550315 val_loss 0.026522536
Epoch 1544: train_loss 0.024548944 val_loss 0.026522031
Epoch 1545: train_loss 0.024547575 val_loss 0.026521536
Epoch 1546: train_loss 0.024546202 val_loss 0.026521038
Epoch 1547: train_loss 0.02454483 val_loss 0.026520522
Epoch 1548: train_loss 0.024543459 val_loss 0.026520014
Epoch 1549: train_loss 0.024542086 val_loss 0.026519518
Epoch 1550: train_loss 0.024540711 val_loss 0.026519015
Epoch 1551: train_loss 0.024539337 val_loss 0.026518501
Epoch 1552: train_loss 0.024537966 val_loss 0.026517995
Epoch 1553: train_loss 0.02453659 val_loss 0.02651749
Epoch 1554: train_loss 0.024535216 val_loss 0.02651698
Epoch 1555: train_loss 0.02453384 val_loss 0.026516458
Epoch 1556: train_loss 0.024532463 val_loss 0.026515948
Epoch 1557: train_loss 0.02453109 val_loss 0.026515437
Epoch 1558: train_loss 0.024529712 val_loss 0.026514925
Epoch 1559: train_loss 0.024528336 val_loss 0.026514407
Epoch 1560: train_loss 0.024526957 val_loss 0.026513893
Epoch 1561: train_loss 0.024525579 val_loss 0.02651338
Epoch 1562: train_loss 0.024524203 val_loss 0.02651286
Epoch 1563: train_loss 0.024522824 val_loss 0.026512347
Epoch 1564: train_loss 0.024521444 val_loss 0.026511833
Epoch 1565: train_loss 0.024520066 val_loss 0.026511312
Epoch 1566: train_loss 0.024518687 val_loss 0.026510783
Epoch 1567: train_loss 0.024517307 val_loss 0.026510257
Epoch 1568: train_loss 0.024515925 val_loss 0.026509743
Epoch 1569: train_loss 0.024514543 val_loss 0.026509222
Epoch 1570: train_loss 0.024513163 val_loss 0.026508698
Epoch 1571: train_loss 0.02451178 val_loss 0.026508166
Epoch 1572: train_loss 0.024510399 val_loss 0.02650764
Epoch 1573: train_loss 0.024509015 val_loss 0.026507113
Epoch 1574: train_loss 0.024507634 val_loss 0.026506586
Epoch 1575: train_loss 0.024506249 val_loss 0.026506051
Epoch 1576: train_loss 0.024504866 val_loss 0.026505526
Epoch 1577: train_loss 0.02450348 val_loss 0.026505003
Epoch 1578: train_loss 0.024502097 val_loss 0.026504466
Epoch 1579: train_loss 0.02450071 val_loss 0.026503934
Epoch 1580: train_loss 0.024499325 val_loss 0.026503406
Epoch 1581: train_loss 0.024497937 val_loss 0.026502877
Epoch 1582: train_loss 0.024496552 val_loss 0.026502341
Epoch 1583: train_loss 0.024495166 val_loss 0.02650181
Epoch 1584: train_loss 0.024493776 val_loss 0.026501276
Epoch 1585: train_loss 0.02449239 val_loss 0.026500743
Epoch 1586: train_loss 0.024491003 val_loss 0.026500208
Epoch 1587: train_loss 0.024489613 val_loss 0.02649967
Epoch 1588: train_loss 0.024488226 val_loss 0.026499128
Epoch 1589: train_loss 0.024486836 val_loss 0.026498597
Epoch 1590: train_loss 0.024485445 val_loss 0.02649806
Epoch 1591: train_loss 0.024484053 val_loss 0.026497519
Epoch 1592: train_loss 0.024482664 val_loss 0.026496982
Epoch 1593: train_loss 0.024481272 val_loss 0.026496442
Epoch 1594: train_loss 0.02447988 val_loss 0.026495894
Epoch 1595: train_loss 0.024478488 val_loss 0.026495352
Epoch 1596: train_loss 0.024477094 val_loss 0.02649481
Epoch 1597: train_loss 0.024475703 val_loss 0.026494265
Epoch 1598: train_loss 0.024474308 val_loss 0.02649372
Epoch 1599: train_loss 0.024472915 val_loss 0.026493173
Epoch 1600: train_loss 0.024471521 val_loss 0.026492631
Epoch 1601: train_loss 0.024470126 val_loss 0.02649208
Epoch 1602: train_loss 0.02446873 val_loss 0.026491528
Epoch 1603: train_loss 0.024467334 val_loss 0.026490977
Epoch 1604: train_loss 0.024465937 val_loss 0.026490424
Epoch 1605: train_loss 0.024464542 val_loss 0.026489884
Epoch 1606: train_loss 0.024463143 val_loss 0.026489336
Epoch 1607: train_loss 0.024461746 val_loss 0.02648878
Epoch 1608: train_loss 0.024460347 val_loss 0.026488228
Epoch 1609: train_loss 0.02445895 val_loss 0.02648767
Epoch 1610: train_loss 0.02445755 val_loss 0.026487118
Epoch 1611: train_loss 0.02445615 val_loss 0.026486564
Epoch 1612: train_loss 0.024454752 val_loss 0.026486013
Epoch 1613: train_loss 0.024453353 val_loss 0.026485467
Epoch 1614: train_loss 0.024451949 val_loss 0.026484905
Epoch 1615: train_loss 0.024450548 val_loss 0.026484342
Epoch 1616: train_loss 0.02444915 val_loss 0.02648379
Epoch 1617: train_loss 0.024447747 val_loss 0.026483238
Epoch 1618: train_loss 0.024446344 val_loss 0.026482675
Epoch 1619: train_loss 0.02444494 val_loss 0.026482109
Epoch 1620: train_loss 0.024443537 val_loss 0.026481552
Epoch 1621: train_loss 0.024442133 val_loss 0.026480997
Epoch 1622: train_loss 0.024440728 val_loss 0.026480442
Epoch 1623: train_loss 0.024439324 val_loss 0.02647986
Epoch 1624: train_loss 0.02443792 val_loss 0.026479308
Epoch 1625: train_loss 0.024436513 val_loss 0.026478743
Epoch 1626: train_loss 0.024435107 val_loss 0.026478171
Epoch 1627: train_loss 0.024433699 val_loss 0.02647761
Epoch 1628: train_loss 0.02443229 val_loss 0.026477056
Epoch 1629: train_loss 0.024430884 val_loss 0.026476488
Epoch 1630: train_loss 0.024429476 val_loss 0.026475903
Epoch 1631: train_loss 0.024428066 val_loss 0.026475333
Epoch 1632: train_loss 0.024426658 val_loss 0.026474778
Epoch 1633: train_loss 0.024425248 val_loss 0.026474215
Epoch 1634: train_loss 0.024423838 val_loss 0.026473641
Epoch 1635: train_loss 0.024422428 val_loss 0.026473064
Epoch 1636: train_loss 0.024421016 val_loss 0.026472498
Epoch 1637: train_loss 0.024419602 val_loss 0.02647193
Epoch 1638: train_loss 0.024418194 val_loss 0.02647136
Epoch 1639: train_loss 0.024416778 val_loss 0.026470792
Epoch 1640: train_loss 0.024415366 val_loss 0.02647022
Epoch 1641: train_loss 0.024413954 val_loss 0.02646964
Epoch 1642: train_loss 0.024412539 val_loss 0.026469063
Epoch 1643: train_loss 0.024411127 val_loss 0.026468486
Epoch 1644: train_loss 0.02440971 val_loss 0.026467916
Epoch 1645: train_loss 0.024408294 val_loss 0.026467348
Epoch 1646: train_loss 0.02440688 val_loss 0.02646677
Epoch 1647: train_loss 0.024405463 val_loss 0.026466187
Epoch 1648: train_loss 0.024404045 val_loss 0.026465604
Epoch 1649: train_loss 0.024402628 val_loss 0.026465032
Epoch 1650: train_loss 0.02440121 val_loss 0.026464459
Epoch 1651: train_loss 0.02439979 val_loss 0.026463872
Epoch 1652: train_loss 0.024398372 val_loss 0.026463289
Epoch 1653: train_loss 0.024396952 val_loss 0.026462715
Epoch 1654: train_loss 0.024395533 val_loss 0.026462128
Epoch 1655: train_loss 0.024394112 val_loss 0.026461557
Epoch 1656: train_loss 0.02439269 val_loss 0.02646098
Epoch 1657: train_loss 0.024391271 val_loss 0.02646039
Epoch 1658: train_loss 0.024389848 val_loss 0.026459794
Epoch 1659: train_loss 0.024388425 val_loss 0.026459225
Epoch 1660: train_loss 0.024387002 val_loss 0.026458645
Epoch 1661: train_loss 0.024385579 val_loss 0.026458062
Epoch 1662: train_loss 0.024384156 val_loss 0.026457468
Epoch 1663: train_loss 0.024382731 val_loss 0.02645688
Epoch 1664: train_loss 0.024381306 val_loss 0.026456295
Epoch 1665: train_loss 0.024379881 val_loss 0.026455708
Epoch 1666: train_loss 0.024378454 val_loss 0.026455121
Epoch 1667: train_loss 0.02437703 val_loss 0.026454536
Epoch 1668: train_loss 0.024375603 val_loss 0.02645394
Epoch 1669: train_loss 0.024374172 val_loss 0.026453346
Epoch 1670: train_loss 0.024372745 val_loss 0.026452761
Epoch 1671: train_loss 0.024371317 val_loss 0.026452176
Epoch 1672: train_loss 0.024369888 val_loss 0.026451582
Epoch 1673: train_loss 0.024368457 val_loss 0.026450988
Epoch 1674: train_loss 0.024367027 val_loss 0.026450392
Epoch 1675: train_loss 0.024365596 val_loss 0.026449798
Epoch 1676: train_loss 0.024364166 val_loss 0.026449213
Epoch 1677: train_loss 0.024362735 val_loss 0.02644862
Epoch 1678: train_loss 0.024361301 val_loss 0.026448024
Epoch 1679: train_loss 0.02435987 val_loss 0.026447432
Epoch 1680: train_loss 0.024358436 val_loss 0.026446834
Epoch 1681: train_loss 0.024357002 val_loss 0.026446242
Epoch 1682: train_loss 0.024355568 val_loss 0.026445638
Epoch 1683: train_loss 0.024354134 val_loss 0.026445042
Epoch 1684: train_loss 0.024352698 val_loss 0.026444454
Epoch 1685: train_loss 0.024351262 val_loss 0.026443848
Epoch 1686: train_loss 0.024349825 val_loss 0.026443247
Epoch 1687: train_loss 0.02434839 val_loss 0.026442656
Epoch 1688: train_loss 0.024346951 val_loss 0.026442055
Epoch 1689: train_loss 0.024345513 val_loss 0.02644145
Epoch 1690: train_loss 0.024344075 val_loss 0.026440859
Epoch 1691: train_loss 0.024342638 val_loss 0.02644026
Epoch 1692: train_loss 0.024341196 val_loss 0.026439646
Epoch 1693: train_loss 0.024339758 val_loss 0.026439048
Epoch 1694: train_loss 0.024338314 val_loss 0.026438458
Epoch 1695: train_loss 0.024336874 val_loss 0.026437853
Epoch 1696: train_loss 0.024335431 val_loss 0.026437245
Epoch 1697: train_loss 0.024333991 val_loss 0.026436636
Epoch 1698: train_loss 0.024332546 val_loss 0.026436033
Epoch 1699: train_loss 0.024331104 val_loss 0.026435437
Epoch 1700: train_loss 0.02432966 val_loss 0.02643483
Epoch 1701: train_loss 0.024328217 val_loss 0.026434228
Epoch 1702: train_loss 0.024326771 val_loss 0.026433617
Epoch 1703: train_loss 0.024325326 val_loss 0.026433013
Epoch 1704: train_loss 0.024323879 val_loss 0.026432406
Epoch 1705: train_loss 0.024322433 val_loss 0.026431797
Epoch 1706: train_loss 0.024320986 val_loss 0.026431194
Epoch 1707: train_loss 0.024319537 val_loss 0.026430586
Epoch 1708: train_loss 0.02431809 val_loss 0.026429981
Epoch 1709: train_loss 0.024316642 val_loss 0.026429372
Epoch 1710: train_loss 0.024315191 val_loss 0.026428763
Epoch 1711: train_loss 0.02431374 val_loss 0.026428161
Epoch 1712: train_loss 0.024312291 val_loss 0.026427545
Epoch 1713: train_loss 0.02431084 val_loss 0.026426941
Epoch 1714: train_loss 0.02430939 val_loss 0.026426334
Epoch 1715: train_loss 0.024307936 val_loss 0.026425716
Epoch 1716: train_loss 0.024306484 val_loss 0.0264251
Epoch 1717: train_loss 0.02430503 val_loss 0.02642449
Epoch 1718: train_loss 0.024303578 val_loss 0.02642387
Epoch 1719: train_loss 0.024302121 val_loss 0.026423259
Epoch 1720: train_loss 0.024300667 val_loss 0.026422653
Epoch 1721: train_loss 0.024299212 val_loss 0.02642204
Epoch 1722: train_loss 0.024297757 val_loss 0.026421417
Epoch 1723: train_loss 0.024296299 val_loss 0.026420802
Epoch 1724: train_loss 0.024294842 val_loss 0.026420193
Epoch 1725: train_loss 0.024293385 val_loss 0.02641958
Epoch 1726: train_loss 0.024291925 val_loss 0.026418967
Epoch 1727: train_loss 0.024290467 val_loss 0.026418349
Epoch 1728: train_loss 0.024289008 val_loss 0.026417732
Epoch 1729: train_loss 0.024287548 val_loss 0.02641712
Epoch 1730: train_loss 0.024286088 val_loss 0.026416501
Epoch 1731: train_loss 0.024284625 val_loss 0.026415896
Epoch 1732: train_loss 0.024283163 val_loss 0.026415274
Epoch 1733: train_loss 0.024281701 val_loss 0.026414648
Epoch 1734: train_loss 0.024280239 val_loss 0.026414026
Epoch 1735: train_loss 0.024278775 val_loss 0.026413424
Epoch 1736: train_loss 0.02427731 val_loss 0.026412806
Epoch 1737: train_loss 0.024275847 val_loss 0.026412176
Epoch 1738: train_loss 0.024274379 val_loss 0.026411554
Epoch 1739: train_loss 0.024272915 val_loss 0.026410935
Epoch 1740: train_loss 0.024271447 val_loss 0.026410315
Epoch 1741: train_loss 0.024269983 val_loss 0.026409697
Epoch 1742: train_loss 0.024268515 val_loss 0.026409077
Epoch 1743: train_loss 0.024267044 val_loss 0.026408453
Epoch 1744: train_loss 0.024265576 val_loss 0.026407836
Epoch 1745: train_loss 0.024264107 val_loss 0.026407221
Epoch 1746: train_loss 0.024262637 val_loss 0.026406594
Epoch 1747: train_loss 0.024261165 val_loss 0.026405966
Epoch 1748: train_loss 0.024259694 val_loss 0.026405348
Epoch 1749: train_loss 0.024258222 val_loss 0.026404724
Epoch 1750: train_loss 0.024256751 val_loss 0.026404098
Epoch 1751: train_loss 0.02425528 val_loss 0.026403477
Epoch 1752: train_loss 0.024253804 val_loss 0.026402859
Epoch 1753: train_loss 0.024252329 val_loss 0.026402226
Epoch 1754: train_loss 0.024250854 val_loss 0.026401598
Epoch 1755: train_loss 0.024249379 val_loss 0.026400978
Epoch 1756: train_loss 0.024247903 val_loss 0.026400363
Epoch 1757: train_loss 0.024246426 val_loss 0.02639974
Epoch 1758: train_loss 0.02424495 val_loss 0.026399106
Epoch 1759: train_loss 0.02424347 val_loss 0.026398476
Epoch 1760: train_loss 0.024241991 val_loss 0.026397863
Epoch 1761: train_loss 0.024240512 val_loss 0.026397226
Epoch 1762: train_loss 0.024239033 val_loss 0.026396593
Epoch 1763: train_loss 0.024237555 val_loss 0.026395971
Epoch 1764: train_loss 0.024236072 val_loss 0.026395345
Epoch 1765: train_loss 0.02423459 val_loss 0.026394708
Epoch 1766: train_loss 0.02423311 val_loss 0.026394077
Epoch 1767: train_loss 0.024231626 val_loss 0.026393458
Epoch 1768: train_loss 0.024230141 val_loss 0.026392834
Epoch 1769: train_loss 0.024228659 val_loss 0.0263922
Epoch 1770: train_loss 0.024227172 val_loss 0.02639157
Epoch 1771: train_loss 0.02422569 val_loss 0.026390946
Epoch 1772: train_loss 0.024224203 val_loss 0.026390318
Epoch 1773: train_loss 0.024222717 val_loss 0.026389685
Epoch 1774: train_loss 0.024221228 val_loss 0.02638906
Epoch 1775: train_loss 0.02421974 val_loss 0.026388427
Epoch 1776: train_loss 0.024218252 val_loss 0.026387798
Epoch 1777: train_loss 0.024216764 val_loss 0.02638718
Epoch 1778: train_loss 0.024215274 val_loss 0.026386542
Epoch 1779: train_loss 0.024213783 val_loss 0.026385901
Epoch 1780: train_loss 0.024212291 val_loss 0.026385274
Epoch 1781: train_loss 0.0242108 val_loss 0.026384654
Epoch 1782: train_loss 0.02420931 val_loss 0.026384024
Epoch 1783: train_loss 0.024207816 val_loss 0.026383383
Epoch 1784: train_loss 0.024206322 val_loss 0.026382748
Epoch 1785: train_loss 0.024204828 val_loss 0.026382126
Epoch 1786: train_loss 0.02420333 val_loss 0.02638149
Epoch 1787: train_loss 0.024201836 val_loss 0.026380854
Epoch 1788: train_loss 0.024200343 val_loss 0.02638022
Epoch 1789: train_loss 0.024198845 val_loss 0.026379598
Epoch 1790: train_loss 0.024197347 val_loss 0.026378956
Epoch 1791: train_loss 0.02419585 val_loss 0.026378315
Epoch 1792: train_loss 0.024194349 val_loss 0.026377691
Epoch 1793: train_loss 0.024192851 val_loss 0.026377065
Epoch 1794: train_loss 0.02419135 val_loss 0.026376426
Epoch 1795: train_loss 0.02418985 val_loss 0.026375785
Epoch 1796: train_loss 0.024188347 val_loss 0.026375156
Epoch 1797: train_loss 0.024186846 val_loss 0.02637452
Epoch 1798: train_loss 0.024185345 val_loss 0.026373882
Epoch 1799: train_loss 0.02418384 val_loss 0.026373245
Epoch 1800: train_loss 0.024182335 val_loss 0.026372617
Epoch 1801: train_loss 0.024180833 val_loss 0.026371978
Epoch 1802: train_loss 0.024179326 val_loss 0.026371337
Epoch 1803: train_loss 0.02417782 val_loss 0.026370706
Epoch 1804: train_loss 0.024176314 val_loss 0.026370075
Epoch 1805: train_loss 0.024174806 val_loss 0.026369436
Epoch 1806: train_loss 0.024173297 val_loss 0.02636879
Epoch 1807: train_loss 0.02417179 val_loss 0.02636817
Epoch 1808: train_loss 0.02417028 val_loss 0.02636753
Epoch 1809: train_loss 0.02416877 val_loss 0.026366893
Epoch 1810: train_loss 0.024167258 val_loss 0.026366254
Epoch 1811: train_loss 0.024165748 val_loss 0.026365625
Epoch 1812: train_loss 0.024164235 val_loss 0.02636499
Epoch 1813: train_loss 0.024162723 val_loss 0.026364343
Epoch 1814: train_loss 0.024161208 val_loss 0.02636371
Epoch 1815: train_loss 0.024159696 val_loss 0.026363079
Epoch 1816: train_loss 0.02415818 val_loss 0.026362443
Epoch 1817: train_loss 0.024156665 val_loss 0.026361799
Epoch 1818: train_loss 0.024155147 val_loss 0.026361164
Epoch 1819: train_loss 0.024153631 val_loss 0.026360529
Epoch 1820: train_loss 0.024152113 val_loss 0.02635989
Epoch 1821: train_loss 0.024150595 val_loss 0.02635925
Epoch 1822: train_loss 0.024149075 val_loss 0.026358614
Epoch 1823: train_loss 0.024147555 val_loss 0.026357977
Epoch 1824: train_loss 0.024146035 val_loss 0.026357327
Epoch 1825: train_loss 0.024144515 val_loss 0.02635669
Epoch 1826: train_loss 0.024142994 val_loss 0.026356058
Epoch 1827: train_loss 0.024141472 val_loss 0.026355423
Epoch 1828: train_loss 0.024139948 val_loss 0.026354777
Epoch 1829: train_loss 0.024138425 val_loss 0.02635414
Epoch 1830: train_loss 0.024136899 val_loss 0.026353503
Epoch 1831: train_loss 0.024135374 val_loss 0.026352858
Epoch 1832: train_loss 0.024133846 val_loss 0.026352225
Epoch 1833: train_loss 0.02413232 val_loss 0.026351593
Epoch 1834: train_loss 0.024130795 val_loss 0.026350943
Epoch 1835: train_loss 0.024129264 val_loss 0.026350299
Epoch 1836: train_loss 0.024127737 val_loss 0.026349666
Epoch 1837: train_loss 0.024126206 val_loss 0.026349029
Epoch 1838: train_loss 0.024124678 val_loss 0.026348384
Epoch 1839: train_loss 0.024123145 val_loss 0.026347743
Epoch 1840: train_loss 0.024121614 val_loss 0.026347104
Epoch 1841: train_loss 0.024120081 val_loss 0.026346458
Epoch 1842: train_loss 0.024118548 val_loss 0.026345816
Epoch 1843: train_loss 0.024117015 val_loss 0.02634518
Epoch 1844: train_loss 0.02411548 val_loss 0.026344543
Epoch 1845: train_loss 0.024113944 val_loss 0.026343899
Epoch 1846: train_loss 0.024112407 val_loss 0.026343258
Epoch 1847: train_loss 0.02411087 val_loss 0.026342615
Epoch 1848: train_loss 0.024109334 val_loss 0.026341967
Epoch 1849: train_loss 0.024107795 val_loss 0.026341332
Epoch 1850: train_loss 0.024106257 val_loss 0.0263407
Epoch 1851: train_loss 0.024104714 val_loss 0.026340054
Epoch 1852: train_loss 0.024103176 val_loss 0.02633941
Epoch 1853: train_loss 0.024101634 val_loss 0.026338771
Epoch 1854: train_loss 0.024100093 val_loss 0.02633813
Epoch 1855: train_loss 0.024098549 val_loss 0.026337491
Epoch 1856: train_loss 0.024097007 val_loss 0.026336845
Epoch 1857: train_loss 0.02409546 val_loss 0.026336202
Epoch 1858: train_loss 0.024093919 val_loss 0.026335565
Epoch 1859: train_loss 0.024092373 val_loss 0.026334926
Epoch 1860: train_loss 0.024090827 val_loss 0.026334291
Epoch 1861: train_loss 0.024089279 val_loss 0.026333643
Epoch 1862: train_loss 0.02408773 val_loss 0.026332999
Epoch 1863: train_loss 0.024086181 val_loss 0.026332362
Epoch 1864: train_loss 0.024084631 val_loss 0.026331717
Epoch 1865: train_loss 0.024083082 val_loss 0.026331082
Epoch 1866: train_loss 0.024081532 val_loss 0.02633044
Epoch 1867: train_loss 0.024079978 val_loss 0.026329799
Epoch 1868: train_loss 0.024078425 val_loss 0.026329149
Epoch 1869: train_loss 0.024076872 val_loss 0.026328506
Epoch 1870: train_loss 0.024075318 val_loss 0.026327875
Epoch 1871: train_loss 0.024073763 val_loss 0.02632723
Epoch 1872: train_loss 0.024072208 val_loss 0.026326582
Epoch 1873: train_loss 0.024070652 val_loss 0.026325956
Epoch 1874: train_loss 0.024069093 val_loss 0.02632531
Epoch 1875: train_loss 0.024067536 val_loss 0.026324663
Epoch 1876: train_loss 0.024065979 val_loss 0.026324028
Epoch 1877: train_loss 0.024064418 val_loss 0.026323397
Epoch 1878: train_loss 0.024062857 val_loss 0.026322754
Epoch 1879: train_loss 0.024061296 val_loss 0.026322108
Epoch 1880: train_loss 0.024059735 val_loss 0.026321463
Epoch 1881: train_loss 0.02405817 val_loss 0.026320824
Epoch 1882: train_loss 0.02405661 val_loss 0.026320186
Epoch 1883: train_loss 0.024055045 val_loss 0.026319543
Epoch 1884: train_loss 0.02405348 val_loss 0.026318902
Epoch 1885: train_loss 0.024051912 val_loss 0.026318261
Epoch 1886: train_loss 0.024050348 val_loss 0.02631762
Epoch 1887: train_loss 0.024048781 val_loss 0.026316985
Epoch 1888: train_loss 0.02404721 val_loss 0.026316341
Epoch 1889: train_loss 0.024045642 val_loss 0.026315697
Epoch 1890: train_loss 0.02404407 val_loss 0.026315054
Epoch 1891: train_loss 0.024042502 val_loss 0.026314422
Epoch 1892: train_loss 0.02404093 val_loss 0.026313782
Epoch 1893: train_loss 0.024039358 val_loss 0.026313135
Epoch 1894: train_loss 0.024037786 val_loss 0.026312498
Epoch 1895: train_loss 0.024036214 val_loss 0.026311863
Epoch 1896: train_loss 0.024034636 val_loss 0.026311219
Epoch 1897: train_loss 0.024033062 val_loss 0.026310574
Epoch 1898: train_loss 0.024031483 val_loss 0.026309948
Epoch 1899: train_loss 0.024029907 val_loss 0.026309298
Epoch 1900: train_loss 0.024028331 val_loss 0.026308654
Epoch 1901: train_loss 0.024026752 val_loss 0.026308022
Epoch 1902: train_loss 0.024025172 val_loss 0.026307382
Epoch 1903: train_loss 0.02402359 val_loss 0.026306733
Epoch 1904: train_loss 0.024022011 val_loss 0.026306096
Epoch 1905: train_loss 0.024020428 val_loss 0.026305465
Epoch 1906: train_loss 0.024018846 val_loss 0.026304826
Epoch 1907: train_loss 0.024017263 val_loss 0.026304176
Epoch 1908: train_loss 0.024015676 val_loss 0.026303543
Epoch 1909: train_loss 0.024014091 val_loss 0.026302917
Epoch 1910: train_loss 0.024012506 val_loss 0.026302274
Epoch 1911: train_loss 0.024010919 val_loss 0.026301626
Epoch 1912: train_loss 0.024009332 val_loss 0.026301002
Epoch 1913: train_loss 0.024007741 val_loss 0.02630037
Epoch 1914: train_loss 0.024006154 val_loss 0.02629972
Epoch 1915: train_loss 0.024004564 val_loss 0.026299078
Epoch 1916: train_loss 0.024002973 val_loss 0.026298454
Epoch 1917: train_loss 0.02400138 val_loss 0.026297819
Epoch 1918: train_loss 0.023999788 val_loss 0.026297163
Epoch 1919: train_loss 0.023998193 val_loss 0.026296532
Epoch 1920: train_loss 0.023996599 val_loss 0.026295912
Epoch 1921: train_loss 0.023995005 val_loss 0.026295265
Epoch 1922: train_loss 0.023993406 val_loss 0.026294619
Epoch 1923: train_loss 0.023991812 val_loss 0.02629399
Epoch 1924: train_loss 0.023990214 val_loss 0.026293354
Epoch 1925: train_loss 0.023988614 val_loss 0.026292706
Epoch 1926: train_loss 0.023987014 val_loss 0.026292074
Epoch 1927: train_loss 0.023985412 val_loss 0.02629145
Epoch 1928: train_loss 0.023983814 val_loss 0.026290813
Epoch 1929: train_loss 0.023982212 val_loss 0.026290173
Epoch 1930: train_loss 0.023980606 val_loss 0.026289536
Epoch 1931: train_loss 0.023979 val_loss 0.026288912
Epoch 1932: train_loss 0.023977397 val_loss 0.02628828
Epoch 1933: train_loss 0.02397579 val_loss 0.026287636
Epoch 1934: train_loss 0.023974184 val_loss 0.026286997
Epoch 1935: train_loss 0.023972578 val_loss 0.026286367
Epoch 1936: train_loss 0.023970969 val_loss 0.026285728
Epoch 1937: train_loss 0.02396936 val_loss 0.026285095
Epoch 1938: train_loss 0.02396775 val_loss 0.026284462
Epoch 1939: train_loss 0.023966137 val_loss 0.026283823
Epoch 1940: train_loss 0.023964528 val_loss 0.02628318
Epoch 1941: train_loss 0.023962911 val_loss 0.02628255
Epoch 1942: train_loss 0.0239613 val_loss 0.026281932
Epoch 1943: train_loss 0.023959685 val_loss 0.02628129
Epoch 1944: train_loss 0.023958072 val_loss 0.026280649
Epoch 1945: train_loss 0.023956453 val_loss 0.026280014
Epoch 1946: train_loss 0.023954837 val_loss 0.026279388
Epoch 1947: train_loss 0.023953218 val_loss 0.026278768
Epoch 1948: train_loss 0.023951598 val_loss 0.02627813
Epoch 1949: train_loss 0.02394998 val_loss 0.026277492
Epoch 1950: train_loss 0.023948357 val_loss 0.02627686
Epoch 1951: train_loss 0.023946736 val_loss 0.02627624
Epoch 1952: train_loss 0.023945114 val_loss 0.026275603
Epoch 1953: train_loss 0.02394349 val_loss 0.026274966
Epoch 1954: train_loss 0.023941863 val_loss 0.026274344
Epoch 1955: train_loss 0.02394024 val_loss 0.026273709
Epoch 1956: train_loss 0.023938613 val_loss 0.026273077
Epoch 1957: train_loss 0.023936983 val_loss 0.026272455
Epoch 1958: train_loss 0.023935357 val_loss 0.026271818
Epoch 1959: train_loss 0.023933727 val_loss 0.02627119
Epoch 1960: train_loss 0.0239321 val_loss 0.026270563
Epoch 1961: train_loss 0.023930468 val_loss 0.026269943
Epoch 1962: train_loss 0.023928832 val_loss 0.026269319
Epoch 1963: train_loss 0.023927204 val_loss 0.026268674
Epoch 1964: train_loss 0.023925569 val_loss 0.026268054
Epoch 1965: train_loss 0.023923934 val_loss 0.026267432
Epoch 1966: train_loss 0.023922298 val_loss 0.026266795
Epoch 1967: train_loss 0.023920663 val_loss 0.026266167
Epoch 1968: train_loss 0.023919024 val_loss 0.026265534
Epoch 1969: train_loss 0.023917384 val_loss 0.026264908
Epoch 1970: train_loss 0.023915745 val_loss 0.026264284
Epoch 1971: train_loss 0.023914106 val_loss 0.026263665
Epoch 1972: train_loss 0.023912465 val_loss 0.02626304
Epoch 1973: train_loss 0.023910822 val_loss 0.026262397
Epoch 1974: train_loss 0.023909178 val_loss 0.026261779
Epoch 1975: train_loss 0.023907535 val_loss 0.026261162
Epoch 1976: train_loss 0.023905888 val_loss 0.026260534
Epoch 1977: train_loss 0.023904245 val_loss 0.026259907
Epoch 1978: train_loss 0.023902595 val_loss 0.02625928
Epoch 1979: train_loss 0.023900948 val_loss 0.026258659
Epoch 1980: train_loss 0.0238993 val_loss 0.026258035
Epoch 1981: train_loss 0.023897648 val_loss 0.026257424
Epoch 1982: train_loss 0.023896001 val_loss 0.026256792
Epoch 1983: train_loss 0.023894347 val_loss 0.026256163
Epoch 1984: train_loss 0.023892693 val_loss 0.026255546
Epoch 1985: train_loss 0.023891041 val_loss 0.026254931
Epoch 1986: train_loss 0.023889385 val_loss 0.026254302
Epoch 1987: train_loss 0.023887731 val_loss 0.026253682
Epoch 1988: train_loss 0.023886073 val_loss 0.026253063
Epoch 1989: train_loss 0.023884416 val_loss 0.026252441
Epoch 1990: train_loss 0.02388276 val_loss 0.026251817
Epoch 1991: train_loss 0.023881098 val_loss 0.026251202
Epoch 1992: train_loss 0.023879437 val_loss 0.026250582
Epoch 1993: train_loss 0.023877777 val_loss 0.026249958
Epoch 1994: train_loss 0.023876114 val_loss 0.026249345
Epoch 1995: train_loss 0.02387445 val_loss 0.02624873
Epoch 1996: train_loss 0.023872785 val_loss 0.026248107
Epoch 1997: train_loss 0.02387112 val_loss 0.02624749
Epoch 1998: train_loss 0.023869453 val_loss 0.026246864
Epoch 1999: train_loss 0.023867786 val_loss 0.026246255
ttest: -19.6041300377439 pValue 1.1242165640074503e-72
Epoch 2000: train_loss 0.023866117 val_loss 0.026245639
ttest: -19.5805541598019 pValue 1.5701855778285451e-72
Epoch 2001: train_loss 0.023864448 val_loss 0.026245018
ttest: -19.554468379298253 pValue 2.2722080713689607e-72
Epoch 2002: train_loss 0.02386278 val_loss 0.0262444
ttest: -19.526078022014318 pValue 3.3966420221257664e-72
Epoch 2003: train_loss 0.023861108 val_loss 0.02624379
ttest: -19.4962572453583 pValue 5.180199058069901e-72
Epoch 2004: train_loss 0.023859434 val_loss 0.026243173
ttest: -19.465966741662527 pValue 7.950752274232758e-72
Epoch 2005: train_loss 0.023857761 val_loss 0.026242552
ttest: -19.436180383983054 pValue 1.2112686405518038e-71
Epoch 2006: train_loss 0.023856089 val_loss 0.026241941
ttest: -19.40746291111524 pValue 1.8170967379641904e-71
Epoch 2007: train_loss 0.023854412 val_loss 0.026241329
ttest: -19.38040588552157 pValue 2.661955439218875e-71
Epoch 2008: train_loss 0.023852734 val_loss 0.026240718
ttest: -19.355328033149434 pValue 3.791145935242043e-71
Epoch 2009: train_loss 0.023851056 val_loss 0.026240109
ttest: -19.33256219927773 pValue 5.224757330595226e-71
Epoch 2010: train_loss 0.02384938 val_loss 0.02623949
ttest: -19.312167583678757 pValue 6.9622011402347285e-71
Epoch 2011: train_loss 0.0238477 val_loss 0.026238874
ttest: -19.293933275424283 pValue 8.99756484718883e-71
Epoch 2012: train_loss 0.023846019 val_loss 0.026238263
ttest: -19.277160640376223 pValue 1.1389321027539456e-70
Epoch 2013: train_loss 0.023844339 val_loss 0.026237657
ttest: -19.261002948741847 pValue 1.4290949024194923e-70
Epoch 2014: train_loss 0.023842655 val_loss 0.026237043
ttest: -19.244676993744225 pValue 1.797272738683871e-70
Epoch 2015: train_loss 0.023840971 val_loss 0.026236432
ttest: -19.227443215336184 pValue 2.2891849696605822e-70
Epoch 2016: train_loss 0.023839287 val_loss 0.026235824
ttest: -19.209085554632395 pValue 2.9619478853058904e-70
Epoch 2017: train_loss 0.023837602 val_loss 0.026235215
ttest: -19.18951136679649 pValue 3.898173713899999e-70
Epoch 2018: train_loss 0.023835916 val_loss 0.0262346
ttest: -19.168895090932818 pValue 5.2054244186683175e-70
Epoch 2019: train_loss 0.023834227 val_loss 0.026233995
ttest: -19.147470365380673 pValue 7.029522617076929e-70
Epoch 2020: train_loss 0.023832537 val_loss 0.026233388
ttest: -19.12543932839241 pValue 9.572561955822052e-70
Epoch 2021: train_loss 0.02383085 val_loss 0.026232768
ttest: -19.103159233535788 pValue 1.3079094974095356e-69
Epoch 2022: train_loss 0.023829158 val_loss 0.026232164
ttest: -19.080756098296607 pValue 1.7897925831207641e-69
Epoch 2023: train_loss 0.023827467 val_loss 0.026231559
ttest: -19.058379612894612 pValue 2.447875546532777e-69
Epoch 2024: train_loss 0.023825772 val_loss 0.026230952
ttest: -19.03612201304553 pValue 3.341764051918207e-69
Epoch 2025: train_loss 0.02382408 val_loss 0.026230348
ttest: -19.013935392087664 pValue 4.5567433045406513e-69
Epoch 2026: train_loss 0.023822386 val_loss 0.026229741
ttest: -18.991779704189053 pValue 6.20969151704883e-69
Epoch 2027: train_loss 0.023820687 val_loss 0.026229134
ttest: -18.969544096219746 pValue 8.470254735056255e-69
Epoch 2028: train_loss 0.023818992 val_loss 0.026228532
ttest: -18.94724247578379 pValue 1.156242225056921e-68
Epoch 2029: train_loss 0.023817295 val_loss 0.026227925
ttest: -18.924734287804664 pValue 1.582631339429527e-68
Epoch 2030: train_loss 0.023815596 val_loss 0.026227321
ttest: -18.90214966717172 pValue 2.1681897933247143e-68
Epoch 2031: train_loss 0.023813896 val_loss 0.02622671
ttest: -18.879441907682914 pValue 2.974975539002804e-68
Epoch 2032: train_loss 0.023812193 val_loss 0.02622611
ttest: -18.856804869061264 pValue 4.0771917982337334e-68
Epoch 2033: train_loss 0.023810491 val_loss 0.026225515
ttest: -18.834153221113503 pValue 5.587884975551599e-68
Epoch 2034: train_loss 0.023808789 val_loss 0.02622491
ttest: -18.81170262524771 pValue 7.635470188804741e-68
Epoch 2035: train_loss 0.023807086 val_loss 0.026224311
ttest: -18.78944154708898 pValue 1.0403929907022835e-67
Epoch 2036: train_loss 0.023805378 val_loss 0.0262237
ttest: -18.767391347478068 pValue 1.413203330074189e-67
Epoch 2037: train_loss 0.023803672 val_loss 0.026223095
ttest: -18.745528544837896 pValue 1.914261903282453e-67
Epoch 2038: train_loss 0.023801966 val_loss 0.026222497
ttest: -18.723838298937267 pValue 2.5863006524772e-67
Epoch 2039: train_loss 0.023800258 val_loss 0.026221907
ttest: -18.702293414431495 pValue 3.4866118416338806e-67
Epoch 2040: train_loss 0.023798546 val_loss 0.0262213
ttest: -18.68080214293778 pValue 4.696024651053486e-67
Epoch 2041: train_loss 0.023796834 val_loss 0.026220694
ttest: -18.659330505260677 pValue 6.322151273755369e-67
Epoch 2042: train_loss 0.023795124 val_loss 0.026220096
ttest: -18.63786758316131 pValue 8.508895454757668e-67
Epoch 2043: train_loss 0.02379341 val_loss 0.026219506
ttest: -18.616424569170533 pValue 1.1446898961271419e-66
Epoch 2044: train_loss 0.023791697 val_loss 0.026218908
ttest: -18.594908765107736 pValue 1.541228724190461e-66
Epoch 2045: train_loss 0.023789981 val_loss 0.026218303
ttest: -18.573407508852863 pValue 2.0743605752718182e-66
Epoch 2046: train_loss 0.023788266 val_loss 0.02621771
ttest: -18.551904067108644 pValue 2.79151401038772e-66
Epoch 2047: train_loss 0.023786549 val_loss 0.026217118
ttest: -18.53046268050635 pValue 3.7527290162332964e-66
Epoch 2048: train_loss 0.023784831 val_loss 0.026216513
ttest: -18.50900343040405 pValue 5.045299008649789e-66
Epoch 2049: train_loss 0.023783112 val_loss 0.026215931
ttest: -18.487667820491815 pValue 6.770312123845732e-66
Epoch 2050: train_loss 0.02378139 val_loss 0.026215332
ttest: -18.4664089997993 pValue 9.073915231150647e-66
Epoch 2051: train_loss 0.023779672 val_loss 0.02621473
ttest: -18.44516760650977 pValue 1.215630668974863e-65
Epoch 2052: train_loss 0.023777947 val_loss 0.026214138
ttest: -18.423994413172416 pValue 1.6267659763032723e-65
Epoch 2053: train_loss 0.023776226 val_loss 0.026213553
ttest: -18.40290046080801 pValue 2.174199422013353e-65
Epoch 2054: train_loss 0.023774501 val_loss 0.026212955
ttest: -18.381864022669312 pValue 2.9030546892298022e-65
Epoch 2055: train_loss 0.023772774 val_loss 0.026212353
ttest: -18.360906973800002 pValue 3.871346514877119e-65
Epoch 2056: train_loss 0.023771046 val_loss 0.026211774
ttest: -18.33999896647844 pValue 5.158245082780322e-65
Epoch 2057: train_loss 0.023769319 val_loss 0.026211184
ttest: -18.319135052731014 pValue 6.867596975194508e-65
Epoch 2058: train_loss 0.02376759 val_loss 0.026210586
ttest: -18.2983135537858 pValue 9.13651828136994e-65
Epoch 2059: train_loss 0.023765858 val_loss 0.026209995
ttest: -18.277636746411584 pValue 1.2128832894390163e-64
Epoch 2060: train_loss 0.02376413 val_loss 0.02620941
ttest: -18.257016627685285 pValue 1.6085929384150652e-64
Epoch 2061: train_loss 0.023762397 val_loss 0.026208822
ttest: -18.23657098691224 pValue 2.127941509484701e-64
Epoch 2062: train_loss 0.023760661 val_loss 0.02620823
ttest: -18.216131704836172 pValue 2.814255937937065e-64
Epoch 2063: train_loss 0.023758927 val_loss 0.026207643
ttest: -18.195827709697628 pValue 3.714405673211192e-64
Epoch 2064: train_loss 0.023757191 val_loss 0.026207052
ttest: -18.175656654301637 pValue 4.892738628716714e-64
Epoch 2065: train_loss 0.023755455 val_loss 0.026206465
ttest: -18.15552732807546 pValue 6.440146691660184e-64
Epoch 2066: train_loss 0.023753718 val_loss 0.02620588
ttest: -18.135540410828636 pValue 8.45905261978004e-64
Epoch 2067: train_loss 0.023751978 val_loss 0.026205294
ttest: -18.11558740837116 pValue 1.1103920191127807e-63
Epoch 2068: train_loss 0.023750238 val_loss 0.026204703
ttest: -18.095777734306576 pValue 1.4544852919641977e-63
Epoch 2069: train_loss 0.023748498 val_loss 0.026204115
ttest: -18.076047572492993 pValue 1.9028344904257994e-63
Epoch 2070: train_loss 0.023746753 val_loss 0.02620354
ttest: -18.056450372352064 pValue 2.484474581729206e-63
Epoch 2071: train_loss 0.023745012 val_loss 0.02620296
ttest: -18.036879818190627 pValue 3.242215954242204e-63
Epoch 2072: train_loss 0.023743266 val_loss 0.026202362
ttest: -18.01747725277296 pValue 4.220702910013981e-63
Epoch 2073: train_loss 0.023741521 val_loss 0.02620178
ttest: -17.998136519310698 pValue 5.489007325988108e-63
Epoch 2074: train_loss 0.023739774 val_loss 0.026201213
ttest: -17.978908071996674 pValue 7.126404202861378e-63
Epoch 2075: train_loss 0.023738025 val_loss 0.026200624
ttest: -17.9598657181241 pValue 9.227380378198574e-63
Epoch 2076: train_loss 0.023736278 val_loss 0.026200041
ttest: -17.940849891517704 pValue 1.1941632981746988e-62
Epoch 2077: train_loss 0.023734527 val_loss 0.026199458
ttest: -17.922024884561623 pValue 1.5411838242420874e-62
Epoch 2078: train_loss 0.023732774 val_loss 0.026198886
ttest: -17.903255773781755 pValue 1.987238278309004e-62
Epoch 2079: train_loss 0.023731023 val_loss 0.026198296
ttest: -17.884638560866986 pValue 2.556719857017884e-62
Epoch 2080: train_loss 0.023729268 val_loss 0.02619773
ttest: -17.866128101223733 pValue 3.2841419626238557e-62
Epoch 2081: train_loss 0.023727514 val_loss 0.026197152
ttest: -17.847756118907387 pValue 4.209976789054444e-62
Epoch 2082: train_loss 0.02372576 val_loss 0.02619656
ttest: -17.8295261688112 pValue 5.385630201648336e-62
Epoch 2083: train_loss 0.023724 val_loss 0.026195988
ttest: -17.81134715505998 pValue 6.883840008214387e-62
Epoch 2084: train_loss 0.023722243 val_loss 0.026195422
ttest: -17.79336786950661 pValue 8.773755897982967e-62
Epoch 2085: train_loss 0.023720484 val_loss 0.026194844
ttest: -17.775473077068224 pValue 1.1168153155603611e-61
Epoch 2086: train_loss 0.023718722 val_loss 0.02619427
ttest: -17.757788973496407 pValue 1.4173449366143324e-61
Epoch 2087: train_loss 0.02371696 val_loss 0.026193688
ttest: -17.74017140801987 pValue 1.796876677476037e-61
Epoch 2088: train_loss 0.023715198 val_loss 0.026193118
ttest: -17.722768797621544 pValue 2.2711023571007127e-61
Epoch 2089: train_loss 0.023713436 val_loss 0.026192553
ttest: -17.705464284086734 pValue 2.8662871961953113e-61
Epoch 2090: train_loss 0.02371167 val_loss 0.02619198
ttest: -17.688307797214605 pValue 3.609727271178501e-61
Epoch 2091: train_loss 0.023709904 val_loss 0.026191402
ttest: -17.671278402204432 pValue 4.537585484780326e-61
Epoch 2092: train_loss 0.023708135 val_loss 0.026190827
ttest: -17.654429270794463 pValue 5.689310043786206e-61
Epoch 2093: train_loss 0.02370637 val_loss 0.026190257
ttest: -17.63771160337138 pValue 7.119778713822219e-61
Epoch 2094: train_loss 0.023704598 val_loss 0.026189694
ttest: -17.6211829685859 pValue 8.886047241812766e-61
Epoch 2095: train_loss 0.023702826 val_loss 0.026189124
ttest: -17.604718226523936 pValue 1.1079524680176584e-60
Epoch 2096: train_loss 0.023701055 val_loss 0.026188549
ttest: -17.58847350451314 pValue 1.3771753491749632e-60
Epoch 2097: train_loss 0.023699282 val_loss 0.026187988
ttest: -17.572429945500872 pValue 1.706966356746167e-60
Epoch 2098: train_loss 0.023697507 val_loss 0.02618742
ttest: -17.556458052209482 pValue 2.113430818883757e-60
Epoch 2099: train_loss 0.023695733 val_loss 0.026186852
ttest: -17.540693851092374 pValue 2.6090511919547853e-60
Epoch 2100: train_loss 0.023693955 val_loss 0.026186291
ttest: -17.525087691984023 pValue 3.213664675502544e-60
Epoch 2101: train_loss 0.02369218 val_loss 0.026185725
ttest: -17.50966973132473 pValue 3.9479037961647845e-60
Epoch 2102: train_loss 0.023690399 val_loss 0.026185155
ttest: -17.49441757905136 pValue 4.838521776201816e-60
Epoch 2103: train_loss 0.023688616 val_loss 0.026184592
ttest: -17.47936024708465 pValue 5.913852611314913e-60
Epoch 2104: train_loss 0.023686837 val_loss 0.026184032
ttest: -17.464422950759147 pValue 7.215692066854677e-60
Epoch 2105: train_loss 0.023685055 val_loss 0.026183464
ttest: -17.449662139507474 pValue 8.782259202443116e-60
Epoch 2106: train_loss 0.02368327 val_loss 0.026182907
ttest: -17.43513310634947 pValue 1.0654515446944802e-59
Epoch 2107: train_loss 0.023681486 val_loss 0.026182346
ttest: -17.420732892765535 pValue 1.290215848669656e-59
Epoch 2108: train_loss 0.0236797 val_loss 0.02618178
ttest: -17.40651998477633 pValue 1.558302937575006e-59
Epoch 2109: train_loss 0.023677912 val_loss 0.026181228
ttest: -17.3925534628433 pValue 1.8756811501666049e-59
Epoch 2110: train_loss 0.023676123 val_loss 0.026180673
ttest: -17.378754042924697 pValue 2.2524084269332568e-59
Epoch 2111: train_loss 0.023674333 val_loss 0.026180107
ttest: -17.36515296787862 pValue 2.697336986546967e-59
Epoch 2112: train_loss 0.023672543 val_loss 0.026179554
ttest: -17.351753951643783 pValue 3.2210887346488836e-59
Epoch 2113: train_loss 0.023670753 val_loss 0.02617899
ttest: -17.338531361258386 pValue 3.8370710771804745e-59
Epoch 2114: train_loss 0.023668958 val_loss 0.02617843
ttest: -17.325518070560396 pValue 4.557601209225573e-59
Epoch 2115: train_loss 0.023667164 val_loss 0.026177885
ttest: -17.31269108392025 pValue 5.399418318719365e-59
Epoch 2116: train_loss 0.023665369 val_loss 0.026177337
ttest: -17.30008005592916 pValue 6.377644670769566e-59
Epoch 2117: train_loss 0.02366357 val_loss 0.026176775
ttest: -17.287659821494433 pValue 7.513177747363139e-59
Epoch 2118: train_loss 0.023661774 val_loss 0.026176216
ttest: -17.2754946518516 pValue 8.819916827737879e-59
Epoch 2119: train_loss 0.023659974 val_loss 0.02617567
ttest: -17.26355785222921 pValue 1.0321412470673554e-58
Epoch 2120: train_loss 0.023658175 val_loss 0.026175113
ttest: -17.251766422980197 pValue 1.2053980282394846e-58
Epoch 2121: train_loss 0.023656372 val_loss 0.026174556
ttest: -17.240268770916238 pValue 1.4020960950500694e-58
Epoch 2122: train_loss 0.02365457 val_loss 0.02617401
ttest: -17.228953464390855 pValue 1.6267803421693122e-58
Epoch 2123: train_loss 0.023652766 val_loss 0.026173469
ttest: -17.217883041710007 pValue 1.8811432512387866e-58
Epoch 2124: train_loss 0.023650961 val_loss 0.026172906
ttest: -17.20703104532916 pValue 2.1687576924177667e-58
Epoch 2125: train_loss 0.023649152 val_loss 0.02617236
ttest: -17.196402658301697 pValue 2.4926839737131385e-58
Epoch 2126: train_loss 0.023647346 val_loss 0.026171817
ttest: -17.18596901989807 pValue 2.8573177256365717e-58
Epoch 2127: train_loss 0.023645537 val_loss 0.026171273
ttest: -17.17585988503955 pValue 3.260872756633087e-58
Epoch 2128: train_loss 0.023643728 val_loss 0.026170718
ttest: -17.165955014203842 pValue 3.710998953526295e-58
Epoch 2129: train_loss 0.023641916 val_loss 0.026170166
ttest: -17.156320184529672 pValue 4.207743298174055e-58
Epoch 2130: train_loss 0.023640104 val_loss 0.026169626
ttest: -17.146897569349473 pValue 4.757126657278471e-58
Epoch 2131: train_loss 0.023638291 val_loss 0.026169097
ttest: -17.137754485002432 pValue 5.357822010549476e-58
Epoch 2132: train_loss 0.023636475 val_loss 0.026168546
ttest: -17.128896764409276 pValue 6.0110030915229914e-58
Epoch 2133: train_loss 0.02363466 val_loss 0.026167994
ttest: -17.120263276796358 pValue 6.72318640058205e-58
Epoch 2134: train_loss 0.023632843 val_loss 0.026167462
ttest: -17.111857620468875 pValue 7.496398113175747e-58
Epoch 2135: train_loss 0.023631023 val_loss 0.026166921
ttest: -17.10368520772126 pValue 8.331986833341905e-58
Epoch 2136: train_loss 0.023629203 val_loss 0.026166389
ttest: -17.095914425153065 pValue 9.210643742812418e-58
Epoch 2137: train_loss 0.023627384 val_loss 0.026165837
ttest: -17.088319458352924 pValue 1.0157410549309549e-57
Epoch 2138: train_loss 0.02362556 val_loss 0.026165294
ttest: -17.081003300047296 pValue 1.1159188466628074e-57
Epoch 2139: train_loss 0.023623738 val_loss 0.026164772
ttest: -17.074003667565513 pValue 1.2207349841089203e-57
Epoch 2140: train_loss 0.023621913 val_loss 0.02616424
ttest: -17.06729449786941 pValue 1.3301492240450952e-57
Epoch 2141: train_loss 0.023620088 val_loss 0.02616369
ttest: -17.060845741939815 pValue 1.4442535460664496e-57
Epoch 2142: train_loss 0.023618262 val_loss 0.026163155
ttest: -17.054730432314763 pValue 1.5610940885435197e-57
Epoch 2143: train_loss 0.023616433 val_loss 0.026162626
ttest: -17.04884754165219 pValue 1.682058331626562e-57
Epoch 2144: train_loss 0.0236146 val_loss 0.02616209
ttest: -17.043310695171485 pValue 1.8039448928486048e-57
Epoch 2145: train_loss 0.023612771 val_loss 0.02616155
ttest: -17.038018252533476 pValue 1.928247183649091e-57
Epoch 2146: train_loss 0.023610938 val_loss 0.026161017
ttest: -17.03311486449693 pValue 2.0503327715119468e-57
Epoch 2147: train_loss 0.023609105 val_loss 0.026160492
ttest: -17.028465368838447 pValue 2.172643677641244e-57
Epoch 2148: train_loss 0.02360727 val_loss 0.026159963
ttest: -17.024182759662263 pValue 2.290882516253059e-57
Epoch 2149: train_loss 0.023605436 val_loss 0.026159424
ttest: -17.02016407550946 pValue 2.406918689544817e-57
Epoch 2150: train_loss 0.0236036 val_loss 0.026158905
ttest: -17.016559016099485 pValue 2.5147809572656965e-57
Epoch 2151: train_loss 0.023601761 val_loss 0.026158372
ttest: -17.013153913285706 pValue 2.6203046691401344e-57
Epoch 2152: train_loss 0.023599923 val_loss 0.026157841
ttest: -17.010175967087402 pValue 2.714592932388971e-57
Epoch 2153: train_loss 0.02359808 val_loss 0.026157321
ttest: -17.007516419142092 pValue 2.8001815205457095e-57
Epoch 2154: train_loss 0.023596238 val_loss 0.026156798
ttest: -17.005151345436126 pValue 2.876958067649898e-57
Epoch 2155: train_loss 0.023594394 val_loss 0.026156252
ttest: -17.003228373533187 pValue 2.938287663817597e-57
Epoch 2156: train_loss 0.02359255 val_loss 0.026155744
ttest: -17.001569893682493 pValue 2.990147957362114e-57
Epoch 2157: train_loss 0.023590706 val_loss 0.026155218
ttest: -17.00033121115223 pValue 3.025739681405274e-57
Epoch 2158: train_loss 0.023588859 val_loss 0.026154688
ttest: -16.999407000136834 pValue 3.048721302616005e-57
Epoch 2159: train_loss 0.02358701 val_loss 0.026154166
ttest: -16.99891529821541 pValue 3.053999380864875e-57
Epoch 2160: train_loss 0.023585163 val_loss 0.026153648
ttest: -16.998709399981824 pValue 3.0474087861386223e-57
Epoch 2161: train_loss 0.023583312 val_loss 0.026153138
ttest: -16.9989116787575 pValue 3.0240952333865285e-57
Epoch 2162: train_loss 0.023581458 val_loss 0.026152615
ttest: -16.999530615984277 pValue 2.9840953905585305e-57
Epoch 2163: train_loss 0.023579605 val_loss 0.026152091
ttest: -17.00048947096986 pValue 2.93105452638337e-57
Epoch 2164: train_loss 0.023577752 val_loss 0.026151575
ttest: -17.001918869828362 pValue 2.860705626190126e-57
Epoch 2165: train_loss 0.023575895 val_loss 0.026151065
ttest: -17.003581139112708 pValue 2.783114222838236e-57
Epoch 2166: train_loss 0.02357404 val_loss 0.026150553
ttest: -17.0058083816292 pValue 2.6870629239440903e-57
Epoch 2167: train_loss 0.02357218 val_loss 0.02615003
ttest: -17.008404313688246 pValue 2.581337388710973e-57
Epoch 2168: train_loss 0.02357032 val_loss 0.026149515
ttest: -17.011415134258336 pValue 2.4658280428028566e-57
Epoch 2169: train_loss 0.023568459 val_loss 0.026149008
ttest: -17.01489516997773 pValue 2.3405364282255822e-57
Epoch 2170: train_loss 0.023566598 val_loss 0.026148494
ttest: -17.018678350151003 pValue 2.212379912031663e-57
Epoch 2171: train_loss 0.023564734 val_loss 0.026147977
ttest: -17.022941280201568 pValue 2.077649912651399e-57
Epoch 2172: train_loss 0.023562869 val_loss 0.026147466
ttest: -17.027737341560083 pValue 1.937052970492444e-57
Epoch 2173: train_loss 0.023561005 val_loss 0.02614696
ttest: -17.032858541443105 pValue 1.797904191808823e-57
Epoch 2174: train_loss 0.023559136 val_loss 0.026146453
ttest: -17.03844081311457 pValue 1.6582813438236946e-57
Epoch 2175: train_loss 0.023557268 val_loss 0.026145944
ttest: -17.044534276956508 pValue 1.5188776019226693e-57
Epoch 2176: train_loss 0.023555398 val_loss 0.026145438
ttest: -17.051063237535768 pValue 1.3829112316978144e-57
Epoch 2177: train_loss 0.023553528 val_loss 0.02614493
ttest: -17.058032116653386 pValue 1.2515360542543943e-57
Epoch 2178: train_loss 0.023551658 val_loss 0.02614442
ttest: -17.06545035066045 pValue 1.125670223743576e-57
Epoch 2179: train_loss 0.02354978 val_loss 0.02614392
ttest: -17.073505395794907 pValue 1.0037072441933574e-57
Epoch 2180: train_loss 0.023547908 val_loss 0.026143428
ttest: -17.08193448326032 pValue 8.903253476701313e-58
Epoch 2181: train_loss 0.023546033 val_loss 0.026142914
ttest: -17.090791373805395 pValue 7.850911722833401e-58
Epoch 2182: train_loss 0.023544155 val_loss 0.026142413
ttest: -17.10026894496281 pValue 6.864314728800749e-58
Epoch 2183: train_loss 0.023542274 val_loss 0.02614192
ttest: -17.110191416504353 pValue 5.9647749448722654e-58
Epoch 2184: train_loss 0.023540396 val_loss 0.026141416
ttest: -17.120754350223955 pValue 5.137710729247945e-58
Epoch 2185: train_loss 0.023538515 val_loss 0.026140919
ttest: -17.13173419052884 pValue 4.399624978377622e-58
Epoch 2186: train_loss 0.023536634 val_loss 0.026140423
ttest: -17.143324942801925 pValue 3.735931224701888e-58
Epoch 2187: train_loss 0.023534749 val_loss 0.026139924
ttest: -17.155348623645583 pValue 3.1531664756041243e-58
Epoch 2188: train_loss 0.023532864 val_loss 0.026139425
ttest: -17.16795711193674 pValue 2.6397927249718222e-58
Epoch 2189: train_loss 0.023530979 val_loss 0.02613893
ttest: -17.181109305234674 pValue 2.1933063382004064e-58
Epoch 2190: train_loss 0.02352909 val_loss 0.026138434
ttest: -17.194818888800523 pValue 1.80820539766424e-58
Epoch 2191: train_loss 0.023527201 val_loss 0.026137942
ttest: -17.20919223442347 pValue 1.477022845996646e-58
Epoch 2192: train_loss 0.023525313 val_loss 0.026137449
ttest: -17.223992805500718 pValue 1.1991891217430792e-58
Epoch 2193: train_loss 0.02352342 val_loss 0.02613696
ttest: -17.239530431060235 pValue 9.636817793928082e-59
Epoch 2194: train_loss 0.023521528 val_loss 0.026136475
ttest: -17.255563286358417 pValue 7.690002010733673e-59
Epoch 2195: train_loss 0.023519631 val_loss 0.02613599
ttest: -17.272151040682296 pValue 6.088478048697333e-59
Epoch 2196: train_loss 0.023517739 val_loss 0.026135497
ttest: -17.289408904852113 pValue 4.775260862063365e-59
Epoch 2197: train_loss 0.023515843 val_loss 0.026135007
ttest: -17.307295122629863 pValue 3.7121413154124058e-59
Epoch 2198: train_loss 0.023513943 val_loss 0.026134524
ttest: -17.32576800965296 pValue 2.8616993594011363e-59
Epoch 2199: train_loss 0.023512045 val_loss 0.026134046
ttest: -17.34499882417174 pValue 2.1826337458232973e-59
Epoch 2200: train_loss 0.023510143 val_loss 0.026133552
ttest: -17.364734871149054 pValue 1.6525834779086211e-59
Epoch 2201: train_loss 0.023508243 val_loss 0.026133066
ttest: -17.385089426423946 pValue 1.2402126478703723e-59
Epoch 2202: train_loss 0.02350634 val_loss 0.026132584
ttest: -17.406347318152786 pValue 9.189523579771094e-60
Epoch 2203: train_loss 0.023504436 val_loss 0.026132107
ttest: -17.428085855329737 pValue 6.761179378772391e-60
Epoch 2204: train_loss 0.023502529 val_loss 0.026131626
ttest: -17.45053416208292 pValue 4.924074088761003e-60
Epoch 2205: train_loss 0.023500623 val_loss 0.02613114
ttest: -17.473705903991327 pValue 3.548979687052429e-60
Epoch 2206: train_loss 0.023498714 val_loss 0.026130667
ttest: -17.49749885661645 pValue 2.534815243553447e-60
Epoch 2207: train_loss 0.023496807 val_loss 0.026130198
ttest: -17.522097035987205 pValue 1.7895841875686426e-60
Epoch 2208: train_loss 0.023494896 val_loss 0.026129713
ttest: -17.54739989317376 pValue 1.2505372381167255e-60
Epoch 2209: train_loss 0.023492984 val_loss 0.02612924
ttest: -17.573420743356802 pValue 8.647367881473243e-61
Epoch 2210: train_loss 0.02349107 val_loss 0.026128778
ttest: -17.60017224704123 pValue 5.915906949277699e-61
Epoch 2211: train_loss 0.023489155 val_loss 0.026128296
ttest: -17.627607836392265 pValue 4.006508783014579e-61
Epoch 2212: train_loss 0.02348724 val_loss 0.026127825
ttest: -17.655917751122278 pValue 2.678992922218102e-61
Epoch 2213: train_loss 0.023485322 val_loss 0.026127353
ttest: -17.684939814619092 pValue 1.7724675049199027e-61
Epoch 2214: train_loss 0.023483405 val_loss 0.026126875
ttest: -17.714744883272175 pValue 1.159164411083144e-61
Epoch 2215: train_loss 0.023481485 val_loss 0.02612641
ttest: -17.745410070130298 pValue 7.484985178593085e-62
Epoch 2216: train_loss 0.023479562 val_loss 0.02612594
ttest: -17.776827132939648 pValue 4.778999765032318e-62
Epoch 2217: train_loss 0.02347764 val_loss 0.026125474
ttest: -17.809070415340045 pValue 3.013818895172668e-62
Epoch 2218: train_loss 0.023475716 val_loss 0.02612501
ttest: -17.842095041554884 pValue 1.8783423282043186e-62
Epoch 2219: train_loss 0.023473792 val_loss 0.02612454
ttest: -17.87603898798022 pValue 1.1546751265013955e-62
Epoch 2220: train_loss 0.023471866 val_loss 0.0261241
ttest: -17.91066392622283 pValue 7.023798846935334e-63
Epoch 2221: train_loss 0.023469938 val_loss 0.026123626
ttest: -17.94623759547916 pValue 4.211938129321166e-63
Epoch 2222: train_loss 0.02346801 val_loss 0.026123162
ttest: -17.982780261445868 pValue 2.4890705589377e-63
Epoch 2223: train_loss 0.023466079 val_loss 0.026122708
ttest: -18.02011042652986 pValue 1.453107576861092e-63
Epoch 2224: train_loss 0.023464147 val_loss 0.02612224
ttest: -18.058375371681198 pValue 8.362734805345608e-64
Epoch 2225: train_loss 0.023462214 val_loss 0.026121788
ttest: -18.097456694365366 pValue 4.7519353075052115e-64
Epoch 2226: train_loss 0.02346028 val_loss 0.026121337
ttest: -18.1376424962973 pValue 2.6551780732297315e-64
Epoch 2227: train_loss 0.023458343 val_loss 0.02612087
ttest: -18.178608724146017 pValue 1.4653423925512626e-64
Epoch 2228: train_loss 0.023456408 val_loss 0.02612042
ttest: -18.22050585009181 pValue 7.970108885730939e-65
Epoch 2229: train_loss 0.023454469 val_loss 0.026119964
ttest: -18.26349424599989 pValue 4.262453962790533e-65
Epoch 2230: train_loss 0.02345253 val_loss 0.026119512
ttest: -18.307378354740937 pValue 2.2473588184737364e-65
Epoch 2231: train_loss 0.023450589 val_loss 0.026119065
ttest: -18.35210609052924 pValue 1.1689137835856514e-65
Epoch 2232: train_loss 0.023448648 val_loss 0.026118625
ttest: -18.397902044966624 pValue 5.97839154886459e-66
Epoch 2233: train_loss 0.023446703 val_loss 0.02611816
ttest: -18.444936318113395 pValue 2.9991220503971674e-66
Epoch 2234: train_loss 0.02344476 val_loss 0.026117716
ttest: -18.4927159261577 pValue 1.4858607651544505e-66
Epoch 2235: train_loss 0.023442812 val_loss 0.026117278
ttest: -18.541549097277876 pValue 7.23787538792504e-67
Epoch 2236: train_loss 0.023440868 val_loss 0.026116831
ttest: -18.59160400958139 pValue 3.457904874331657e-67
Epoch 2237: train_loss 0.023438917 val_loss 0.026116367
ttest: -18.642450281024058 pValue 1.629960958994724e-67
Epoch 2238: train_loss 0.02343697 val_loss 0.026115935
ttest: -18.694633302982126 pValue 7.521463293311032e-68
Epoch 2239: train_loss 0.023435015 val_loss 0.026115494
ttest: -18.747716170809102 pValue 3.418420300382037e-68
Epoch 2240: train_loss 0.023433063 val_loss 0.02611505
ttest: -18.802023713244377 pValue 1.5229661630109596e-68
Epoch 2241: train_loss 0.023431111 val_loss 0.026114605
ttest: -18.8572647017755 pValue 6.677939183751059e-69
Epoch 2242: train_loss 0.023429155 val_loss 0.02611417
ttest: -18.91392780270981 pValue 2.861522194747492e-69
Epoch 2243: train_loss 0.0234272 val_loss 0.026113734
ttest: -18.971481840382804 pValue 1.2072303730363682e-69
Epoch 2244: train_loss 0.02342524 val_loss 0.026113294
ttest: -19.03026318744301 pValue 4.9896741327096996e-70
Epoch 2245: train_loss 0.02342328 val_loss 0.026112856
ttest: -19.09029186273932 pValue 2.019543918597636e-70
Epoch 2246: train_loss 0.023421321 val_loss 0.026112419
ttest: -19.151344346784644 pValue 8.02905952818799e-71
Epoch 2247: train_loss 0.02341936 val_loss 0.026111986
ttest: -19.21368428982072 pValue 3.1231422915567734e-71
Epoch 2248: train_loss 0.023417398 val_loss 0.02611156
ttest: -19.277250722526727 pValue 1.1894472527170727e-71
Epoch 2249: train_loss 0.023415433 val_loss 0.026111122
ttest: -19.341979042809843 pValue 4.438738239259154e-72
Epoch 2250: train_loss 0.023413468 val_loss 0.026110698
ttest: -19.40797570460027 pValue 1.6202844329709576e-72
Epoch 2251: train_loss 0.023411501 val_loss 0.02611027
ttest: -19.4751729962208 pValue 5.790183345745817e-73
Epoch 2252: train_loss 0.023409534 val_loss 0.026109837
ttest: -19.54367746773306 pValue 2.0221349532251543e-73
Epoch 2253: train_loss 0.023407564 val_loss 0.02610941
ttest: -19.613602472553765 pValue 6.8887575022683975e-74
Epoch 2254: train_loss 0.023405593 val_loss 0.026108995
ttest: -19.684524444561024 pValue 2.303007743783282e-74
Epoch 2255: train_loss 0.023403618 val_loss 0.02610857
ttest: -19.75681726319882 pValue 7.512965413775395e-75
Epoch 2256: train_loss 0.023401646 val_loss 0.02610814
ttest: -19.830595271254236 pValue 2.3870648084497066e-75
Epoch 2257: train_loss 0.023399673 val_loss 0.02610772
ttest: -19.90569828531109 pValue 7.402599062667804e-76
Epoch 2258: train_loss 0.023397695 val_loss 0.026107311
ttest: -19.98205478207302 pValue 2.242529227267674e-76
Epoch 2259: train_loss 0.023395717 val_loss 0.026106881
ttest: -20.059588241243308 pValue 6.642427216859867e-77
Epoch 2260: train_loss 0.023393739 val_loss 0.02610646
ttest: -20.138887350587368 pValue 1.90655204187783e-77
Epoch 2261: train_loss 0.023391757 val_loss 0.026106052
ttest: -20.219501495239893 pValue 5.337115239053117e-78
Epoch 2262: train_loss 0.023389779 val_loss 0.026105637
ttest: -20.30125885855184 pValue 1.4605453069978762e-78
Epoch 2263: train_loss 0.023387797 val_loss 0.026105212
ttest: -20.384662680773825 pValue 3.876986464934911e-79
Epoch 2264: train_loss 0.023385812 val_loss 0.0261048
ttest: -20.469344575644087 pValue 1.0035307553098535e-79
Epoch 2265: train_loss 0.023383826 val_loss 0.026104398
ttest: -20.555519418181575 pValue 2.524102280390082e-80
Epoch 2266: train_loss 0.02338184 val_loss 0.026103988
ttest: -20.643115294669773 pValue 6.174095753716594e-81
Epoch 2267: train_loss 0.023379851 val_loss 0.02610357
ttest: -20.732247336299995 pValue 1.4657183212397957e-81
Epoch 2268: train_loss 0.023377862 val_loss 0.026103165
ttest: -20.822840511718884 pValue 3.379906549403188e-82
Epoch 2269: train_loss 0.02337587 val_loss 0.026102757
ttest: -20.914808988531053 pValue 7.578399262771436e-83
Epoch 2270: train_loss 0.02337388 val_loss 0.02610236
ttest: -21.008588493407974 pValue 1.640748900769036e-83
Epoch 2271: train_loss 0.023371886 val_loss 0.026101949
ttest: -21.103469299075375 pValue 3.4664312801222326e-84
Epoch 2272: train_loss 0.023369892 val_loss 0.02610155
ttest: -21.200318911757446 pValue 7.049759245134906e-85
Epoch 2273: train_loss 0.023367897 val_loss 0.026101155
ttest: -21.29840847252546 pValue 1.3952201811128357e-85
Epoch 2274: train_loss 0.0233659 val_loss 0.026100747
ttest: -21.398300877897356 pValue 2.6629570449106347e-86
Epoch 2275: train_loss 0.023363901 val_loss 0.026100354
ttest: -21.499358127079265 pValue 4.948647751932105e-87
Epoch 2276: train_loss 0.023361903 val_loss 0.026099963
ttest: -21.60237441177271 pValue 8.840813147269721e-88
Epoch 2277: train_loss 0.0233599 val_loss 0.026099559
ttest: -21.706698513639093 pValue 1.533307076650831e-88
Epoch 2278: train_loss 0.023357898 val_loss 0.02609916
ttest: -21.81268674813271 pValue 2.5663664946613497e-89
Epoch 2279: train_loss 0.023355892 val_loss 0.026098771
ttest: -21.920243042008902 pValue 4.150013407557227e-90
Epoch 2280: train_loss 0.02335389 val_loss 0.026098385
ttest: -22.02961916427007 pValue 6.455394188674092e-91
Epoch 2281: train_loss 0.023351882 val_loss 0.026097987
ttest: -22.140488445033494 pValue 9.705150240600997e-92
Epoch 2282: train_loss 0.023349874 val_loss 0.026097596
ttest: -22.25286537276076 pValue 1.409346505897363e-92
Epoch 2283: train_loss 0.023347866 val_loss 0.026097208
ttest: -22.366885773863782 pValue 1.971758838851609e-93
Epoch 2284: train_loss 0.023345854 val_loss 0.02609683
ttest: -22.4828104186557 pValue 2.645494883367172e-94
Epoch 2285: train_loss 0.023343842 val_loss 0.026096437
ttest: -22.599927109984264 pValue 3.4415068199534216e-95
Epoch 2286: train_loss 0.02334183 val_loss 0.026096053
ttest: -22.719109349400732 pValue 4.278479296539283e-96
Epoch 2287: train_loss 0.023339815 val_loss 0.026095673
ttest: -22.83963182930923 pValue 5.14058077521741e-97
Epoch 2288: train_loss 0.0233378 val_loss 0.026095293
ttest: -22.962135622693914 pValue 5.90484921270331e-98
Epoch 2289: train_loss 0.023335785 val_loss 0.02609491
ttest: -23.08587715262135 pValue 6.56038031191414e-99
Epoch 2290: train_loss 0.023333767 val_loss 0.026094535
ttest: -23.21176625377538 pValue 6.942559806469872e-100
Epoch 2291: train_loss 0.023331746 val_loss 0.026094167
ttest: -23.338917207406386 pValue 7.096873599604249e-101
Epoch 2292: train_loss 0.023329727 val_loss 0.026093783
ttest: -23.46799550009986 pValue 6.9280858316803315e-102
Epoch 2293: train_loss 0.023327705 val_loss 0.026093405
ttest: -23.59848172578795 pValue 6.511330210477117e-103
Epoch 2294: train_loss 0.02332568 val_loss 0.026093038
ttest: -23.731057363318488 pValue 5.822513994946286e-104
Epoch 2295: train_loss 0.023323655 val_loss 0.026092662
ttest: -23.865062666478615 pValue 5.005818819769962e-105
Epoch 2296: train_loss 0.023321632 val_loss 0.026092287
ttest: -24.000643654425428 pValue 4.1257161856662845e-106
Epoch 2297: train_loss 0.023319604 val_loss 0.026091922
ttest: -24.137942516431853 pValue 3.250346681845416e-107
Epoch 2298: train_loss 0.023317575 val_loss 0.026091551
ttest: -24.277112272549036 pValue 2.4401415693476548e-108
Epoch 2299: train_loss 0.023315545 val_loss 0.026091184
ttest: -24.417601969147512 pValue 1.7610784646181415e-109
Epoch 2300: train_loss 0.023313513 val_loss 0.026090821
ttest: -24.560126926123797 pValue 1.2065013220403724e-110
Epoch 2301: train_loss 0.02331148 val_loss 0.026090454
ttest: -24.703831080621562 pValue 7.957532795136178e-112
Epoch 2302: train_loss 0.023309447 val_loss 0.026090093
ttest: -24.84944730000871 pValue 4.9873621585905685e-113
Epoch 2303: train_loss 0.023307413 val_loss 0.02608973
ttest: -24.996690701316002 pValue 2.9832990951550568e-114
Epoch 2304: train_loss 0.023305377 val_loss 0.026089368
ttest: -25.145566839049742 pValue 1.702142129167265e-115
Epoch 2305: train_loss 0.023303337 val_loss 0.026089022
ttest: -25.296082128496874 pValue 9.257724001285985e-117
Epoch 2306: train_loss 0.0233013 val_loss 0.026088662
ttest: -25.44823898088563 pValue 4.797163123462858e-118
Epoch 2307: train_loss 0.023299258 val_loss 0.026088301
ttest: -25.60188772453402 pValue 2.3733714579994572e-119
Epoch 2308: train_loss 0.023297219 val_loss 0.02608795
ttest: -25.75718275469892 pValue 1.1175496090099005e-120
Epoch 2309: train_loss 0.023295175 val_loss 0.0260876
ttest: -25.913810131599202 pValue 5.033866257416603e-122
Epoch 2310: train_loss 0.023293132 val_loss 0.02608725
ttest: -26.07207977586497 pValue 2.1562651624665595e-123
Epoch 2311: train_loss 0.023291085 val_loss 0.026086893
ttest: -26.232151729047157 pValue 8.754380795980631e-125
Epoch 2312: train_loss 0.02328904 val_loss 0.026086548
ttest: -26.393222668997648 pValue 3.41617485921129e-126
Epoch 2313: train_loss 0.02328699 val_loss 0.026086211
ttest: -26.556254051586016 pValue 1.258886228322438e-127
Epoch 2314: train_loss 0.023284942 val_loss 0.026085852
ttest: -26.72076021649777 pValue 4.417526793110639e-129
Epoch 2315: train_loss 0.023282893 val_loss 0.02608551
ttest: -26.88623335692774 pValue 1.4893458875241021e-130
Epoch 2316: train_loss 0.023280839 val_loss 0.02608517
ttest: -27.053665779757644 pValue 4.736444447380401e-132
Epoch 2317: train_loss 0.023278786 val_loss 0.026084827
ttest: -27.222043482380652 pValue 1.4470134386401918e-133
Epoch 2318: train_loss 0.023276733 val_loss 0.026084485
ttest: -27.392030992150907 pValue 4.194310146826779e-135
Epoch 2319: train_loss 0.023274679 val_loss 0.026084142
ttest: -27.563103974513236 pValue 1.1644663564214301e-136
Epoch 2320: train_loss 0.023272619 val_loss 0.02608382
ttest: -27.735949056196816 pValue 3.056757440154816e-138
Epoch 2321: train_loss 0.023270562 val_loss 0.026083482
ttest: -27.909854305626425 pValue 7.6868777639151385e-140
Epoch 2322: train_loss 0.023268502 val_loss 0.026083142
ttest: -28.08497967424998 pValue 1.8461707051116257e-141
Epoch 2323: train_loss 0.023266442 val_loss 0.026082816
ttest: -28.261487677202442 pValue 4.221611201418419e-143
Epoch 2324: train_loss 0.02326438 val_loss 0.026082486
ttest: -28.43864074290556 pValue 9.320631939414822e-145
Epoch 2325: train_loss 0.023262316 val_loss 0.026082156
ttest: -28.617695929662993 pValue 1.9401317928908392e-146
Epoch 2326: train_loss 0.023260253 val_loss 0.026081823
ttest: -28.797535975956528 pValue 3.889709104105278e-148
Epoch 2327: train_loss 0.023258187 val_loss 0.026081508
ttest: -28.978513730592187 pValue 7.462142519012986e-150
Epoch 2328: train_loss 0.02325612 val_loss 0.02608118
ttest: -29.160417271198835 pValue 1.3757946072044724e-151
Epoch 2329: train_loss 0.02325405 val_loss 0.026080849
ttest: -29.343411571002946 pValue 2.4307264683681254e-153
Epoch 2330: train_loss 0.02325198 val_loss 0.026080534
ttest: -29.52729048276496 pValue 4.1334915473794615e-155
Epoch 2331: train_loss 0.023249911 val_loss 0.026080212
ttest: -29.712607869635534 pValue 6.695309337466575e-157
Epoch 2332: train_loss 0.023247838 val_loss 0.026079897
ttest: -29.897983054183104 pValue 1.061921727480627e-158
Epoch 2333: train_loss 0.023245767 val_loss 0.026079576
ttest: -30.084952966279413 pValue 1.601036839012701e-160
Epoch 2334: train_loss 0.023243692 val_loss 0.02607926
ttest: -30.27211358867119 pValue 2.360698437780018e-162
Epoch 2335: train_loss 0.023241615 val_loss 0.02607895
ttest: -30.460431933515814 pValue 3.3420038365028776e-164
Epoch 2336: train_loss 0.023239538 val_loss 0.026078641
ttest: -30.648876223120336 pValue 4.640927027931973e-166
Epoch 2337: train_loss 0.02323746 val_loss 0.026078325
ttest: -30.83883774748622 pValue 6.153113053808349e-168
Epoch 2338: train_loss 0.02323538 val_loss 0.026078023
ttest: -31.029278893789314 pValue 7.960474928957295e-170
Epoch 2339: train_loss 0.023233298 val_loss 0.026077712
ttest: -31.219538616692123 pValue 1.0196584270478071e-171
Epoch 2340: train_loss 0.023231216 val_loss 0.026077395
ttest: -31.41103827383278 pValue 1.2577777255683256e-173
Epoch 2341: train_loss 0.023229133 val_loss 0.026077105
ttest: -31.60250255083892 pValue 1.5353166865697624e-175
Epoch 2342: train_loss 0.023227047 val_loss 0.026076809
ttest: -31.79474115286476 pValue 1.826460447678298e-177
Epoch 2343: train_loss 0.023224965 val_loss 0.026076492
ttest: -31.987728922525314 pValue 2.121387841125497e-179
Epoch 2344: train_loss 0.023222875 val_loss 0.026076194
ttest: -32.1801440827561 pValue 2.475890182215823e-181
Epoch 2345: train_loss 0.023220787 val_loss 0.026075901
ttest: -32.37390058916891 pValue 2.7949151610787907e-183
Epoch 2346: train_loss 0.023218695 val_loss 0.026075607
ttest: -32.56678605624975 pValue 3.200417182945499e-185
Epoch 2347: train_loss 0.023216607 val_loss 0.026075311
ttest: -32.760523964759365 pValue 3.592110718678856e-187
Epoch 2348: train_loss 0.023214515 val_loss 0.026075017
ttest: -32.95353494849683 pValue 4.091825279841632e-189
Epoch 2349: train_loss 0.023212422 val_loss 0.026074735
ttest: -33.147802698362476 pValue 4.54509990422635e-191
Epoch 2350: train_loss 0.023210324 val_loss 0.026074445
ttest: -33.341494288799076 pValue 5.126007189140907e-193
Epoch 2351: train_loss 0.02320823 val_loss 0.026074147
ttest: -33.535030614916344 pValue 5.828825022938901e-195
Epoch 2352: train_loss 0.023206135 val_loss 0.026073864
ttest: -33.72883572526253 pValue 6.635216312749903e-197
Epoch 2353: train_loss 0.023204036 val_loss 0.026073584
ttest: -33.922663102177914 pValue 7.61524690284782e-199
Epoch 2354: train_loss 0.023201935 val_loss 0.026073292
ttest: -34.11553800529834 pValue 9.015181602299348e-201
Epoch 2355: train_loss 0.023199834 val_loss 0.02607301
ttest: -34.30906599178566 pValue 1.0656978749585502e-202
Epoch 2356: train_loss 0.023197733 val_loss 0.026072742
ttest: -34.50134186414557 pValue 1.313034142000604e-204
Epoch 2357: train_loss 0.023195628 val_loss 0.026072467
ttest: -34.694229630231526 pValue 1.6230117166150195e-206
Epoch 2358: train_loss 0.023193527 val_loss 0.02607219
ttest: -34.88603294910922 pValue 2.09168301173845e-208
Epoch 2359: train_loss 0.02319142 val_loss 0.026071914
ttest: -35.077924646724384 pValue 2.745491283708311e-210
Epoch 2360: train_loss 0.023189314 val_loss 0.026071649
ttest: -35.26916013656697 pValue 3.736886625330777e-212
Epoch 2361: train_loss 0.023187205 val_loss 0.026071379
ttest: -35.4599552022451 pValue 5.2593717455835e-214
Epoch 2362: train_loss 0.023185097 val_loss 0.026071105
ttest: -35.65004028583674 pValue 7.712944983166298e-216
Epoch 2363: train_loss 0.023182988 val_loss 0.026070839
ttest: -35.839895020220546 pValue 1.1683778521033965e-217
Epoch 2364: train_loss 0.023180874 val_loss 0.02607057
ttest: -36.02849045579553 pValue 1.8728608967821163e-219
Epoch 2365: train_loss 0.023178762 val_loss 0.026070314
ttest: -36.21681062344935 pValue 3.1142465017332304e-221
Epoch 2366: train_loss 0.023176646 val_loss 0.026070053
ttest: -36.404331696573024 pValue 5.441989339751907e-223
Epoch 2367: train_loss 0.023174532 val_loss 0.026069794
ttest: -36.591288306256594 pValue 9.956631221647244e-225
Epoch 2368: train_loss 0.023172416 val_loss 0.026069533
ttest: -36.77714923449142 pValue 1.9322119303806622e-226
Epoch 2369: train_loss 0.0231703 val_loss 0.026069265
ttest: -36.96240671272087 pValue 3.9394961843459384e-228
Epoch 2370: train_loss 0.02316818 val_loss 0.026069032
ttest: -37.14680317124854 pValue 8.495977125455564e-230
Epoch 2371: train_loss 0.02316606 val_loss 0.026068771
ttest: -37.32952962117375 pValue 1.974825002264293e-231
Epoch 2372: train_loss 0.023163939 val_loss 0.026068509
ttest: -37.51241616147497 pValue 4.757214555371269e-233
Epoch 2373: train_loss 0.023161815 val_loss 0.026068276
ttest: -37.69361105354708 pValue 1.2378433209827791e-234
Epoch 2374: train_loss 0.023159694 val_loss 0.02606802
ttest: -37.87441696371578 pValue 3.3833987729951115e-236
Epoch 2375: train_loss 0.023157567 val_loss 0.026067767
ttest: -38.05403630052716 pValue 9.891145199753995e-238
Epoch 2376: train_loss 0.023155442 val_loss 0.026067548
ttest: -38.232197271149005 pValue 3.1125085975254803e-239
Epoch 2377: train_loss 0.023153314 val_loss 0.026067287
ttest: -38.4094208350727 pValue 1.0426126623892073e-240
Epoch 2378: train_loss 0.023151187 val_loss 0.026067045
ttest: -38.58624926947114 pValue 3.6745933529989303e-242
Epoch 2379: train_loss 0.023149058 val_loss 0.02606682
ttest: -38.76078226216056 pValue 1.4204175087319777e-243
Epoch 2380: train_loss 0.023146927 val_loss 0.026066571
ttest: -38.935200333671766 pValue 5.741787693442049e-245
Epoch 2381: train_loss 0.023144796 val_loss 0.026066333
ttest: -39.107861632927865 pValue 2.5151710728617366e-246
Epoch 2382: train_loss 0.023142662 val_loss 0.026066115
ttest: -39.279879636141175 pValue 1.1649774812604847e-247
Epoch 2383: train_loss 0.023140527 val_loss 0.026065873
ttest: -39.44985915739934 pValue 5.87845247204869e-249
Epoch 2384: train_loss 0.023138393 val_loss 0.026065638
ttest: -39.619199528475725 pValue 3.133197037063541e-250
Epoch 2385: train_loss 0.023136258 val_loss 0.026065428
ttest: -39.78763910182666 pValue 1.7728862565661234e-251
Epoch 2386: train_loss 0.02313412 val_loss 0.026065195
ttest: -39.95432869656778 pValue 1.083902379176466e-252
Epoch 2387: train_loss 0.023131981 val_loss 0.02606496
ttest: -40.120699655634866 pValue 6.936909269996298e-254
Epoch 2388: train_loss 0.023129841 val_loss 0.026064754
ttest: -40.28391485399553 pValue 4.934747234003387e-255
Epoch 2389: train_loss 0.023127701 val_loss 0.026064524
ttest: -40.44769615149553 pValue 3.598812881058195e-256
Epoch 2390: train_loss 0.02312556 val_loss 0.026064303
ttest: -40.60976947351672 pValue 2.8220045139138835e-257
Epoch 2391: train_loss 0.023123415 val_loss 0.026064092
ttest: -40.77015186515582 pValue 2.375666333338099e-258
Epoch 2392: train_loss 0.023121271 val_loss 0.026063876
ttest: -40.92943167829542 pValue 2.1176149086382418e-259
Epoch 2393: train_loss 0.023119126 val_loss 0.02606366
ttest: -41.08821156600075 pValue 1.9709409084417073e-260
Epoch 2394: train_loss 0.02311698 val_loss 0.026063453
ttest: -41.2447621626906 pValue 1.9842014244819583e-261
Epoch 2395: train_loss 0.02311483 val_loss 0.026063249
ttest: -41.40057101851829 pValue 2.090841098117553e-262
Epoch 2396: train_loss 0.023112683 val_loss 0.02606304
ttest: -41.555066156224456 pValue 2.3308904852921228e-263
Epoch 2397: train_loss 0.023110531 val_loss 0.026062831
ttest: -41.7082677290105 pValue 2.7439467721875477e-264
Epoch 2398: train_loss 0.023108382 val_loss 0.026062636
ttest: -41.86050757648201 pValue 3.3827201683491713e-265
Epoch 2399: train_loss 0.02310623 val_loss 0.026062422
ttest: -42.01150826434415 pValue 4.3859917290623465e-266
Epoch 2400: train_loss 0.023104075 val_loss 0.026062228
ttest: -42.16039578283325 pValue 6.08092169703091e-267
Epoch 2401: train_loss 0.023101922 val_loss 0.026062045
ttest: -42.30839089259558 pValue 8.776432030823981e-268
Epoch 2402: train_loss 0.023099765 val_loss 0.026061853
ttest: -42.45582448976687 pValue 1.3077593510668146e-268
Epoch 2403: train_loss 0.02309761 val_loss 0.02606164
ttest: -42.602419981108966 pValue 2.0205479694563116e-269
Epoch 2404: train_loss 0.023095451 val_loss 0.026061462
ttest: -42.74669923662557 pValue 3.3312798145271045e-270
Epoch 2405: train_loss 0.023093292 val_loss 0.02606128
ttest: -42.89080820608803 pValue 5.600934399352232e-271
Epoch 2406: train_loss 0.023091134 val_loss 0.026061077
ttest: -43.03294854739784 pValue 9.947911136513015e-272
Epoch 2407: train_loss 0.023088973 val_loss 0.026060894
ttest: -43.17468580771954 pValue 1.8054247369626557e-272
Epoch 2408: train_loss 0.02308681 val_loss 0.026060725
ttest: -43.31452399011328 pValue 3.446128798241015e-273
Epoch 2409: train_loss 0.023084648 val_loss 0.026060533
ttest: -43.45341518054048 pValue 6.777373236020098e-274
Epoch 2410: train_loss 0.023082484 val_loss 0.026060361
ttest: -43.592006331224084 pValue 1.35385218239599e-274
Epoch 2411: train_loss 0.02308032 val_loss 0.026060188
ttest: -43.72878494014883 pValue 2.8276336166872203e-275
Epoch 2412: train_loss 0.023078151 val_loss 0.026060006
ttest: -43.86470965006514 pValue 6.049627927059786e-276
Epoch 2413: train_loss 0.023075985 val_loss 0.02605984
ttest: -43.999195360343485 pValue 1.3396019744259004e-276
Epoch 2414: train_loss 0.023073817 val_loss 0.026059663
ttest: -44.133517114378556 pValue 2.9903089257317493e-277
Epoch 2415: train_loss 0.023071647 val_loss 0.02605949
ttest: -44.26614781539756 pValue 6.926626765724275e-278
Epoch 2416: train_loss 0.023069479 val_loss 0.026059324
ttest: -44.3983796515542 pValue 1.6212053473706777e-278
Epoch 2417: train_loss 0.023067305 val_loss 0.026059166
ttest: -44.52899138598792 pValue 3.922731507271444e-279
Epoch 2418: train_loss 0.023065131 val_loss 0.026059002
ttest: -44.65927324128005 pValue 9.559129838761588e-280
Epoch 2419: train_loss 0.02306296 val_loss 0.026058841
ttest: -44.788002295087466 pValue 2.400035529251046e-280
Epoch 2420: train_loss 0.023060784 val_loss 0.026058681
ttest: -44.916481412694374 pValue 6.047690426213499e-281
Epoch 2421: train_loss 0.02305861 val_loss 0.026058521
ttest: -45.04315232126874 pValue 1.5746824186246593e-281
Epoch 2422: train_loss 0.023056433 val_loss 0.026058381
ttest: -45.17060128287339 pValue 4.027580739779483e-282
Epoch 2423: train_loss 0.023054253 val_loss 0.026058214
ttest: -45.29566841830099 pValue 1.0743305635829227e-282
Epoch 2424: train_loss 0.023052076 val_loss 0.026058061
ttest: -45.42062159818317 pValue 2.8591072987419074e-283
Epoch 2425: train_loss 0.023049895 val_loss 0.026057929
ttest: -45.54454845910029 pValue 7.719456261470284e-284
Epoch 2426: train_loss 0.023047714 val_loss 0.026057778
ttest: -45.668437032904 pValue 2.0738525183485053e-284
Epoch 2427: train_loss 0.023045532 val_loss 0.026057625
ttest: -45.79071528057963 pValue 5.7072451152849264e-285
Epoch 2428: train_loss 0.02304335 val_loss 0.026057495
ttest: -45.91239396881426 pValue 1.5773548842547384e-285
Epoch 2429: train_loss 0.023041166 val_loss 0.02605736
ttest: -46.03382561513307 pValue 4.346993147557777e-286
Epoch 2430: train_loss 0.023038981 val_loss 0.026057204
ttest: -46.15438841209339 pValue 1.2079519939100219e-286
Epoch 2431: train_loss 0.023036797 val_loss 0.02605708
ttest: -46.274126421342594 pValue 3.3801547141013943e-287
Epoch 2432: train_loss 0.02303461 val_loss 0.026056949
ttest: -46.393075894188605 pValue 9.512800814421839e-288
Epoch 2433: train_loss 0.023032423 val_loss 0.026056811
ttest: -46.51158775826541 pValue 2.6739251286444235e-288
Epoch 2434: train_loss 0.023030233 val_loss 0.026056686
ttest: -46.630030790124096 pValue 7.452801915969652e-289
Epoch 2435: train_loss 0.023028042 val_loss 0.026056556
ttest: -46.74712816761855 pValue 2.108312510132632e-289
Epoch 2436: train_loss 0.023025852 val_loss 0.026056426
ttest: -46.86389241482159 pValue 5.938997796905278e-290
Epoch 2437: train_loss 0.023023661 val_loss 0.02605632
ttest: -46.98035658231825 pValue 1.6643419202529325e-290
Epoch 2438: train_loss 0.023021469 val_loss 0.026056195
ttest: -47.09622631425864 pValue 4.663874363913083e-291
Epoch 2439: train_loss 0.023019277 val_loss 0.026056075
ttest: -47.21120416405743 pValue 1.3136673602788533e-291
Epoch 2440: train_loss 0.023017079 val_loss 0.02605595
ttest: -47.32664016729436 pValue 3.6281705029521937e-292
Epoch 2441: train_loss 0.023014884 val_loss 0.026055846
ttest: -47.44025959159473 pValue 1.0237155142950874e-292
Epoch 2442: train_loss 0.02301269 val_loss 0.026055725
ttest: -47.55472458985679 pValue 2.811514210414241e-293
Epoch 2443: train_loss 0.023010492 val_loss 0.026055625
ttest: -47.66842535675485 pValue 7.737050678329324e-294
Epoch 2444: train_loss 0.023008294 val_loss 0.026055507
ttest: -47.78138184644204 pValue 2.132260131096287e-294
Epoch 2445: train_loss 0.023006093 val_loss 0.02605541
ttest: -47.89429442505578 pValue 5.81050936842166e-295
Epoch 2446: train_loss 0.023003895 val_loss 0.026055308
ttest: -48.00653193742032 pValue 1.5833446259762994e-295
Epoch 2447: train_loss 0.023001693 val_loss 0.02605521
ttest: -48.1191180008593 pValue 4.23594525488929e-296
Epoch 2448: train_loss 0.02299949 val_loss 0.026055114
ttest: -48.23075410707505 pValue 1.1386330424933859e-296
Epoch 2449: train_loss 0.022997288 val_loss 0.026055008
ttest: -48.342122857933276 pValue 3.0377699809223474e-297
Epoch 2450: train_loss 0.022995085 val_loss 0.026054928
ttest: -48.45326363537181 pValue 8.037976200254591e-298
Epoch 2451: train_loss 0.02299288 val_loss 0.026054827
ttest: -48.56419461487497 pValue 2.108760708879261e-298
Epoch 2452: train_loss 0.022990674 val_loss 0.026054738
ttest: -48.67495025771694 pValue 5.481525207882115e-299
Epoch 2453: train_loss 0.022988467 val_loss 0.026054658
ttest: -48.78589077586659 pValue 1.4029956934627155e-299
Epoch 2454: train_loss 0.022986261 val_loss 0.026054578
ttest: -48.89604358815439 pValue 3.5967387511940225e-300
Epoch 2455: train_loss 0.02298405 val_loss 0.02605449
ttest: -49.00609400373473 pValue 9.125618260263256e-301
Epoch 2456: train_loss 0.022981841 val_loss 0.026054412
ttest: -49.11640572428725 pValue 2.277068528946897e-301
Epoch 2457: train_loss 0.022979632 val_loss 0.026054341
ttest: -49.22600726941063 pValue 5.684202340840019e-302
Epoch 2458: train_loss 0.022977423 val_loss 0.026054254
ttest: -49.33592075910252 pValue 1.3944753972998293e-302
Epoch 2459: train_loss 0.022975208 val_loss 0.026054185
ttest: -49.44584264829329 pValue 3.380370055253625e-303
Epoch 2460: train_loss 0.022972997 val_loss 0.02605412
ttest: -49.55478972793501 pValue 8.237278120686746e-304
Epoch 2461: train_loss 0.022970784 val_loss 0.026054034
ttest: -49.66412124699322 pValue 1.9707933207983447e-304
Epoch 2462: train_loss 0.022968568 val_loss 0.026053982
ttest: -49.773195270542004 pValue 4.6817126727072e-305
Epoch 2463: train_loss 0.022966351 val_loss 0.026053919
ttest: -49.88270495836946 pValue 1.091225421089445e-305
Epoch 2464: train_loss 0.022964135 val_loss 0.026053848
ttest: -49.99199562161304 pValue 2.524320371646384e-306
Epoch 2465: train_loss 0.02296192 val_loss 0.026053801
ttest: -50.101440978061156 pValue 5.7588473937594515e-307
Epoch 2466: train_loss 0.022959702 val_loss 0.026053743
ttest: -50.21003687405363 pValue 1.3184758199088532e-307
Epoch 2467: train_loss 0.022957485 val_loss 0.026053691
ttest: -50.31949939123127 pValue 2.9417743256396593e-308
Epoch 2468: train_loss 0.022955265 val_loss 0.026053647
ttest: -50.42815713155443 pValue 6.58338388862877e-309
Epoch 2469: train_loss 0.022953045 val_loss 0.026053583
ttest: -50.53738327885177 pValue 1.443580936962645e-309
Epoch 2470: train_loss 0.022950824 val_loss 0.026053542
ttest: -50.646530196266404 pValue 3.1364477947412e-310
Epoch 2471: train_loss 0.022948602 val_loss 0.0260535
ttest: -50.75561157870665 pValue 6.7517077993017e-311
Epoch 2472: train_loss 0.02294638 val_loss 0.026053458
ttest: -50.865329126207655 pValue 1.4231694322407e-311
Epoch 2473: train_loss 0.022944156 val_loss 0.026053417
ttest: -50.974339110586115 pValue 3.005713461956e-312
Epoch 2474: train_loss 0.022941932 val_loss 0.026053367
ttest: -51.08401358010095 pValue 6.21497547574e-313
Epoch 2475: train_loss 0.022939708 val_loss 0.02605335
ttest: -51.19371155588166 pValue 1.272024058e-313
Epoch 2476: train_loss 0.02293748 val_loss 0.026053317
ttest: -51.30343734708714 pValue 2.577351433e-314
Epoch 2477: train_loss 0.022935254 val_loss 0.026053276
ttest: -51.412864714865506 pValue 5.19919866e-315
Epoch 2478: train_loss 0.022933027 val_loss 0.026053261
ttest: -51.52305084867221 pValue 1.02587499e-315
Epoch 2479: train_loss 0.022930797 val_loss 0.026053242
ttest: -51.632292962959056 pValue 2.0379099e-316
Epoch 2480: train_loss 0.02292857 val_loss 0.026053213
ttest: -51.74267217250355 pValue 3.9360293e-317
Epoch 2481: train_loss 0.02292634 val_loss 0.026053194
ttest: -51.85316832726958 pValue 7.5205e-318
Epoch 2482: train_loss 0.02292411 val_loss 0.026053183
ttest: -51.964491112998665 pValue 1.404974e-318
Epoch 2483: train_loss 0.022921879 val_loss 0.02605317
ttest: -52.07458720812482 pValue 2.6567e-319
Epoch 2484: train_loss 0.022919647 val_loss 0.026053144
ttest: -52.18519500829843 pValue 4.9387e-320
Epoch 2485: train_loss 0.022917416 val_loss 0.026053153
ttest: -52.29668945237947 pValue 8.97e-321
Epoch 2486: train_loss 0.022915183 val_loss 0.026053147
ttest: -52.40734791208549 pValue 1.64e-321
Epoch 2487: train_loss 0.022912946 val_loss 0.026053129
ttest: -52.51925938118219 pValue 2.96e-322
Epoch 2488: train_loss 0.022910712 val_loss 0.026053144
ttest: -52.63106590100959 pValue 6e-323
Epoch 2489: train_loss 0.022908479 val_loss 0.026053146
ttest: -52.74346463718415 pValue 0.0
Epoch 2490: train_loss 0.022906242 val_loss 0.026053138
ttest: -52.85542701691408 pValue 0.0
Epoch 2491: train_loss 0.022904005 val_loss 0.02605316
ttest: -52.96767906938657 pValue 0.0
Epoch 2492: train_loss 0.022901768 val_loss 0.026053164
ttest: -53.07987700041925 pValue 0.0
Epoch 2493: train_loss 0.02289953 val_loss 0.026053173
ttest: -53.194118798566066 pValue 0.0
Epoch 2494: train_loss 0.022897292 val_loss 0.026053188
ttest: -53.306937232833434 pValue 0.0
Epoch 2495: train_loss 0.022895053 val_loss 0.026053203
ttest: -53.42043807213051 pValue 0.0
Epoch 2496: train_loss 0.02289281 val_loss 0.026053227
ttest: -53.53428903581977 pValue 0.0
Epoch 2497: train_loss 0.022890572 val_loss 0.02605324
ttest: -53.648501961263015 pValue 0.0
Epoch 2498: train_loss 0.022888329 val_loss 0.026053268
ttest: -53.76309471477932 pValue 0.0
Epoch 2499: train_loss 0.022886086 val_loss 0.02605329
ttest: -53.8780679888735 pValue 0.0
Epoch 2500: train_loss 0.022883845 val_loss 0.026053328
ttest: -53.992752195914065 pValue 0.0
Epoch 2501: train_loss 0.022881601 val_loss 0.026053362
ttest: -54.1081898649078 pValue 0.0
Epoch 2502: train_loss 0.022879357 val_loss 0.026053388
ttest: -54.224056939886005 pValue 0.0
Epoch 2503: train_loss 0.022877116 val_loss 0.026053427
ttest: -54.34036423744351 pValue 0.0
Epoch 2504: train_loss 0.022874868 val_loss 0.02605347
ttest: -54.45677456455399 pValue 0.0
Epoch 2505: train_loss 0.022872623 val_loss 0.026053507
ttest: -54.57364583639941 pValue 0.0
Epoch 2506: train_loss 0.022870377 val_loss 0.026053563
ttest: -54.690989526516624 pValue 0.0
Epoch 2507: train_loss 0.02286813 val_loss 0.026053611
ttest: -54.80847116589431 pValue 0.0
Epoch 2508: train_loss 0.022865882 val_loss 0.02605366
ttest: -54.926447076328614 pValue 0.0
Epoch 2509: train_loss 0.022863634 val_loss 0.026053717
ttest: -55.044233203507886 pValue 0.0
Epoch 2510: train_loss 0.022861384 val_loss 0.02605377
ttest: -55.16359153633897 pValue 0.0
Epoch 2511: train_loss 0.022859138 val_loss 0.026053818
ttest: -55.2824148129382 pValue 0.0
Epoch 2512: train_loss 0.022856886 val_loss 0.026053896
ttest: -55.40214273078056 pValue 0.0
Epoch 2513: train_loss 0.022854635 val_loss 0.02605395
ttest: -55.5217136648606 pValue 0.0
Epoch 2514: train_loss 0.022852384 val_loss 0.02605401
ttest: -55.64219274363624 pValue 0.0
Epoch 2515: train_loss 0.022850133 val_loss 0.026054107
ttest: -55.76325732159742 pValue 0.0
Epoch 2516: train_loss 0.022847882 val_loss 0.026054168
ttest: -55.8845539759094 pValue 0.0
Epoch 2517: train_loss 0.02284563 val_loss 0.026054243
ttest: -56.0064344430082 pValue 0.0
Epoch 2518: train_loss 0.022843376 val_loss 0.02605433
ttest: -56.12821193854152 pValue 0.0
Epoch 2519: train_loss 0.022841122 val_loss 0.026054407
ttest: -56.250610150158764 pValue 0.0
Epoch 2520: train_loss 0.022838868 val_loss 0.026054496
ttest: -56.37327924052217 pValue 0.0
Epoch 2521: train_loss 0.022836614 val_loss 0.026054572
ttest: -56.49728502354673 pValue 0.0
Epoch 2522: train_loss 0.022834359 val_loss 0.026054665
ttest: -56.620514069407065 pValue 0.0
Epoch 2523: train_loss 0.022832103 val_loss 0.026054759
ttest: -56.745103102563526 pValue 0.0
Epoch 2524: train_loss 0.022829847 val_loss 0.026054846
ttest: -56.86928720549461 pValue 0.0
Epoch 2525: train_loss 0.022827592 val_loss 0.026054952
ttest: -56.99556872856875 pValue 0.0
Epoch 2526: train_loss 0.022825334 val_loss 0.02605507
ttest: -57.12075952509215 pValue 0.0
Epoch 2527: train_loss 0.022823077 val_loss 0.026055157
ttest: -57.24663501758745 pValue 0.0
Epoch 2528: train_loss 0.02282082 val_loss 0.026055273
ttest: -57.37356888162571 pValue 0.0
Epoch 2529: train_loss 0.02281856 val_loss 0.026055396
ttest: -57.50085602845195 pValue 0.0
Epoch 2530: train_loss 0.022816302 val_loss 0.02605548
ttest: -57.62849422145231 pValue 0.0
Epoch 2531: train_loss 0.022814043 val_loss 0.02605561
ttest: -57.75686178772916 pValue 0.0
Epoch 2532: train_loss 0.022811782 val_loss 0.026055738
ttest: -57.885254801134835 pValue 0.0
Epoch 2533: train_loss 0.022809524 val_loss 0.026055837
ttest: -58.01366354388988 pValue 0.0
Epoch 2534: train_loss 0.022807263 val_loss 0.026055973
ttest: -58.14390029217928 pValue 0.0
Epoch 2535: train_loss 0.022805002 val_loss 0.026056122
ttest: -58.27419109101229 pValue 0.0
Epoch 2536: train_loss 0.02280274 val_loss 0.026056226
ttest: -58.40487613260461 pValue 0.0
Epoch 2537: train_loss 0.02280048 val_loss 0.026056375
ttest: -58.53598126492709 pValue 0.0
Epoch 2538: train_loss 0.022798216 val_loss 0.026056513
ttest: -58.66751379661036 pValue 0.0
Epoch 2539: train_loss 0.022795953 val_loss 0.026056644
ttest: -58.800187991312384 pValue 0.0
Epoch 2540: train_loss 0.022793692 val_loss 0.026056783
ttest: -58.93294722384336 pValue 0.0
Epoch 2541: train_loss 0.02279143 val_loss 0.026056945
ttest: -59.06616202244605 pValue 0.0
Epoch 2542: train_loss 0.022789165 val_loss 0.026057083
ttest: -59.20018355363221 pValue 0.0
Epoch 2543: train_loss 0.022786902 val_loss 0.026057232
ttest: -59.334308866816066 pValue 0.0
Epoch 2544: train_loss 0.022784637 val_loss 0.026057394
ttest: -59.4692595075759 pValue 0.0
Epoch 2545: train_loss 0.022782374 val_loss 0.026057547
ttest: -59.605050268858875 pValue 0.0
Epoch 2546: train_loss 0.02278011 val_loss 0.026057713
ttest: -59.7406140664993 pValue 0.0
Epoch 2547: train_loss 0.022777844 val_loss 0.026057882
ttest: -59.877382925974075 pValue 0.0
Epoch 2548: train_loss 0.02277558 val_loss 0.026058037
ttest: -60.015370370278276 pValue 0.0
Epoch 2549: train_loss 0.022773314 val_loss 0.026058214
ttest: -60.15278322434888 pValue 0.0
Epoch 2550: train_loss 0.022771046 val_loss 0.026058385
ttest: -60.29034858751177 pValue 0.0
Epoch 2551: train_loss 0.022768782 val_loss 0.026058549
ttest: -60.429511714066244 pValue 0.0
Epoch 2552: train_loss 0.022766516 val_loss 0.026058735
ttest: -60.56920657622745 pValue 0.0
Epoch 2553: train_loss 0.022764247 val_loss 0.026058923
ttest: -60.7090794481337 pValue 0.0
Epoch 2554: train_loss 0.022761982 val_loss 0.026059097
ttest: -60.84984278046298 pValue 0.0
Epoch 2555: train_loss 0.022759713 val_loss 0.026059305
ttest: -60.990428240346304 pValue 0.0
Epoch 2556: train_loss 0.022757446 val_loss 0.026059492
ttest: -61.132294269852856 pValue 0.0
Epoch 2557: train_loss 0.02275518 val_loss 0.026059685
ttest: -61.27435579454034 pValue 0.0
Epoch 2558: train_loss 0.02275291 val_loss 0.026059883
ttest: -61.41808021168894 pValue 0.0
Epoch 2559: train_loss 0.022750642 val_loss 0.026060093
ttest: -61.561643517426006 pValue 0.0
Epoch 2560: train_loss 0.022748373 val_loss 0.026060278
ttest: -61.706145590912215 pValue 0.0
Epoch 2561: train_loss 0.022746107 val_loss 0.026060494
ttest: -61.85050837924151 pValue 0.0
Epoch 2562: train_loss 0.022743836 val_loss 0.026060704
ttest: -61.99508867420153 pValue 0.0
Epoch 2563: train_loss 0.022741567 val_loss 0.026060909
ttest: -62.14173668318334 pValue 0.0
Epoch 2564: train_loss 0.022739299 val_loss 0.026061121
ttest: -62.28788874331014 pValue 0.0
Epoch 2565: train_loss 0.022737028 val_loss 0.026061352
ttest: -62.43538325421068 pValue 0.0
Epoch 2566: train_loss 0.02273476 val_loss 0.026061565
ttest: -62.582747192880696 pValue 0.0
Epoch 2567: train_loss 0.022732489 val_loss 0.026061786
ttest: -62.73146092179363 pValue 0.0
Epoch 2568: train_loss 0.02273022 val_loss 0.026062025
ttest: -62.88007437233046 pValue 0.0
Epoch 2569: train_loss 0.02272795 val_loss 0.026062252
ttest: -63.03005657547509 pValue 0.0
Epoch 2570: train_loss 0.022725679 val_loss 0.026062487
ttest: -63.17994113648082 pValue 0.0
Epoch 2571: train_loss 0.02272341 val_loss 0.026062734
ttest: -63.330830182455294 pValue 0.0
Epoch 2572: train_loss 0.022721142 val_loss 0.026062967
ttest: -63.4819974829283 pValue 0.0
Epoch 2573: train_loss 0.022718867 val_loss 0.026063228
ttest: -63.63419199677034 pValue 0.0
Epoch 2574: train_loss 0.0227166 val_loss 0.026063465
ttest: -63.78741368302918 pValue 0.0
Epoch 2575: train_loss 0.022714328 val_loss 0.026063718
ttest: -63.94018268789056 pValue 0.0
Epoch 2576: train_loss 0.022712056 val_loss 0.026063988
ttest: -64.09437483261696 pValue 0.0
Epoch 2577: train_loss 0.022709787 val_loss 0.026064226
ttest: -64.24849183283241 pValue 0.0
Epoch 2578: train_loss 0.022707514 val_loss 0.026064502
ttest: -64.40403743808402 pValue 0.0
Epoch 2579: train_loss 0.022705246 val_loss 0.026064767
ttest: -64.55989271501953 pValue 0.0
Epoch 2580: train_loss 0.022702973 val_loss 0.026065022
ttest: -64.71644158788575 pValue 0.0
Epoch 2581: train_loss 0.022700703 val_loss 0.026065292
ttest: -64.87293662346494 pValue 0.0
Epoch 2582: train_loss 0.02269843 val_loss 0.026065594
ttest: -65.03126423658975 pValue 0.0
Epoch 2583: train_loss 0.02269616 val_loss 0.026065847
ttest: -65.18879461024673 pValue 0.0
Epoch 2584: train_loss 0.022693891 val_loss 0.02606614
ttest: -65.34816743320637 pValue 0.0
Epoch 2585: train_loss 0.022691619 val_loss 0.02606643
ttest: -65.50788355344432 pValue 0.0
Epoch 2586: train_loss 0.02268935 val_loss 0.0260667
ttest: -65.66831438771429 pValue 0.0
Epoch 2587: train_loss 0.022687078 val_loss 0.026067005
ttest: -65.8291018804109 pValue 0.0
Epoch 2588: train_loss 0.022684805 val_loss 0.026067296
ttest: -65.99099171637705 pValue 0.0
Epoch 2589: train_loss 0.022682536 val_loss 0.02606758
ttest: -66.15285996502818 pValue 0.0
Epoch 2590: train_loss 0.022680264 val_loss 0.026067896
ttest: -66.31547588212399 pValue 0.0
Epoch 2591: train_loss 0.022677995 val_loss 0.026068201
ttest: -66.4795913595088 pValue 0.0
Epoch 2592: train_loss 0.022675723 val_loss 0.026068507
ttest: -66.64370210963504 pValue 0.0
Epoch 2593: train_loss 0.022673452 val_loss 0.026068823
ttest: -66.8093286584468 pValue 0.0
Epoch 2594: train_loss 0.022671182 val_loss 0.026069134
ttest: -66.97418494655155 pValue 0.0
Epoch 2595: train_loss 0.02266891 val_loss 0.026069434
ttest: -67.14017996411806 pValue 0.0
Epoch 2596: train_loss 0.02266664 val_loss 0.026069783
ttest: -67.30732741625297 pValue 0.0
Epoch 2597: train_loss 0.02266437 val_loss 0.026070097
ttest: -67.47486570736788 pValue 0.0
Epoch 2598: train_loss 0.0226621 val_loss 0.02607041
ttest: -67.64240524945536 pValue 0.0
Epoch 2599: train_loss 0.022659829 val_loss 0.02607078
ttest: -67.81150059082815 pValue 0.0
Epoch 2600: train_loss 0.02265756 val_loss 0.026071081
ttest: -67.98136480400298 pValue 0.0
Epoch 2601: train_loss 0.02265529 val_loss 0.02607142
ttest: -68.15124926467834 pValue 0.0
Epoch 2602: train_loss 0.022653019 val_loss 0.026071783
ttest: -68.32231250168827 pValue 0.0
Epoch 2603: train_loss 0.02265075 val_loss 0.026072113
ttest: -68.49377687243388 pValue 0.0
Epoch 2604: train_loss 0.02264848 val_loss 0.026072465
ttest: -68.66526489180696 pValue 0.0
Epoch 2605: train_loss 0.022646211 val_loss 0.026072832
ttest: -68.83833888811468 pValue 0.0
Epoch 2606: train_loss 0.022643942 val_loss 0.02607317
ttest: -69.01144128516219 pValue 0.0
Epoch 2607: train_loss 0.022641674 val_loss 0.026073534
ttest: -69.18650485549311 pValue 0.0
Epoch 2608: train_loss 0.022639405 val_loss 0.026073916
ttest: -69.36045601366271 pValue 0.0
Epoch 2609: train_loss 0.022637136 val_loss 0.026074257
ttest: -69.53598053276782 pValue 0.0
Epoch 2610: train_loss 0.022634868 val_loss 0.026074635
ttest: -69.71193366533828 pValue 0.0
Epoch 2611: train_loss 0.022632599 val_loss 0.026075037
ttest: -69.8895014124434 pValue 0.0
Epoch 2612: train_loss 0.02263033 val_loss 0.026075376
ttest: -70.06593808827427 pValue 0.0
Epoch 2613: train_loss 0.022628065 val_loss 0.026075784
ttest: -70.24477730576344 pValue 0.0
Epoch 2614: train_loss 0.022625796 val_loss 0.026076173
ttest: -70.42288667424434 pValue 0.0
Epoch 2615: train_loss 0.022623532 val_loss 0.026076525
ttest: -70.60297341814119 pValue 0.0
Epoch 2616: train_loss 0.022621263 val_loss 0.026076948
ttest: -70.78235548481261 pValue 0.0
Epoch 2617: train_loss 0.022618998 val_loss 0.026077334
ttest: -70.96335724357611 pValue 0.0
Epoch 2618: train_loss 0.02261673 val_loss 0.026077727
ttest: -71.14401886360466 pValue 0.0
Epoch 2619: train_loss 0.022614464 val_loss 0.026078139
ttest: -71.3270909083696 pValue 0.0
Epoch 2620: train_loss 0.0226122 val_loss 0.026078545
ttest: -71.50944030069215 pValue 0.0
Epoch 2621: train_loss 0.022609932 val_loss 0.026078952
ttest: -71.6926384852624 pValue 0.0
Epoch 2622: train_loss 0.02260767 val_loss 0.02607938
ttest: -71.87670840140466 pValue 0.0
Epoch 2623: train_loss 0.022605404 val_loss 0.026079794
ttest: -72.06081727775378 pValue 0.0
Epoch 2624: train_loss 0.02260314 val_loss 0.026080208
ttest: -72.24617703139818 pValue 0.0
Epoch 2625: train_loss 0.022600874 val_loss 0.026080634
ttest: -72.43200044764598 pValue 0.0
Epoch 2626: train_loss 0.022598613 val_loss 0.026081063
ttest: -72.6186713994622 pValue 0.0
Epoch 2627: train_loss 0.022596348 val_loss 0.02608149
ttest: -72.80620798756996 pValue 0.0
Epoch 2628: train_loss 0.022594083 val_loss 0.026081922
ttest: -72.99421949435147 pValue 0.0
Epoch 2629: train_loss 0.022591822 val_loss 0.026082363
ttest: -73.18190439396534 pValue 0.0
Epoch 2630: train_loss 0.02258956 val_loss 0.026082803
ttest: -73.37201904925604 pValue 0.0
Epoch 2631: train_loss 0.022587296 val_loss 0.026083255
ttest: -73.5610129540651 pValue 0.0
Epoch 2632: train_loss 0.022585034 val_loss 0.02608371
ttest: -73.75128142458698 pValue 0.0
Epoch 2633: train_loss 0.022582775 val_loss 0.026084164
ttest: -73.94243076500636 pValue 0.0
Epoch 2634: train_loss 0.022580514 val_loss 0.026084628
ttest: -74.13364264175574 pValue 0.0
Epoch 2635: train_loss 0.02257825 val_loss 0.026085077
ttest: -74.32611194740997 pValue 0.0
Epoch 2636: train_loss 0.022575993 val_loss 0.026085544
ttest: -74.51905467651544 pValue 0.0
Epoch 2637: train_loss 0.022573734 val_loss 0.026086027
ttest: -74.71287238806394 pValue 0.0
Epoch 2638: train_loss 0.022571476 val_loss 0.026086483
ttest: -74.90715917721153 pValue 0.0
Epoch 2639: train_loss 0.022569217 val_loss 0.026086967
ttest: -75.10152532924576 pValue 0.0
Epoch 2640: train_loss 0.02256696 val_loss 0.026087463
ttest: -75.29636295651534 pValue 0.0
Epoch 2641: train_loss 0.022564702 val_loss 0.026087934
ttest: -75.4928701684649 pValue 0.0
Epoch 2642: train_loss 0.022562444 val_loss 0.026088437
ttest: -75.68944693974406 pValue 0.0
Epoch 2643: train_loss 0.022560187 val_loss 0.026088916
ttest: -75.88648227392648 pValue 0.0
Epoch 2644: train_loss 0.022557933 val_loss 0.026089413
ttest: -76.08398768870867 pValue 0.0
Epoch 2645: train_loss 0.022555677 val_loss 0.026089916
ttest: -76.28237781125628 pValue 0.0
Epoch 2646: train_loss 0.022553422 val_loss 0.026090404
ttest: -76.48203420502084 pValue 0.0
Epoch 2647: train_loss 0.022551168 val_loss 0.026090924
ttest: -76.68174381870969 pValue 0.0
Epoch 2648: train_loss 0.022548914 val_loss 0.026091425
ttest: -76.88150407577282 pValue 0.0
Epoch 2649: train_loss 0.02254666 val_loss 0.026091944
ttest: -77.08254669805585 pValue 0.0
Epoch 2650: train_loss 0.022544406 val_loss 0.026092466
ttest: -77.28365657652547 pValue 0.0
Epoch 2651: train_loss 0.022542156 val_loss 0.02609298
ttest: -77.48520686911158 pValue 0.0
Epoch 2652: train_loss 0.022539902 val_loss 0.026093513
ttest: -77.68761383917416 pValue 0.0
Epoch 2653: train_loss 0.022537652 val_loss 0.026094038
ttest: -77.89090042968333 pValue 0.0
Epoch 2654: train_loss 0.022535402 val_loss 0.026094573
ttest: -78.09380657655092 pValue 0.0
Epoch 2655: train_loss 0.022533152 val_loss 0.026095107
ttest: -78.29841006452808 pValue 0.0
Epoch 2656: train_loss 0.022530902 val_loss 0.026095644
ttest: -78.50304673523645 pValue 0.0
Epoch 2657: train_loss 0.022528656 val_loss 0.026096197
ttest: -78.70812242480372 pValue 0.0
Epoch 2658: train_loss 0.022526408 val_loss 0.026096722
ttest: -78.9144422991185 pValue 0.0
Epoch 2659: train_loss 0.022524158 val_loss 0.0260973
ttest: -79.12081598129403 pValue 0.0
Epoch 2660: train_loss 0.022521913 val_loss 0.026097842
ttest: -79.32721032139015 pValue 0.0
Epoch 2661: train_loss 0.022519667 val_loss 0.026098398
ttest: -79.53402922637395 pValue 0.0
Epoch 2662: train_loss 0.02251742 val_loss 0.02609898
ttest: -79.74251495857033 pValue 0.0
Epoch 2663: train_loss 0.022515178 val_loss 0.02609954
ttest: -79.94975205357521 pValue 0.0
Epoch 2664: train_loss 0.022512931 val_loss 0.026100121
ttest: -80.15949909904778 pValue 0.0
Epoch 2665: train_loss 0.022510689 val_loss 0.026100699
ttest: -80.36840560633668 pValue 0.0
Epoch 2666: train_loss 0.022508446 val_loss 0.026101273
ttest: -80.57814338707166 pValue 0.0
Epoch 2667: train_loss 0.022506204 val_loss 0.02610187
ttest: -80.78871256710573 pValue 0.0
Epoch 2668: train_loss 0.022503965 val_loss 0.02610245
ttest: -80.99924976442999 pValue 0.0
Epoch 2669: train_loss 0.022501722 val_loss 0.026103046
ttest: -81.21059832067117 pValue 0.0
Epoch 2670: train_loss 0.022499483 val_loss 0.026103634
ttest: -81.42234016815566 pValue 0.0
Epoch 2671: train_loss 0.022497242 val_loss 0.026104245
ttest: -81.63405872614685 pValue 0.0
Epoch 2672: train_loss 0.022495005 val_loss 0.026104847
ttest: -81.84657142950915 pValue 0.0
Epoch 2673: train_loss 0.022492766 val_loss 0.026105449
ttest: -82.05987357362216 pValue 0.0
Epoch 2674: train_loss 0.022490531 val_loss 0.026106074
ttest: -82.27311873970423 pValue 0.0
Epoch 2675: train_loss 0.022488296 val_loss 0.02610667
ttest: -82.48628490437879 pValue 0.0
Epoch 2676: train_loss 0.02248606 val_loss 0.026107298
ttest: -82.70110292859368 pValue 0.0
Epoch 2677: train_loss 0.022483826 val_loss 0.026107922
ttest: -82.91541553044927 pValue 0.0
Epoch 2678: train_loss 0.022481594 val_loss 0.026108546
ttest: -83.13048613015756 pValue 0.0
Epoch 2679: train_loss 0.022479359 val_loss 0.026109185
ttest: -83.34546643574848 pValue 0.0
Epoch 2680: train_loss 0.022477128 val_loss 0.026109828
ttest: -83.56119307128546 pValue 0.0
Epoch 2681: train_loss 0.022474898 val_loss 0.02611046
ttest: -83.77724467359323 pValue 0.0
Epoch 2682: train_loss 0.022472667 val_loss 0.026111113
ttest: -83.9935994318099 pValue 0.0
Epoch 2683: train_loss 0.022470437 val_loss 0.026111756
ttest: -84.20981661696636 pValue 0.0
Epoch 2684: train_loss 0.02246821 val_loss 0.026112407
ttest: -84.42677600314038 pValue 0.0
Epoch 2685: train_loss 0.022465982 val_loss 0.026113067
ttest: -84.64400000833808 pValue 0.0
Epoch 2686: train_loss 0.022463756 val_loss 0.026113717
ttest: -84.86235727010171 pValue 0.0
Epoch 2687: train_loss 0.02246153 val_loss 0.026114393
ttest: -85.07969600749867 pValue 0.0
Epoch 2688: train_loss 0.022459306 val_loss 0.026115056
ttest: -85.2981540492858 pValue 0.0
Epoch 2689: train_loss 0.022457082 val_loss 0.02611573
ttest: -85.51597470865855 pValue 0.0
Epoch 2690: train_loss 0.022454858 val_loss 0.026116407
ttest: -85.73492292401139 pValue 0.0
Epoch 2691: train_loss 0.022452634 val_loss 0.02611709
ttest: -85.9532233914841 pValue 0.0
Epoch 2692: train_loss 0.022450414 val_loss 0.026117766
ttest: -86.17214718019052 pValue 0.0
Epoch 2693: train_loss 0.022448193 val_loss 0.026118465
ttest: -86.39129188501897 pValue 0.0
Epoch 2694: train_loss 0.022445973 val_loss 0.026119156
ttest: -86.61064423130532 pValue 0.0
Epoch 2695: train_loss 0.022443756 val_loss 0.026119849
ttest: -86.83060712068549 pValue 0.0
Epoch 2696: train_loss 0.022441538 val_loss 0.026120558
ttest: -87.05072966932558 pValue 0.0
Epoch 2697: train_loss 0.022439322 val_loss 0.026121262
ttest: -87.27056154371833 pValue 0.0
Epoch 2698: train_loss 0.022437107 val_loss 0.026121967
ttest: -87.49009548217634 pValue 0.0
Epoch 2699: train_loss 0.02243489 val_loss 0.026122682
ttest: -87.71021872892216 pValue 0.0
Epoch 2700: train_loss 0.022432676 val_loss 0.026123403
ttest: -87.93090125988994 pValue 0.0
Epoch 2701: train_loss 0.022430465 val_loss 0.026124116
ttest: -88.15080390773677 pValue 0.0
Epoch 2702: train_loss 0.022428252 val_loss 0.026124857
ttest: -88.37168010325979 pValue 0.0
Epoch 2703: train_loss 0.022426043 val_loss 0.026125582
ttest: -88.59174556324162 pValue 0.0
Epoch 2704: train_loss 0.022423834 val_loss 0.026126299
ttest: -88.81276355708052 pValue 0.0
Epoch 2705: train_loss 0.022421625 val_loss 0.026127072
ttest: -89.03293190063512 pValue 0.0
Epoch 2706: train_loss 0.022419415 val_loss 0.026127778
ttest: -89.25223842219752 pValue 0.0
Epoch 2707: train_loss 0.02241721 val_loss 0.026128542
ttest: -89.47290828594883 pValue 0.0
Epoch 2708: train_loss 0.022415005 val_loss 0.0261293
ttest: -89.69359435589305 pValue 0.0
Epoch 2709: train_loss 0.0224128 val_loss 0.026130036
ttest: -89.9142557433726 pValue 0.0
Epoch 2710: train_loss 0.022410598 val_loss 0.026130807
ttest: -90.13353840635497 pValue 0.0
Epoch 2711: train_loss 0.022408394 val_loss 0.026131572
ttest: -90.35369216336348 pValue 0.0
Epoch 2712: train_loss 0.022406194 val_loss 0.026132321
ttest: -90.57466385299362 pValue 0.0
Epoch 2713: train_loss 0.022403993 val_loss 0.026133124
ttest: -90.79467588902747 pValue 0.0
Epoch 2714: train_loss 0.022401795 val_loss 0.026133869
ttest: -91.01366732080069 pValue 0.0
Epoch 2715: train_loss 0.022399597 val_loss 0.026134664
ttest: -91.2338975132093 pValue 0.0
Epoch 2716: train_loss 0.0223974 val_loss 0.02613545
ttest: -91.4530784938413 pValue 0.0
Epoch 2717: train_loss 0.022395201 val_loss 0.026136233
ttest: -91.67210369307772 pValue 0.0
Epoch 2718: train_loss 0.02239301 val_loss 0.026137028
ttest: -91.89093551559671 pValue 0.0
Epoch 2719: train_loss 0.022390816 val_loss 0.02613783
ttest: -92.10958079590426 pValue 0.0
Epoch 2720: train_loss 0.022388624 val_loss 0.026138615
ttest: -92.32802168989032 pValue 0.0
Epoch 2721: train_loss 0.022386432 val_loss 0.02613943
ttest: -92.54531193855246 pValue 0.0
Epoch 2722: train_loss 0.022384243 val_loss 0.026140241
ttest: -92.76324289184716 pValue 0.0
Epoch 2723: train_loss 0.022382054 val_loss 0.026141047
ttest: -92.98042785417593 pValue 0.0
Epoch 2724: train_loss 0.022379866 val_loss 0.026141878
ttest: -93.19779945276666 pValue 0.0
Epoch 2725: train_loss 0.02237768 val_loss 0.026142688
ttest: -93.4139302921796 pValue 0.0
Epoch 2726: train_loss 0.022375494 val_loss 0.026143521
ttest: -93.62972135758953 pValue 0.0
Epoch 2727: train_loss 0.02237331 val_loss 0.026144346
ttest: -93.84514310006796 pValue 0.0
Epoch 2728: train_loss 0.022371128 val_loss 0.026145186
ttest: -94.06063925005283 pValue 0.0
Epoch 2729: train_loss 0.022368945 val_loss 0.026146017
ttest: -94.2752585450993 pValue 0.0
Epoch 2730: train_loss 0.022366766 val_loss 0.026146878
ttest: -94.48991275959061 pValue 0.0
Epoch 2731: train_loss 0.022364587 val_loss 0.026147708
ttest: -94.70273678970852 pValue 0.0
Epoch 2732: train_loss 0.022362405 val_loss 0.026148578
ttest: -94.91600325416415 pValue 0.0
Epoch 2733: train_loss 0.022360232 val_loss 0.026149416
ttest: -95.12780034302696 pValue 0.0
Epoch 2734: train_loss 0.022358056 val_loss 0.026150284
ttest: -95.33905260039899 pValue 0.0
Epoch 2735: train_loss 0.02235588 val_loss 0.026151154
ttest: -95.55025851056502 pValue 0.0
Epoch 2736: train_loss 0.022353709 val_loss 0.026152007
ttest: -95.76134878073509 pValue 0.0
Epoch 2737: train_loss 0.022351535 val_loss 0.026152894
ttest: -95.9713343062504 pValue 0.0
Epoch 2738: train_loss 0.022349365 val_loss 0.026153758
ttest: -96.17972579499883 pValue 0.0
Epoch 2739: train_loss 0.022347195 val_loss 0.026154649
ttest: -96.38745269592842 pValue 0.0
Epoch 2740: train_loss 0.022345027 val_loss 0.026155543
ttest: -96.59401235680784 pValue 0.0
Epoch 2741: train_loss 0.02234286 val_loss 0.026156418
ttest: -96.80128603212688 pValue 0.0
Epoch 2742: train_loss 0.022340694 val_loss 0.02615731
ttest: -97.00591491316197 pValue 0.0
Epoch 2743: train_loss 0.022338528 val_loss 0.026158223
ttest: -97.21070690475665 pValue 0.0
Epoch 2744: train_loss 0.022336368 val_loss 0.026159126
ttest: -97.4146965523333 pValue 0.0
Epoch 2745: train_loss 0.022334207 val_loss 0.026160037
ttest: -97.61643558397087 pValue 0.0
Epoch 2746: train_loss 0.022332042 val_loss 0.026160961
ttest: -97.81871981213068 pValue 0.0
Epoch 2747: train_loss 0.022329886 val_loss 0.026161855
ttest: -98.0186646028546 pValue 0.0
Epoch 2748: train_loss 0.022327729 val_loss 0.02616278
ttest: -98.21720583479171 pValue 0.0
Epoch 2749: train_loss 0.022325572 val_loss 0.026163716
ttest: -98.4167161466081 pValue 0.0
Epoch 2750: train_loss 0.022323418 val_loss 0.02616463
ttest: -98.61329692211696 pValue 0.0
Epoch 2751: train_loss 0.022321261 val_loss 0.02616558
ttest: -98.80887249008393 pValue 0.0
Epoch 2752: train_loss 0.02231911 val_loss 0.026166502
ttest: -99.00484989931421 pValue 0.0
Epoch 2753: train_loss 0.02231696 val_loss 0.026167452
ttest: -99.19778769100907 pValue 0.0
Epoch 2754: train_loss 0.02231481 val_loss 0.026168399
ttest: -99.38959287200232 pValue 0.0
Epoch 2755: train_loss 0.02231266 val_loss 0.026169341
ttest: -99.58122484225433 pValue 0.0
Epoch 2756: train_loss 0.022310514 val_loss 0.026170311
ttest: -99.77072282741844 pValue 0.0
Epoch 2757: train_loss 0.022308368 val_loss 0.026171261
ttest: -99.95852766674982 pValue 0.0
Epoch 2758: train_loss 0.022306224 val_loss 0.026172224
ttest: -100.14603626481164 pValue 0.0
Epoch 2759: train_loss 0.02230408 val_loss 0.026173193
ttest: -100.33175576403133 pValue 0.0
Epoch 2760: train_loss 0.022301942 val_loss 0.02617416
ttest: -100.51423856202213 pValue 0.0
Epoch 2761: train_loss 0.0222998 val_loss 0.026175141
ttest: -100.69631388028901 pValue 0.0
Epoch 2762: train_loss 0.022297662 val_loss 0.026176114
ttest: -100.8779617784036 pValue 0.0
Epoch 2763: train_loss 0.022295523 val_loss 0.02617711
ttest: -101.0572171383145 pValue 0.0
Epoch 2764: train_loss 0.022293387 val_loss 0.026178095
ttest: -101.23403163560471 pValue 0.0
Epoch 2765: train_loss 0.022291254 val_loss 0.02617909
ttest: -101.40981132920311 pValue 0.0
Epoch 2766: train_loss 0.02228912 val_loss 0.026180096
ttest: -101.58305929013031 pValue 0.0
Epoch 2767: train_loss 0.022286989 val_loss 0.026181092
ttest: -101.75575111494648 pValue 0.0
Epoch 2768: train_loss 0.022284854 val_loss 0.02618211
ttest: -101.92585585214964 pValue 0.0
Epoch 2769: train_loss 0.022282727 val_loss 0.026183112
ttest: -102.09479076678191 pValue 0.0
Epoch 2770: train_loss 0.0222806 val_loss 0.02618413
ttest: -102.26155413696993 pValue 0.0
Epoch 2771: train_loss 0.022278473 val_loss 0.026185155
ttest: -102.42562259088699 pValue 0.0
Epoch 2772: train_loss 0.02227635 val_loss 0.026186178
ttest: -102.58892634216757 pValue 0.0
Epoch 2773: train_loss 0.022274226 val_loss 0.026187215
ttest: -102.74894534392206 pValue 0.0
Epoch 2774: train_loss 0.022272103 val_loss 0.02618825
ttest: -102.90764490560358 pValue 0.0
Epoch 2775: train_loss 0.022269983 val_loss 0.026189273
ttest: -103.06496829924527 pValue 0.0
Epoch 2776: train_loss 0.022267865 val_loss 0.026190326
ttest: -103.2193806689587 pValue 0.0
Epoch 2777: train_loss 0.022265749 val_loss 0.02619137
ttest: -103.3718896726744 pValue 0.0
Epoch 2778: train_loss 0.022263633 val_loss 0.026192427
ttest: -103.52140106194746 pValue 0.0
Epoch 2779: train_loss 0.022261515 val_loss 0.026193481
ttest: -103.66936424456176 pValue 0.0
Epoch 2780: train_loss 0.022259403 val_loss 0.026194533
ttest: -103.81427854884777 pValue 0.0
Epoch 2781: train_loss 0.022257295 val_loss 0.026195599
ttest: -103.9581307007793 pValue 0.0
Epoch 2782: train_loss 0.022255184 val_loss 0.026196666
ttest: -104.09885776974927 pValue 0.0
Epoch 2783: train_loss 0.022253077 val_loss 0.026197726
ttest: -104.23839016629539 pValue 0.0
Epoch 2784: train_loss 0.022250967 val_loss 0.02619883
ttest: -104.37318129684783 pValue 0.0
Epoch 2785: train_loss 0.022248864 val_loss 0.026199894
ttest: -104.5072737815567 pValue 0.0
Epoch 2786: train_loss 0.02224676 val_loss 0.026200987
ttest: -104.63755421076242 pValue 0.0
Epoch 2787: train_loss 0.022244656 val_loss 0.026202075
ttest: -104.76650360723222 pValue 0.0
Epoch 2788: train_loss 0.022242554 val_loss 0.02620318
ttest: -104.8925803933506 pValue 0.0
Epoch 2789: train_loss 0.022240456 val_loss 0.026204268
ttest: -105.0152941615092 pValue 0.0
Epoch 2790: train_loss 0.022238359 val_loss 0.026205374
ttest: -105.1355383482112 pValue 0.0
Epoch 2791: train_loss 0.022236262 val_loss 0.026206488
ttest: -105.25327327845741 pValue 0.0
Epoch 2792: train_loss 0.022234168 val_loss 0.026207592
ttest: -105.36853231580187 pValue 0.0
Epoch 2793: train_loss 0.022232074 val_loss 0.026208716
ttest: -105.48074785040286 pValue 0.0
Epoch 2794: train_loss 0.022229983 val_loss 0.026209837
ttest: -105.5903688226215 pValue 0.0
Epoch 2795: train_loss 0.022227893 val_loss 0.026210953
ttest: -105.69582315365403 pValue 0.0
Epoch 2796: train_loss 0.022225803 val_loss 0.026212094
ttest: -105.80015318162502 pValue 0.0
Epoch 2797: train_loss 0.022223715 val_loss 0.026213216
ttest: -105.90026170911119 pValue 0.0
Epoch 2798: train_loss 0.02222163 val_loss 0.026214354
ttest: -105.99813630367827 pValue 0.0
Epoch 2799: train_loss 0.022219546 val_loss 0.026215494
ttest: -106.0927390509669 pValue 0.0
Epoch 2800: train_loss 0.022217464 val_loss 0.026216647
ttest: -106.18353130303801 pValue 0.0
Epoch 2801: train_loss 0.022215381 val_loss 0.026217794
ttest: -106.27198920106038 pValue 0.0
Epoch 2802: train_loss 0.022213303 val_loss 0.026218971
ttest: -106.35754417812053 pValue 0.0
Epoch 2803: train_loss 0.022211228 val_loss 0.026220098
ttest: -106.4397049070628 pValue 0.0
Epoch 2804: train_loss 0.022209149 val_loss 0.02622129
ttest: -106.51842550793681 pValue 0.0
Epoch 2805: train_loss 0.022207076 val_loss 0.026222443
ttest: -106.59262500255873 pValue 0.0
Epoch 2806: train_loss 0.022205003 val_loss 0.02622362
ttest: -106.66532877499598 pValue 0.0
Epoch 2807: train_loss 0.02220293 val_loss 0.026224816
ttest: -106.73401352503707 pValue 0.0
Epoch 2808: train_loss 0.02220086 val_loss 0.026225971
ttest: -106.800147116731 pValue 0.0
Epoch 2809: train_loss 0.02219879 val_loss 0.026227193
ttest: -106.86212725914046 pValue 0.0
Epoch 2810: train_loss 0.022196725 val_loss 0.026228353
ttest: -106.92046701954182 pValue 0.0
Epoch 2811: train_loss 0.022194657 val_loss 0.02622957
ttest: -106.97514199614093 pValue 0.0
Epoch 2812: train_loss 0.022192597 val_loss 0.026230766
ttest: -107.02766374432797 pValue 0.0
Epoch 2813: train_loss 0.022190534 val_loss 0.026231961
ttest: -107.07539919053679 pValue 0.0
Epoch 2814: train_loss 0.022188472 val_loss 0.02623318
ttest: -107.11988821608243 pValue 0.0
Epoch 2815: train_loss 0.022186413 val_loss 0.026234388
ttest: -107.16112773941448 pValue 0.0
Epoch 2816: train_loss 0.022184357 val_loss 0.026235603
ttest: -107.19907538265255 pValue 0.0
Epoch 2817: train_loss 0.0221823 val_loss 0.026236832
ttest: -107.23314875458854 pValue 0.0
Epoch 2818: train_loss 0.022180246 val_loss 0.02623805
ttest: -107.26284378382371 pValue 0.0
Epoch 2819: train_loss 0.022178192 val_loss 0.02623929
ttest: -107.28916094407435 pValue 0.0
Epoch 2820: train_loss 0.022176143 val_loss 0.02624053
ttest: -107.31206421907456 pValue 0.0
Epoch 2821: train_loss 0.022174094 val_loss 0.026241759
ttest: -107.33103192376551 pValue 0.0
Epoch 2822: train_loss 0.022172045 val_loss 0.026243025
ttest: -107.34602560008867 pValue 0.0
Epoch 2823: train_loss 0.02217 val_loss 0.02624425
ttest: -107.35756539371866 pValue 0.0
Epoch 2824: train_loss 0.022167955 val_loss 0.026245521
ttest: -107.36453925634234 pValue 0.0
Epoch 2825: train_loss 0.022165913 val_loss 0.026246767
ttest: -107.36904398690767 pValue 0.0
Epoch 2826: train_loss 0.022163872 val_loss 0.02624803
ttest: -107.36900333351882 pValue 0.0
Epoch 2827: train_loss 0.022161834 val_loss 0.0262493
ttest: -107.36439405807315 pValue 0.0
Epoch 2828: train_loss 0.022159794 val_loss 0.026250582
ttest: -107.35720996848347 pValue 0.0
Epoch 2829: train_loss 0.022157757 val_loss 0.026251841
ttest: -107.34491251365125 pValue 0.0
Epoch 2830: train_loss 0.022155723 val_loss 0.026253145
ttest: -107.32900893428516 pValue 0.0
Epoch 2831: train_loss 0.02215369 val_loss 0.026254408
ttest: -107.30897497852024 pValue 0.0
Epoch 2832: train_loss 0.022151656 val_loss 0.026255697
ttest: -107.28637243620663 pValue 0.0
Epoch 2833: train_loss 0.022149626 val_loss 0.026256995
ttest: -107.25859946275858 pValue 0.0
Epoch 2834: train_loss 0.0221476 val_loss 0.026258282
ttest: -107.22663483020472 pValue 0.0
Epoch 2835: train_loss 0.022145573 val_loss 0.026259592
ttest: -107.19150156260812 pValue 0.0
Epoch 2836: train_loss 0.022143546 val_loss 0.026260903
ttest: -107.15168060473611 pValue 0.0
Epoch 2837: train_loss 0.022141524 val_loss 0.026262205
ttest: -107.10819864036738 pValue 0.0
Epoch 2838: train_loss 0.0221395 val_loss 0.02626354
ttest: -107.05996201561386 pValue 0.0
Epoch 2839: train_loss 0.022137482 val_loss 0.026264833
ttest: -107.00805910507862 pValue 0.0
Epoch 2840: train_loss 0.022135463 val_loss 0.026266169
ttest: -106.95245216141069 pValue 0.0
Epoch 2841: train_loss 0.022133445 val_loss 0.026267495
ttest: -106.89210270892859 pValue 0.0
Epoch 2842: train_loss 0.022131432 val_loss 0.02626883
ttest: -106.82906815951898 pValue 0.0
Epoch 2843: train_loss 0.022129418 val_loss 0.026270172
ttest: -106.76079111654104 pValue 0.0
Epoch 2844: train_loss 0.022127407 val_loss 0.026271505
ttest: -106.68831311059904 pValue 0.0
Epoch 2845: train_loss 0.022125393 val_loss 0.026272856
ttest: -106.61261903073311 pValue 0.0
Epoch 2846: train_loss 0.022123385 val_loss 0.026274202
ttest: -106.53270620936385 pValue 0.0
Epoch 2847: train_loss 0.02212138 val_loss 0.026275553
ttest: -106.4485796071078 pValue 0.0
Epoch 2848: train_loss 0.022119375 val_loss 0.026276913
ttest: -106.3602202473203 pValue 0.0
Epoch 2849: train_loss 0.022117373 val_loss 0.026278265
ttest: -106.26716771885414 pValue 0.0
Epoch 2850: train_loss 0.022115368 val_loss 0.026279654
ttest: -106.17094959143506 pValue 0.0
Epoch 2851: train_loss 0.022113368 val_loss 0.026280988
ttest: -106.07106504623842 pValue 0.0
Epoch 2852: train_loss 0.022111367 val_loss 0.026282407
ttest: -105.9664262405973 pValue 0.0
Epoch 2853: train_loss 0.02210937 val_loss 0.026283763
ttest: -105.85764224199386 pValue 0.0
Epoch 2854: train_loss 0.022107378 val_loss 0.026285151
ttest: -105.7452061936891 pValue 0.0
Epoch 2855: train_loss 0.02210538 val_loss 0.02628655
ttest: -105.62912546997292 pValue 0.0
Epoch 2856: train_loss 0.022103392 val_loss 0.02628792
ttest: -105.50789159663113 pValue 0.0
Epoch 2857: train_loss 0.022101399 val_loss 0.026289346
ttest: -105.38453342661741 pValue 0.0
Epoch 2858: train_loss 0.022099413 val_loss 0.026290722
ttest: -105.25506911223663 pValue 0.0
Epoch 2859: train_loss 0.022097424 val_loss 0.026292145
ttest: -105.12353499637504 pValue 0.0
Epoch 2860: train_loss 0.022095438 val_loss 0.026293559
ttest: -104.98641561676413 pValue 0.0
Epoch 2861: train_loss 0.022093454 val_loss 0.026294943
ttest: -104.84626375687589 pValue 0.0
Epoch 2862: train_loss 0.02209147 val_loss 0.026296398
ttest: -104.70201853670609 pValue 0.0
Epoch 2863: train_loss 0.022089493 val_loss 0.02629779
ttest: -104.55430870720224 pValue 0.0
Epoch 2864: train_loss 0.022087514 val_loss 0.026299236
ttest: -104.40257286491997 pValue 0.0
Epoch 2865: train_loss 0.022085536 val_loss 0.026300661
ttest: -104.24638330176805 pValue 0.0
Epoch 2866: train_loss 0.02208356 val_loss 0.026302088
ttest: -104.08624602411973 pValue 0.0
Epoch 2867: train_loss 0.022081587 val_loss 0.026303552
ttest: -103.92266273202902 pValue 0.0
Epoch 2868: train_loss 0.022079617 val_loss 0.026304962
ttest: -103.7561997432573 pValue 0.0
Epoch 2869: train_loss 0.022077642 val_loss 0.02630645
ttest: -103.5853043684424 pValue 0.0
Epoch 2870: train_loss 0.022075675 val_loss 0.026307875
ttest: -103.41012184459395 pValue 0.0
Epoch 2871: train_loss 0.022073708 val_loss 0.026309343
ttest: -103.23210491973973 pValue 0.0
Epoch 2872: train_loss 0.022071742 val_loss 0.026310807
ttest: -103.0497701407868 pValue 0.0
Epoch 2873: train_loss 0.022069778 val_loss 0.026312247
ttest: -102.86375077626283 pValue 0.0
Epoch 2874: train_loss 0.022067815 val_loss 0.026313761
ttest: -102.67493494143547 pValue 0.0
Epoch 2875: train_loss 0.022065856 val_loss 0.026315195
ttest: -102.48195784008414 pValue 0.0
Epoch 2876: train_loss 0.022063896 val_loss 0.026316712
ttest: -102.28528536955216 pValue 0.0
Epoch 2877: train_loss 0.02206194 val_loss 0.026318168
ttest: -102.08504210374912 pValue 0.0
Epoch 2878: train_loss 0.022059985 val_loss 0.026319655
ttest: -101.88218880534198 pValue 0.0
Epoch 2879: train_loss 0.022058029 val_loss 0.026321154
ttest: -101.67426755072935 pValue 0.0
Epoch 2880: train_loss 0.022056079 val_loss 0.026322626
ttest: -101.4648137460332 pValue 0.0
Epoch 2881: train_loss 0.022054128 val_loss 0.026324157
ttest: -101.25035515761925 pValue 0.0
Epoch 2882: train_loss 0.02205218 val_loss 0.026325637
ttest: -101.03445387232716 pValue 0.0
Epoch 2883: train_loss 0.022050234 val_loss 0.026327176
ttest: -100.81312448534534 pValue 0.0
Epoch 2884: train_loss 0.022048287 val_loss 0.026328653
ttest: -100.58993825060658 pValue 0.0
Epoch 2885: train_loss 0.022046342 val_loss 0.026330194
ttest: -100.36338987377727 pValue 0.0
Epoch 2886: train_loss 0.022044402 val_loss 0.026331706
ttest: -100.1330571805665 pValue 0.0
Epoch 2887: train_loss 0.022042463 val_loss 0.026333222
ttest: -99.89949381824445 pValue 0.0
Epoch 2888: train_loss 0.022040522 val_loss 0.02633477
ttest: -99.66415080047904 pValue 0.0
Epoch 2889: train_loss 0.022038585 val_loss 0.026336296
ttest: -99.42415985477535 pValue 0.0
Epoch 2890: train_loss 0.02203665 val_loss 0.026337836
ttest: -99.18152214863738 pValue 0.0
Epoch 2891: train_loss 0.022034716 val_loss 0.026339376
ttest: -98.93581577233697 pValue 0.0
Epoch 2892: train_loss 0.022032786 val_loss 0.02634091
ttest: -98.68708184976732 pValue 0.0
Epoch 2893: train_loss 0.022030856 val_loss 0.026342483
ttest: -98.43675162214237 pValue 0.0
Epoch 2894: train_loss 0.022028927 val_loss 0.026343994
ttest: -98.18158931727731 pValue 0.0
Epoch 2895: train_loss 0.022027 val_loss 0.026345596
ttest: -97.92539582860148 pValue 0.0
Epoch 2896: train_loss 0.022025075 val_loss 0.026347129
ttest: -97.66539285707655 pValue 0.0
Epoch 2897: train_loss 0.022023153 val_loss 0.026348712
ttest: -97.40301317537323 pValue 0.0
Epoch 2898: train_loss 0.022021228 val_loss 0.026350277
ttest: -97.1378948059941 pValue 0.0
Epoch 2899: train_loss 0.02201931 val_loss 0.026351841
ttest: -96.87002706441638 pValue 0.0
Epoch 2900: train_loss 0.02201739 val_loss 0.026353432
ttest: -96.59944046415089 pValue 0.0
Epoch 2901: train_loss 0.022015475 val_loss 0.026355006
ttest: -96.32623885198271 pValue 0.0
Epoch 2902: train_loss 0.02201356 val_loss 0.02635659
ttest: -96.05044698920653 pValue 0.0
Epoch 2903: train_loss 0.022011647 val_loss 0.026358178
ttest: -95.77255477929381 pValue 0.0
Epoch 2904: train_loss 0.022009734 val_loss 0.02635978
ttest: -95.4931064332116 pValue 0.0
Epoch 2905: train_loss 0.022007827 val_loss 0.026361376
ttest: -95.20932576786717 pValue 0.0
Epoch 2906: train_loss 0.022005916 val_loss 0.026362985
ttest: -94.92407662409512 pValue 0.0
Epoch 2907: train_loss 0.02200401 val_loss 0.026364593
ttest: -94.63643463666219 pValue 0.0
Epoch 2908: train_loss 0.022002105 val_loss 0.026366197
ttest: -94.34743518373354 pValue 0.0
Epoch 2909: train_loss 0.022000203 val_loss 0.026367838
ttest: -94.05475914327623 pValue 0.0
Epoch 2910: train_loss 0.021998301 val_loss 0.026369419
ttest: -93.76036594564741 pValue 0.0
Epoch 2911: train_loss 0.021996401 val_loss 0.026371079
ttest: -93.46512824826122 pValue 0.0
Epoch 2912: train_loss 0.021994505 val_loss 0.026372679
ttest: -93.16642734640251 pValue 0.0
Epoch 2913: train_loss 0.021992609 val_loss 0.02637433
ttest: -92.86656577236964 pValue 0.0
Epoch 2914: train_loss 0.021990713 val_loss 0.026375953
ttest: -92.56465720243037 pValue 0.0
Epoch 2915: train_loss 0.02198882 val_loss 0.026377596
ttest: -92.25982554228908 pValue 0.0
Epoch 2916: train_loss 0.02198693 val_loss 0.026379239
ttest: -91.95400361344076 pValue 0.0
Epoch 2917: train_loss 0.02198504 val_loss 0.026380887
ttest: -91.64628725801235 pValue 0.0
Epoch 2918: train_loss 0.021983152 val_loss 0.026382534
ttest: -91.33715470453508 pValue 0.0
Epoch 2919: train_loss 0.021981265 val_loss 0.02638419
ttest: -91.02531312509292 pValue 0.0
Epoch 2920: train_loss 0.021979382 val_loss 0.026385851
ttest: -90.7122050268756 pValue 0.0
Epoch 2921: train_loss 0.0219775 val_loss 0.026387518
ttest: -90.39827598792665 pValue 0.0
Epoch 2922: train_loss 0.021975618 val_loss 0.026389185
ttest: -90.08175739900877 pValue 0.0
Epoch 2923: train_loss 0.02197374 val_loss 0.026390862
ttest: -89.76407950178245 pValue 0.0
Epoch 2924: train_loss 0.021971863 val_loss 0.026392527
ttest: -89.44439571264887 pValue 0.0
Epoch 2925: train_loss 0.021969985 val_loss 0.026394213
ttest: -89.1231539237377 pValue 0.0
Epoch 2926: train_loss 0.021968111 val_loss 0.026395883
ttest: -88.80135640469265 pValue 0.0
Epoch 2927: train_loss 0.021966241 val_loss 0.02639758
ttest: -88.477235201094 pValue 0.0
Epoch 2928: train_loss 0.02196437 val_loss 0.026399272
ttest: -88.1530552234602 pValue 0.0
Epoch 2929: train_loss 0.021962501 val_loss 0.026400974
ttest: -87.8275084343549 pValue 0.0
Epoch 2930: train_loss 0.021960633 val_loss 0.026402673
ttest: -87.49932109449739 pValue 0.0
Epoch 2931: train_loss 0.021958768 val_loss 0.026404373
ttest: -87.17077822934345 pValue 0.0
Epoch 2932: train_loss 0.021956906 val_loss 0.026406066
ttest: -86.84060433646907 pValue 0.0
Epoch 2933: train_loss 0.021955043 val_loss 0.026407791
ttest: -86.50922120796271 pValue 0.0
Epoch 2934: train_loss 0.021953182 val_loss 0.026409484
ttest: -86.17760053724236 pValue 0.0
Epoch 2935: train_loss 0.021951323 val_loss 0.02641122
ttest: -85.84531072229353 pValue 0.0
Epoch 2936: train_loss 0.021949466 val_loss 0.026412925
ttest: -85.5106764400179 pValue 0.0
Epoch 2937: train_loss 0.021947611 val_loss 0.026414653
ttest: -85.17503150672363 pValue 0.0
Epoch 2938: train_loss 0.02194576 val_loss 0.026416378
ttest: -84.84060076755979 pValue 0.0
Epoch 2939: train_loss 0.021943908 val_loss 0.026418108
ttest: -84.5026347946675 pValue 0.0
Epoch 2940: train_loss 0.021942057 val_loss 0.026419854
ttest: -84.16548559982212 pValue 0.0
Epoch 2941: train_loss 0.021940213 val_loss 0.026421582
ttest: -83.82664899875556 pValue 0.0
Epoch 2942: train_loss 0.021938363 val_loss 0.026423331
ttest: -83.48745472846797 pValue 0.0
Epoch 2943: train_loss 0.021936521 val_loss 0.02642507
ttest: -83.148341511965 pValue 0.0
Epoch 2944: train_loss 0.021934677 val_loss 0.026426813
ttest: -82.80721347893724 pValue 0.0
Epoch 2945: train_loss 0.021932837 val_loss 0.02642855
ttest: -82.4654131077313 pValue 0.0
Epoch 2946: train_loss 0.021930996 val_loss 0.02643032
ttest: -82.12379984515107 pValue 0.0
Epoch 2947: train_loss 0.02192916 val_loss 0.02643206
ttest: -81.78161554929278 pValue 0.0
Epoch 2948: train_loss 0.021927323 val_loss 0.026433812
ttest: -81.43843792879724 pValue 0.0
Epoch 2949: train_loss 0.021925487 val_loss 0.02643557
ttest: -81.09513302293539 pValue 0.0
Epoch 2950: train_loss 0.021923658 val_loss 0.026437325
ttest: -80.75091820504662 pValue 0.0
Epoch 2951: train_loss 0.021921825 val_loss 0.026439102
ttest: -80.40749804084575 pValue 0.0
Epoch 2952: train_loss 0.021919996 val_loss 0.02644087
ttest: -80.06238718333627 pValue 0.0
Epoch 2953: train_loss 0.02191817 val_loss 0.02644265
ttest: -79.71769632559275 pValue 0.0
Epoch 2954: train_loss 0.021916345 val_loss 0.026444439
ttest: -79.3722262892506 pValue 0.0
Epoch 2955: train_loss 0.021914521 val_loss 0.026446229
ttest: -79.02723571820124 pValue 0.0
Epoch 2956: train_loss 0.021912701 val_loss 0.026448013
ttest: -78.68070894484048 pValue 0.0
Epoch 2957: train_loss 0.02191088 val_loss 0.026449813
ttest: -78.33392073537998 pValue 0.0
Epoch 2958: train_loss 0.021909062 val_loss 0.026451597
ttest: -77.98852126312025 pValue 0.0
Epoch 2959: train_loss 0.021907248 val_loss 0.026453413
ttest: -77.64127597558382 pValue 0.0
Epoch 2960: train_loss 0.021905433 val_loss 0.026455203
ttest: -77.2954908233486 pValue 0.0
Epoch 2961: train_loss 0.02190362 val_loss 0.02645701
ttest: -76.9479451512241 pValue 0.0
Epoch 2962: train_loss 0.021901809 val_loss 0.026458807
ttest: -76.60151813167577 pValue 0.0
Epoch 2963: train_loss 0.021899998 val_loss 0.026460608
ttest: -76.25459014197482 pValue 0.0
Epoch 2964: train_loss 0.021898195 val_loss 0.026462397
ttest: -75.90803960423189 pValue 0.0
Epoch 2965: train_loss 0.021896388 val_loss 0.026464213
ttest: -75.56067068752509 pValue 0.0
Epoch 2966: train_loss 0.021894582 val_loss 0.026466012
ttest: -75.21450393226863 pValue 0.0
Epoch 2967: train_loss 0.021892782 val_loss 0.026467837
ttest: -74.86716782711697 pValue 0.0
Epoch 2968: train_loss 0.02189098 val_loss 0.026469653
ttest: -74.52067788235227 pValue 0.0
Epoch 2969: train_loss 0.021889182 val_loss 0.026471464
ttest: -74.17389098419153 pValue 0.0
Epoch 2970: train_loss 0.021887384 val_loss 0.02647329
ttest: -73.82839907168108 pValue 0.0
Epoch 2971: train_loss 0.021885589 val_loss 0.026475104
ttest: -73.48149529579223 pValue 0.0
Epoch 2972: train_loss 0.021883797 val_loss 0.02647696
ttest: -73.13550919607964 pValue 0.0
Epoch 2973: train_loss 0.021882005 val_loss 0.026478777
ttest: -72.78970378400889 pValue 0.0
Epoch 2974: train_loss 0.021880213 val_loss 0.026480641
ttest: -72.44447868985432 pValue 0.0
Epoch 2975: train_loss 0.021878425 val_loss 0.026482461
ttest: -72.09912454853347 pValue 0.0
Epoch 2976: train_loss 0.02187664 val_loss 0.026484327
ttest: -71.75399277458772 pValue 0.0
Epoch 2977: train_loss 0.021874856 val_loss 0.026486177
ttest: -71.40952350249984 pValue 0.0
Epoch 2978: train_loss 0.021873072 val_loss 0.026488034
ttest: -71.06495817566001 pValue 0.0
Epoch 2979: train_loss 0.021871291 val_loss 0.026489899
ttest: -70.72108149750288 pValue 0.0
Epoch 2980: train_loss 0.021869512 val_loss 0.026491757
ttest: -70.37716446406813 pValue 0.0
Epoch 2981: train_loss 0.021867733 val_loss 0.026493635
ttest: -70.0347328192272 pValue 0.0
Epoch 2982: train_loss 0.021865956 val_loss 0.026495507
ttest: -69.69152961503985 pValue 0.0
Epoch 2983: train_loss 0.021864183 val_loss 0.026497375
ttest: -69.34909805689969 pValue 0.0
Epoch 2984: train_loss 0.021862412 val_loss 0.02649925
ttest: -69.00707659697383 pValue 0.0
Epoch 2985: train_loss 0.02186064 val_loss 0.02650113
ttest: -68.66549563127566 pValue 0.0
Epoch 2986: train_loss 0.021858871 val_loss 0.026503017
ttest: -68.32434936038086 pValue 0.0
Epoch 2987: train_loss 0.021857105 val_loss 0.026504906
ttest: -67.98366744051155 pValue 0.0
Epoch 2988: train_loss 0.02185534 val_loss 0.026506806
ttest: -67.64383322219051 pValue 0.0
Epoch 2989: train_loss 0.021853577 val_loss 0.02650868
ttest: -67.30451060716827 pValue 0.0
Epoch 2990: train_loss 0.021851813 val_loss 0.026510598
ttest: -66.96457394205713 pValue 0.0
Epoch 2991: train_loss 0.021850053 val_loss 0.026512474
ttest: -66.62666924515773 pValue 0.0
Epoch 2992: train_loss 0.021848295 val_loss 0.026514404
ttest: -66.28854716815901 pValue 0.0
Epoch 2993: train_loss 0.02184654 val_loss 0.02651628
ttest: -65.9517447382003 pValue 0.0
Epoch 2994: train_loss 0.021844786 val_loss 0.026518224
ttest: -65.61512341410247 pValue 0.0
Epoch 2995: train_loss 0.021843031 val_loss 0.026520105
ttest: -65.27912664512553 pValue 0.0
Epoch 2996: train_loss 0.02184128 val_loss 0.026522037
ttest: -64.94298898081642 pValue 0.0
Epoch 2997: train_loss 0.02183953 val_loss 0.026523953
ttest: -64.60819679417258 pValue 0.0
Epoch 2998: train_loss 0.021837782 val_loss 0.02652587
ttest: -64.27403231204033 pValue 0.0
Epoch 2999: train_loss 0.021836035 val_loss 0.026527803
ttest: -63.94087096493466 pValue 0.0
Epoch 3000: train_loss 0.021834292 val_loss 0.026529726
ttest: -63.60836563645468 pValue 0.0
Epoch 3001: train_loss 0.021832552 val_loss 0.026531657
ttest: -63.27652388109667 pValue 0.0
Epoch 3002: train_loss 0.021830808 val_loss 0.026533578
ttest: -62.9450181271755 pValue 0.0
Epoch 3003: train_loss 0.02182907 val_loss 0.02653552
ttest: -62.61350174442982 pValue 0.0
Epoch 3004: train_loss 0.021827335 val_loss 0.026537446
ttest: -62.28445900244945 pValue 0.0
Epoch 3005: train_loss 0.021825597 val_loss 0.026539402
ttest: -61.95541027804559 pValue 0.0
Epoch 3006: train_loss 0.021823864 val_loss 0.026541336
ttest: -61.62709753116064 pValue 0.0
Epoch 3007: train_loss 0.021822132 val_loss 0.026543284
ttest: -61.29847809219769 pValue 0.0
Epoch 3008: train_loss 0.021820404 val_loss 0.026545223
ttest: -60.9720075070026 pValue 0.0
Epoch 3009: train_loss 0.021818675 val_loss 0.026547167
ttest: -60.64629633412203 pValue 0.0
Epoch 3010: train_loss 0.021816947 val_loss 0.026549127
ttest: -60.32099924422392 pValue 0.0
Epoch 3011: train_loss 0.021815222 val_loss 0.026551085
ttest: -59.99647419966 pValue 0.0
Epoch 3012: train_loss 0.0218135 val_loss 0.026553035
ttest: -59.672738973091256 pValue 0.0
Epoch 3013: train_loss 0.021811778 val_loss 0.026555017
ttest: -59.349787736558646 pValue 0.0
Epoch 3014: train_loss 0.021810062 val_loss 0.026556954
ttest: -59.027652376679185 pValue 0.0
Epoch 3015: train_loss 0.021808341 val_loss 0.02655895
ttest: -58.70630243522988 pValue 0.0
Epoch 3016: train_loss 0.021806628 val_loss 0.026560895
ttest: -58.38612958003063 pValue 0.0
Epoch 3017: train_loss 0.021804914 val_loss 0.026562873
ttest: -58.06643969471344 pValue 0.0
Epoch 3018: train_loss 0.0218032 val_loss 0.026564833
ttest: -57.74758758588253 pValue 0.0
Epoch 3019: train_loss 0.02180149 val_loss 0.026566796
ttest: -57.42992273718956 pValue 0.0
Epoch 3020: train_loss 0.021799779 val_loss 0.026568798
ttest: -57.11308069771541 pValue 0.0
Epoch 3021: train_loss 0.021798074 val_loss 0.026570762
ttest: -56.79743196730188 pValue 0.0
Epoch 3022: train_loss 0.021796368 val_loss 0.026572775
ttest: -56.48195180102245 pValue 0.0
Epoch 3023: train_loss 0.021794667 val_loss 0.026574727
ttest: -56.16769180784063 pValue 0.0
Epoch 3024: train_loss 0.021792963 val_loss 0.026576744
ttest: -55.85427838678578 pValue 0.0
Epoch 3025: train_loss 0.021791264 val_loss 0.026578696
ttest: -55.541440016089346 pValue 0.0
Epoch 3026: train_loss 0.021789566 val_loss 0.026580723
ttest: -55.23043687548329 pValue 0.0
Epoch 3027: train_loss 0.02178787 val_loss 0.026582684
ttest: -54.91968466968196 pValue 0.0
Epoch 3028: train_loss 0.021786176 val_loss 0.026584713
ttest: -54.60980275431934 pValue 0.0
Epoch 3029: train_loss 0.021784484 val_loss 0.026586683
ttest: -54.3011475309001 pValue 0.0
Epoch 3030: train_loss 0.021782791 val_loss 0.026588695
ttest: -53.99338585864048 pValue 0.0
Epoch 3031: train_loss 0.021781102 val_loss 0.026590694
ttest: -53.686524839099626 pValue 0.0
Epoch 3032: train_loss 0.021779414 val_loss 0.02659271
ttest: -53.37992084696177 pValue 0.0
Epoch 3033: train_loss 0.02177773 val_loss 0.026594715
ttest: -53.07486950433809 pValue 0.0
Epoch 3034: train_loss 0.021776047 val_loss 0.026596734
ttest: -52.77040380964969 pValue 0.0
Epoch 3035: train_loss 0.021774366 val_loss 0.026598748
ttest: -52.46748839147524 pValue 0.0
Epoch 3036: train_loss 0.021772686 val_loss 0.02660076
ttest: -52.165165347018615 pValue 0.0
Epoch 3037: train_loss 0.021771006 val_loss 0.026602805
ttest: -51.863430591473204 pValue 0.0
Epoch 3038: train_loss 0.021769332 val_loss 0.026604796
ttest: -51.563265595167394 pValue 0.0
Epoch 3039: train_loss 0.021767657 val_loss 0.02660682
ttest: -51.263709703549075 pValue 0.0
Epoch 3040: train_loss 0.021765985 val_loss 0.026608825
ttest: -50.965403108071456 pValue 0.0
Epoch 3041: train_loss 0.021764314 val_loss 0.02661085
ttest: -50.668022525602865 pValue 0.0
Epoch 3042: train_loss 0.021762647 val_loss 0.026612863
ttest: -50.37096056619769 pValue 0.0
Epoch 3043: train_loss 0.021760978 val_loss 0.026614869
ttest: -50.075465058105614 pValue 0.0
Epoch 3044: train_loss 0.021759313 val_loss 0.026616886
ttest: -49.78060587651901 pValue 0.0
Epoch 3045: train_loss 0.021757651 val_loss 0.026618911
ttest: -49.48698459230236 pValue 0.0
Epoch 3046: train_loss 0.02175599 val_loss 0.026620915
ttest: -49.194622739699994 pValue 0.0
Epoch 3047: train_loss 0.021754328 val_loss 0.026622957
ttest: -48.902588294627776 pValue 0.0
Epoch 3048: train_loss 0.02175267 val_loss 0.026624981
ttest: -48.61179822082986 pValue 0.0
Epoch 3049: train_loss 0.021751016 val_loss 0.02662699
ttest: -48.32257368830054 pValue 0.0
Epoch 3050: train_loss 0.021749359 val_loss 0.02662904
ttest: -48.03307604140386 pValue 0.0
Epoch 3051: train_loss 0.021747708 val_loss 0.026631052
ttest: -47.74572898870441 pValue 0.0
Epoch 3052: train_loss 0.021746054 val_loss 0.026633112
ttest: -47.458716036502054 pValue 0.0
Epoch 3053: train_loss 0.021744408 val_loss 0.026635135
ttest: -47.17266129238576 pValue 0.0
Epoch 3054: train_loss 0.021742757 val_loss 0.026637195
ttest: -46.88753659192533 pValue 2.17e-322
Epoch 3055: train_loss 0.021741115 val_loss 0.026639214
ttest: -46.60336495938016 pValue 1.3839e-319
Epoch 3056: train_loss 0.021739468 val_loss 0.026641281
ttest: -46.32071969008554 pValue 8.4597546e-317
Epoch 3057: train_loss 0.021737829 val_loss 0.026643304
ttest: -46.03872985538389 pValue 5.0944257445e-314
Epoch 3058: train_loss 0.021736186 val_loss 0.026645392
ttest: -45.75766156204337 pValue 3.0036612759006e-311
Epoch 3059: train_loss 0.021734549 val_loss 0.026647415
ttest: -45.47754826671119 pValue 1.7319186091781825e-308
Epoch 3060: train_loss 0.021732911 val_loss 0.026649484
ttest: -45.19836890498854 pValue 9.768325069313718e-306
Epoch 3061: train_loss 0.021731276 val_loss 0.026651518
ttest: -44.92013895684665 pValue 5.385453105007436e-303
Epoch 3062: train_loss 0.021729644 val_loss 0.026653616
ttest: -44.64282540697722 pValue 2.9036549073213063e-300
Epoch 3063: train_loss 0.021728015 val_loss 0.026655644
ttest: -44.36675189893816 pValue 1.5195739589330642e-297
Epoch 3064: train_loss 0.021726385 val_loss 0.02665774
ttest: -44.091306076968436 pValue 7.822128764997958e-295
Epoch 3065: train_loss 0.021724757 val_loss 0.026659755
ttest: -43.81710705270238 pValue 3.9052509853440773e-292
Epoch 3066: train_loss 0.02172313 val_loss 0.02666186
ttest: -43.543815849788125 pValue 1.9048694375981467e-289
Epoch 3067: train_loss 0.021721506 val_loss 0.02666391
ttest: -43.27118263706715 pValue 9.124525205381426e-287
Epoch 3068: train_loss 0.021719886 val_loss 0.026665986
ttest: -43.00004330410126 pValue 4.212811635542971e-284
Epoch 3069: train_loss 0.021718264 val_loss 0.026668059
ttest: -42.72982311137722 pValue 1.8981424408125246e-281
Epoch 3070: train_loss 0.021716645 val_loss 0.026670134
ttest: -42.45968666997844 pValue 8.498380981004937e-279
Epoch 3071: train_loss 0.02171503 val_loss 0.026672209
ttest: -42.1916007605274 pValue 3.6199754533103184e-276
Epoch 3072: train_loss 0.021713413 val_loss 0.026674269
ttest: -41.923876126333035 pValue 1.5220175663133253e-273
Epoch 3073: train_loss 0.0217118 val_loss 0.026676372
ttest: -41.657616690635564 pValue 6.164122616659012e-271
Epoch 3074: train_loss 0.021710187 val_loss 0.026678426
ttest: -41.39173247997283 pValue 2.4621380114908074e-268
Epoch 3075: train_loss 0.02170858 val_loss 0.026680533
ttest: -41.12675634076018 pValue 9.584494193674594e-266
Epoch 3076: train_loss 0.021706972 val_loss 0.026682599
ttest: -40.86297280537549 pValue 3.6125157089017356e-263
Epoch 3077: train_loss 0.021705365 val_loss 0.02668469
ttest: -40.60009788863401 pValue 1.3262998995999554e-260
Epoch 3078: train_loss 0.021703761 val_loss 0.026686758
ttest: -40.338141405921476 pValue 4.740882145060902e-258
Epoch 3079: train_loss 0.021702157 val_loss 0.026688866
ttest: -40.07708037960963 pValue 1.650387807529318e-255
Epoch 3080: train_loss 0.021700557 val_loss 0.02669093
ttest: -39.81720713526071 pValue 5.558339851488925e-253
Epoch 3081: train_loss 0.02169896 val_loss 0.026693044
ttest: -39.557954734245435 pValue 1.8332158740028244e-250
Epoch 3082: train_loss 0.021697361 val_loss 0.026695114
ttest: -39.29960746713961 pValue 5.8827579035856726e-248
Epoch 3083: train_loss 0.021695767 val_loss 0.026697204
ttest: -39.04242795442435 pValue 1.825948155554357e-245
Epoch 3084: train_loss 0.021694172 val_loss 0.026699314
ttest: -38.78613903952465 pValue 5.514098532694112e-243
Epoch 3085: train_loss 0.021692581 val_loss 0.02670139
ttest: -38.53101463962076 pValue 1.610075292292094e-240
Epoch 3086: train_loss 0.02169099 val_loss 0.026703505
ttest: -38.27597143755226 pValue 4.652750758005341e-238
Epoch 3087: train_loss 0.021689404 val_loss 0.026705582
ttest: -38.0228839105085 pValue 1.2774827797790407e-235
Epoch 3088: train_loss 0.021687817 val_loss 0.026707705
ttest: -37.76987394461126 pValue 3.4700814931060884e-233
Epoch 3089: train_loss 0.021686234 val_loss 0.02670979
ttest: -37.518020917997376 pValue 9.108381370923285e-231
Epoch 3090: train_loss 0.02168465 val_loss 0.0267119
ttest: -37.26756876939937 pValue 2.2977872806687935e-228
Epoch 3091: train_loss 0.02168307 val_loss 0.026714016
ttest: -37.017719154925196 pValue 5.6667514461597726e-226
Epoch 3092: train_loss 0.021681491 val_loss 0.0267161
ttest: -36.76848164806507 pValue 1.3655785431242233e-223
Epoch 3093: train_loss 0.021679914 val_loss 0.026718207
ttest: -36.5206400695095 pValue 3.1614886030816577e-221
Epoch 3094: train_loss 0.02167834 val_loss 0.026720315
ttest: -36.27287770549427 pValue 7.231432715604304e-219
Epoch 3095: train_loss 0.021676766 val_loss 0.026722407
ttest: -36.02676722624746 pValue 1.579884839773831e-216
Epoch 3096: train_loss 0.021675196 val_loss 0.026724497
ttest: -35.781778874629296 pValue 3.333960210739373e-214
Epoch 3097: train_loss 0.021673625 val_loss 0.026726594
ttest: -35.536615545186024 pValue 6.984505497256701e-212
Epoch 3098: train_loss 0.021672057 val_loss 0.026728684
ttest: -35.29308542053694 pValue 1.3975585885397492e-209
Epoch 3099: train_loss 0.02167049 val_loss 0.02673077
ttest: -35.05041222162967 pValue 2.7150212756621525e-207
Epoch 3100: train_loss 0.021668926 val_loss 0.026732847
ttest: -34.808342196412354 pValue 5.147525482421946e-205
Epoch 3101: train_loss 0.021667361 val_loss 0.026734954
ttest: -34.56762617124954 pValue 9.373546396122174e-203
Epoch 3102: train_loss 0.021665804 val_loss 0.026737042
ttest: -34.32750139381866 pValue 1.6657588040321664e-200
Epoch 3103: train_loss 0.021664245 val_loss 0.026739145
ttest: -34.08821562410645 pValue 2.8733624728743743e-198
Epoch 3104: train_loss 0.021662686 val_loss 0.026741246
ttest: -33.85001861705093 pValue 4.785281567007968e-196
Epoch 3105: train_loss 0.021661133 val_loss 0.026743338
ttest: -33.61265674931627 pValue 7.734092730024258e-194
Epoch 3106: train_loss 0.02165958 val_loss 0.026745474
ttest: -33.375859425937726 pValue 1.2198591964743712e-191
Epoch 3107: train_loss 0.021658028 val_loss 0.026747549
ttest: -33.14039933534078 pValue 1.8472433890887124e-189
Epoch 3108: train_loss 0.021656476 val_loss 0.026749704
ttest: -32.90573961436245 pValue 2.715771918695376e-187
Epoch 3109: train_loss 0.021654928 val_loss 0.026751792
ttest: -32.67140993114937 pValue 3.913606812451659e-185
Epoch 3110: train_loss 0.021653382 val_loss 0.026753912
ttest: -32.4383843371849 pValue 5.416955869382946e-183
Epoch 3111: train_loss 0.021651838 val_loss 0.026756018
ttest: -32.20592420785275 pValue 7.311573702675092e-181
Epoch 3112: train_loss 0.021650296 val_loss 0.02675814
ttest: -31.974513639506988 pValue 9.527102472026872e-179
Epoch 3113: train_loss 0.021648753 val_loss 0.026760235
ttest: -31.74366084052267 pValue 1.2104398008276787e-176
Epoch 3114: train_loss 0.021647215 val_loss 0.02676237
ttest: -31.513846625865003 pValue 1.484660076077467e-174
Epoch 3115: train_loss 0.021645676 val_loss 0.02676447
ttest: -31.285077504302148 pValue 1.7576060679915502e-172
Epoch 3116: train_loss 0.02164414 val_loss 0.02676658
ttest: -31.0570992664864 pValue 2.018569015286134e-170
Epoch 3117: train_loss 0.021642607 val_loss 0.026768701
ttest: -30.82990424644903 pValue 2.249136390717642e-168
Epoch 3118: train_loss 0.021641076 val_loss 0.026770795
ttest: -30.603496411208432 pValue 2.430811745165121e-166
Epoch 3119: train_loss 0.021639545 val_loss 0.026772924
ttest: -30.377383842873805 pValue 2.5735353301256395e-164
Epoch 3120: train_loss 0.021638013 val_loss 0.026775045
ttest: -30.152766823509314 pValue 2.604830544916373e-162
Epoch 3121: train_loss 0.021636488 val_loss 0.026777172
ttest: -29.928678705815177 pValue 2.5700187408997763e-160
Epoch 3122: train_loss 0.021634962 val_loss 0.026779285
ttest: -29.705360429886973 pValue 2.4595084594551848e-158
Epoch 3123: train_loss 0.021633439 val_loss 0.02678142
ttest: -29.483039588750316 pValue 2.2725362285992563e-156
Epoch 3124: train_loss 0.021631919 val_loss 0.026783537
ttest: -29.261479187877 pValue 2.036766434576183e-154
Epoch 3125: train_loss 0.021630397 val_loss 0.026785694
ttest: -29.040427654462025 pValue 1.7794298461594135e-152
Epoch 3126: train_loss 0.021628879 val_loss 0.026787825
ttest: -28.821072801345156 pValue 1.480049999878073e-150
Epoch 3127: train_loss 0.021627363 val_loss 0.026789965
ttest: -28.60151472728486 pValue 1.2168001660062412e-148
Epoch 3128: train_loss 0.021625847 val_loss 0.026792107
ttest: -28.383402493687203 pValue 9.570584521888788e-147
Epoch 3129: train_loss 0.021624334 val_loss 0.026794259
ttest: -28.165086351635125 pValue 7.438365597238389e-145
Epoch 3130: train_loss 0.021622825 val_loss 0.026796408
ttest: -27.94843759835315 pValue 5.506586307645289e-143
Epoch 3131: train_loss 0.021621313 val_loss 0.026798548
ttest: -27.732285387657065 pValue 3.9726541658290766e-141
Epoch 3132: train_loss 0.021619804 val_loss 0.026800694
ttest: -27.516626272680796 pValue 2.792833607584844e-139
Epoch 3133: train_loss 0.0216183 val_loss 0.026802842
ttest: -27.30191733762041 pValue 1.8962583225081342e-137
Epoch 3134: train_loss 0.021616796 val_loss 0.026804976
ttest: -27.088159777691157 pValue 1.2434311932890163e-135
Epoch 3135: train_loss 0.021615293 val_loss 0.026807133
ttest: -26.87557371146208 pValue 7.841188445517804e-134
Epoch 3136: train_loss 0.021613793 val_loss 0.026809283
ttest: -26.66300727984792 pValue 4.861128957410187e-132
Epoch 3137: train_loss 0.021612294 val_loss 0.026811391
ttest: -26.451384965745937 pValue 2.910430317212181e-130
Epoch 3138: train_loss 0.021610796 val_loss 0.026813557
ttest: -26.24091436693751 pValue 1.6762600821868438e-128
Epoch 3139: train_loss 0.0216093 val_loss 0.026815696
ttest: -26.0304623094789 pValue 9.488235437872392e-127
Epoch 3140: train_loss 0.021607809 val_loss 0.026817821
ttest: -25.821391033329714 pValue 5.143851072736867e-125
Epoch 3141: train_loss 0.021606315 val_loss 0.02681996
ttest: -25.612332315594863 pValue 2.7403589900898478e-123
Epoch 3142: train_loss 0.021604825 val_loss 0.026822127
ttest: -25.404635001236155 pValue 1.3987718905231792e-121
Epoch 3143: train_loss 0.021603338 val_loss 0.026824225
ttest: -25.19785722925547 pValue 6.897712706884494e-120
Epoch 3144: train_loss 0.02160185 val_loss 0.026826417
ttest: -24.991074722271488 pValue 3.342794540987433e-118
Epoch 3145: train_loss 0.021600366 val_loss 0.026828533
ttest: -24.785421334008994 pValue 1.5588516787181255e-116
Epoch 3146: train_loss 0.021598881 val_loss 0.026830679
ttest: -24.580217346699943 pValue 7.083165525316135e-115
Epoch 3147: train_loss 0.0215974 val_loss 0.02683284
ttest: -24.37634660725346 pValue 3.085613556384374e-113
Epoch 3148: train_loss 0.021595921 val_loss 0.026834968
ttest: -24.172698947938077 pValue 1.315059617428808e-111
Epoch 3149: train_loss 0.021594442 val_loss 0.026837124
ttest: -23.969712253599837 pValue 5.439290029517177e-110
Epoch 3150: train_loss 0.021592967 val_loss 0.026839286
ttest: -23.767600924363496 pValue 2.174923784515739e-108
Epoch 3151: train_loss 0.021591492 val_loss 0.026841417
ttest: -23.56636619485418 pValue 8.407081518033923e-107
Epoch 3152: train_loss 0.02159002 val_loss 0.026843563
ttest: -23.365558793036836 pValue 3.1668800506200694e-105
Epoch 3153: train_loss 0.021588549 val_loss 0.026845716
ttest: -23.165616376562415 pValue 1.1534448831610513e-103
Epoch 3154: train_loss 0.02158708 val_loss 0.026847845
ttest: -22.966099678362774 pValue 4.093691496423358e-102
Epoch 3155: train_loss 0.021585613 val_loss 0.026850024
ttest: -22.76743320476474 pValue 1.4051240414863953e-100
Epoch 3156: train_loss 0.021584146 val_loss 0.026852157
ttest: -22.56962034790047 pValue 4.664270484653011e-99
Epoch 3157: train_loss 0.02158268 val_loss 0.026854295
ttest: -22.372009460926414 pValue 1.5144300164482302e-97
Epoch 3158: train_loss 0.021581218 val_loss 0.026856434
ttest: -22.175886928583914 pValue 4.703232232206903e-96
Epoch 3159: train_loss 0.021579754 val_loss 0.026858592
ttest: -21.979739437332434 pValue 1.434219607572059e-94
Epoch 3160: train_loss 0.021578299 val_loss 0.026860738
ttest: -21.784427094638588 pValue 4.230919152405638e-93
Epoch 3161: train_loss 0.021576837 val_loss 0.026862903
ttest: -21.589940873565613 pValue 1.2076370983486432e-91
Epoch 3162: train_loss 0.021575384 val_loss 0.026865063
ttest: -21.395851864127074 pValue 3.359601261833565e-90
Epoch 3163: train_loss 0.021573927 val_loss 0.02686722
ttest: -21.202581613715196 pValue 9.044307989018289e-89
Epoch 3164: train_loss 0.021572474 val_loss 0.026869368
ttest: -21.009491238432574 pValue 2.3815485333341587e-87
Epoch 3165: train_loss 0.021571025 val_loss 0.026871566
ttest: -20.81805305849899 pValue 5.984753759748424e-86
Epoch 3166: train_loss 0.021569576 val_loss 0.02687372
ttest: -20.62678650411625 pValue 1.4712748391781331e-84
Epoch 3167: train_loss 0.021568127 val_loss 0.026875906
ttest: -20.43589659414897 pValue 3.525979394222051e-83
Epoch 3168: train_loss 0.021566682 val_loss 0.026878105
ttest: -20.24579874234809 pValue 8.18132501053835e-82
Epoch 3169: train_loss 0.02156524 val_loss 0.02688028
ttest: -20.05586598870616 pValue 1.8568201917804867e-80
Epoch 3170: train_loss 0.021563794 val_loss 0.026882445
ttest: -19.867348820218574 pValue 4.0391278281124505e-79
Epoch 3171: train_loss 0.021562353 val_loss 0.02688465
ttest: -19.67940092207207 pValue 8.538530361064256e-78
Epoch 3172: train_loss 0.021560915 val_loss 0.026886815
ttest: -19.491610623222602 pValue 1.7656423449913988e-76
Epoch 3173: train_loss 0.021559477 val_loss 0.026888978
ttest: -19.304804535786264 pValue 3.5245107153297307e-75
Epoch 3174: train_loss 0.021558043 val_loss 0.026891168
ttest: -19.11856091496662 pValue 6.837696181072126e-74
Epoch 3175: train_loss 0.021556608 val_loss 0.026893336
ttest: -18.93246672320282 pValue 1.2975950016388585e-72
Epoch 3176: train_loss 0.021555176 val_loss 0.026895523
ttest: -18.74754538173253 pValue 2.3704029525484993e-71
Epoch 3177: train_loss 0.021553744 val_loss 0.026897674
ttest: -18.562976367855647 pValue 4.2222290810258005e-70
Epoch 3178: train_loss 0.021552315 val_loss 0.026899843
ttest: -18.379163971859096 pValue 7.28749509819361e-69
Epoch 3179: train_loss 0.021550886 val_loss 0.026902046
ttest: -18.19568996961672 pValue 1.226670385222125e-67
Epoch 3180: train_loss 0.021549463 val_loss 0.026904194
ttest: -18.012966982491253 pValue 2.001017051826207e-66
Epoch 3181: train_loss 0.021548038 val_loss 0.026906388
ttest: -17.83118614573902 pValue 3.15450290059689e-65
Epoch 3182: train_loss 0.021546617 val_loss 0.026908517
ttest: -17.64974599068593 pValue 4.84974140060636e-64
Epoch 3183: train_loss 0.021545194 val_loss 0.026910733
ttest: -17.4688322137561 pValue 7.251020279556559e-63
Epoch 3184: train_loss 0.021543778 val_loss 0.026912855
ttest: -17.28805385565714 pValue 1.0603912402308505e-61
Epoch 3185: train_loss 0.02154236 val_loss 0.026915068
ttest: -17.108599537931713 pValue 1.490523769809893e-60
Epoch 3186: train_loss 0.021540944 val_loss 0.02691719
ttest: -16.929074987633577 pValue 2.0554128520689967e-59
Epoch 3187: train_loss 0.021539528 val_loss 0.026919408
ttest: -16.75066245096845 pValue 2.7333253817655914e-58
Epoch 3188: train_loss 0.02153812 val_loss 0.02692156
ttest: -16.572569412328786 pValue 3.5457717105727035e-57
Epoch 3189: train_loss 0.021536708 val_loss 0.026923748
ttest: -16.39498644754673 pValue 4.474708515328116e-56
Epoch 3190: train_loss 0.0215353 val_loss 0.026925901
ttest: -16.218312215287547 pValue 5.462945533144591e-55
Epoch 3191: train_loss 0.021533892 val_loss 0.02692808
ttest: -16.041944082590096 pValue 6.50738815935097e-54
Epoch 3192: train_loss 0.021532487 val_loss 0.026930237
ttest: -15.866278224664406 pValue 7.521500597351347e-53
Epoch 3193: train_loss 0.021531083 val_loss 0.026932428
ttest: -15.690911325200886 pValue 8.483143888600914e-52
Epoch 3194: train_loss 0.02152968 val_loss 0.026934588
ttest: -15.516239319860409 pValue 9.285498056239113e-51
Epoch 3195: train_loss 0.021528281 val_loss 0.026936755
ttest: -15.342449534331651 pValue 9.839928466577797e-50
Epoch 3196: train_loss 0.02152688 val_loss 0.026938945
ttest: -15.168950638532653 pValue 1.0176835171517694e-48
Epoch 3197: train_loss 0.021525484 val_loss 0.026941093
ttest: -14.995745296910354 pValue 1.0271411800183421e-47
Epoch 3198: train_loss 0.02152409 val_loss 0.02694329
ttest: -14.823407604376142 pValue 1.0040491516372915e-46
Epoch 3199: train_loss 0.021522693 val_loss 0.026945457
ttest: -14.651549206438053 pValue 9.555380073819686e-46
Epoch 3200: train_loss 0.021521302 val_loss 0.02694764
ttest: -14.479780200571033 pValue 8.898160664418279e-45
Epoch 3201: train_loss 0.021519914 val_loss 0.026949784
ttest: -14.309255622583155 pValue 7.988399632528009e-44
Epoch 3202: train_loss 0.021518523 val_loss 0.026952002
ttest: -14.138619300050038 pValue 7.0356702085663674e-43
Epoch 3203: train_loss 0.021517135 val_loss 0.026954135
ttest: -13.969219758545302 pValue 5.976152524351073e-42
Epoch 3204: train_loss 0.021515751 val_loss 0.026956353
ttest: -13.799894228555441 pValue 4.968283149270043e-41
Epoch 3205: train_loss 0.021514365 val_loss 0.026958525
ttest: -13.631026671445913 pValue 4.023107478366246e-40
Epoch 3206: train_loss 0.021512987 val_loss 0.026960688
ttest: -13.462807872836892 pValue 3.165861377266538e-39
Epoch 3207: train_loss 0.021511607 val_loss 0.026962912
ttest: -13.29503679852236 pValue 2.4271304203972934e-38
Epoch 3208: train_loss 0.021510229 val_loss 0.026965125
ttest: -13.127711307606662 pValue 1.8129617728608996e-37
Epoch 3209: train_loss 0.021508852 val_loss 0.026967295
ttest: -12.96120993517127 pValue 1.3135647712742974e-36
Epoch 3210: train_loss 0.02150748 val_loss 0.026969459
ttest: -12.79496380719918 pValue 9.294826814596046e-36
Epoch 3211: train_loss 0.021506105 val_loss 0.026971627
ttest: -12.629345368851098 pValue 6.395490872222476e-35
Epoch 3212: train_loss 0.021504734 val_loss 0.026973758
ttest: -12.463793620267381 pValue 4.307044591008378e-34
Epoch 3213: train_loss 0.021503367 val_loss 0.026975894
ttest: -12.29905255226003 pValue 2.814884646710206e-33
Epoch 3214: train_loss 0.021502 val_loss 0.026978012
ttest: -12.135118846373393 pValue 1.7857081379828837e-32
Epoch 3215: train_loss 0.021500632 val_loss 0.026980113
ttest: -11.971431806160702 pValue 1.1066215779857738e-31
Epoch 3216: train_loss 0.021499269 val_loss 0.026982239
ttest: -11.807985596637874 pValue 6.699497275856007e-31
Epoch 3217: train_loss 0.021497905 val_loss 0.026984323
ttest: -11.645153458927451 pValue 3.9461644503140224e-30
Epoch 3218: train_loss 0.021496542 val_loss 0.026986443
ttest: -11.482927453892533 pValue 2.2619589838657377e-29
Epoch 3219: train_loss 0.021495186 val_loss 0.026988557
ttest: -11.321119829434263 pValue 1.2643873156660326e-28
Epoch 3220: train_loss 0.021493826 val_loss 0.026990717
ttest: -11.160273787141518 pValue 6.853650394241867e-28
Epoch 3221: train_loss 0.02149247 val_loss 0.026992818
ttest: -10.999472724002565 pValue 3.637634139146107e-27
Epoch 3222: train_loss 0.021491114 val_loss 0.026994994
ttest: -10.839075714208704 pValue 1.8834102016022464e-26
Epoch 3223: train_loss 0.021489762 val_loss 0.026997115
ttest: -10.679083156186365 pValue 9.512998692458475e-26
Epoch 3224: train_loss 0.02148841 val_loss 0.026999293
ttest: -10.520034793557764 pValue 4.662655791780071e-25
Epoch 3225: train_loss 0.021487061 val_loss 0.027001439
ttest: -10.361200178386788 pValue 2.2341317059599944e-24
Epoch 3226: train_loss 0.021485712 val_loss 0.027003612
ttest: -10.202395458371727 pValue 1.048347980517121e-23
Epoch 3227: train_loss 0.021484364 val_loss 0.027005805
ttest: -10.044521562582599 pValue 4.7759094843034025e-23
Epoch 3228: train_loss 0.02148302 val_loss 0.027007941
ttest: -9.887215174884133 pValue 2.120100306066138e-22
Epoch 3229: train_loss 0.021481678 val_loss 0.02701014
ttest: -9.729929990828223 pValue 9.218461430803995e-22
Epoch 3230: train_loss 0.021480337 val_loss 0.027012285
ttest: -9.573385363762352 pValue 3.90003610853072e-21
Epoch 3231: train_loss 0.021478996 val_loss 0.027014464
ttest: -9.417217760243656 pValue 1.6109920274151174e-20
Epoch 3232: train_loss 0.021477658 val_loss 0.027016638
ttest: -9.26160215092708 pValue 6.487565990321943e-20
Epoch 3233: train_loss 0.021476317 val_loss 0.027018812
ttest: -9.106178994637894 pValue 2.555368644056052e-19
Epoch 3234: train_loss 0.021474984 val_loss 0.027020987
ttest: -8.951301992592237 pValue 9.814753328202268e-19
Epoch 3235: train_loss 0.02147365 val_loss 0.027023144
ttest: -8.796969000646502 pValue 3.676397548493781e-18
Epoch 3236: train_loss 0.021472318 val_loss 0.027025346
ttest: -8.642820051840928 pValue 1.3472380401533489e-17
Epoch 3237: train_loss 0.02147099 val_loss 0.027027514
ttest: -8.489383967858846 pValue 4.808971727347828e-17
Epoch 3238: train_loss 0.021469658 val_loss 0.027029697
ttest: -8.33630415109726 pValue 1.6772033761459425e-16
Epoch 3239: train_loss 0.021468332 val_loss 0.02703186
ttest: -8.183402978194573 pValue 5.723809246834881e-16
Epoch 3240: train_loss 0.021467004 val_loss 0.027034061
ttest: -8.031203006398327 pValue 1.903583826707001e-15
Epoch 3241: train_loss 0.021465681 val_loss 0.027036216
ttest: -7.879526498729014 pValue 6.179139004179011e-15
Epoch 3242: train_loss 0.021464359 val_loss 0.027038418
ttest: -7.727670917472565 pValue 1.9684197266653956e-14
Epoch 3243: train_loss 0.021463037 val_loss 0.027040586
ttest: -7.576855628401762 pValue 6.097429962420468e-14
Epoch 3244: train_loss 0.021461718 val_loss 0.027042788
ttest: -7.426031608269262 pValue 1.851346356584492e-13
Epoch 3245: train_loss 0.021460399 val_loss 0.027044915
ttest: -7.275721616664589 pValue 5.488884269294949e-13
Epoch 3246: train_loss 0.021459084 val_loss 0.02704714
ttest: -7.126262303946731 pValue 1.5855122033193102e-12
Epoch 3247: train_loss 0.021457769 val_loss 0.027049283
ttest: -6.9766182562968 pValue 4.495360764815787e-12
Epoch 3248: train_loss 0.021456454 val_loss 0.02705148
ttest: -6.8276473266992825 pValue 1.2436323064700915e-11
Epoch 3249: train_loss 0.021455146 val_loss 0.027053634
ttest: -6.679175296873069 pValue 3.361591923343329e-11
Epoch 3250: train_loss 0.021453833 val_loss 0.027055847
ttest: -6.530682567568942 pValue 8.909466807406927e-11
Epoch 3251: train_loss 0.021452527 val_loss 0.027058009
ttest: -6.383025373329708 pValue 2.3026081699880673e-10
Epoch 3252: train_loss 0.021451216 val_loss 0.027060201
ttest: -6.235344833309928 pValue 5.835744941544356e-10
Epoch 3253: train_loss 0.021449916 val_loss 0.027062368
ttest: -6.088491691976958 pValue 1.442792158442344e-09
Epoch 3254: train_loss 0.021448608 val_loss 0.027064549
ttest: -5.941781848924756 pValue 3.494923875401791e-09
Epoch 3255: train_loss 0.021447306 val_loss 0.027066752
ttest: -5.7957204334848145 pValue 8.270388740319991e-09
Epoch 3256: train_loss 0.021446006 val_loss 0.027068933
ttest: -5.649628362679687 pValue 1.9197447881706774e-08
Epoch 3257: train_loss 0.021444708 val_loss 0.02707112
ttest: -5.504348143551012 pValue 4.350445577476703e-08
Epoch 3258: train_loss 0.02144341 val_loss 0.027073275
ttest: -5.3590345461505935 pValue 9.67199318312288e-08
Epoch 3259: train_loss 0.021442115 val_loss 0.027075507
ttest: -5.214356093208254 pValue 2.10200194705234e-07
Epoch 3260: train_loss 0.02144082 val_loss 0.027077667
ttest: -5.069808505095497 pValue 4.478311047300039e-07
Epoch 3261: train_loss 0.021439526 val_loss 0.027079878
ttest: -4.925556138993339 pValue 9.345712002097174e-07
Epoch 3262: train_loss 0.021438235 val_loss 0.027082039
ttest: -4.7822653302951705 pValue 1.9043702585325985e-06
Epoch 3263: train_loss 0.021436946 val_loss 0.027084243
ttest: -4.638763388981541 pValue 3.8116945024576824e-06
Epoch 3264: train_loss 0.021435658 val_loss 0.027086405
ttest: -4.495883197024205 pValue 7.4643711478787235e-06
Epoch 3265: train_loss 0.021434372 val_loss 0.027088616
ttest: -4.3532872345376274 pValue 1.4326495527334196e-05
Epoch 3266: train_loss 0.021433085 val_loss 0.027090792
ttest: -4.211140257965411 pValue 2.6933437167021395e-05
Epoch 3267: train_loss 0.021431804 val_loss 0.027093003
ttest: -4.069437510639804 pValue 4.960595415561852e-05
Epoch 3268: train_loss 0.021430522 val_loss 0.02709517
ttest: -3.927683776704157 pValue 8.97073716253652e-05
Epoch 3269: train_loss 0.02142924 val_loss 0.02709735
ttest: -3.7866996528022825 pValue 0.00015876337255356018
Epoch 3270: train_loss 0.021427963 val_loss 0.02709956
ttest: -3.645495833111553 pValue 0.0002761166732140072
Epoch 3271: train_loss 0.021426685 val_loss 0.027101757
ttest: -3.505381840936292 pValue 0.00046957846719650805
Epoch 3272: train_loss 0.021425407 val_loss 0.027103933
ttest: -3.364882912794962 pValue 0.0007854064328823055
Epoch 3273: train_loss 0.021424135 val_loss 0.027106117
ttest: -3.2254657498787305 pValue 0.0012852617591119964
Epoch 3274: train_loss 0.021422861 val_loss 0.027108327
ttest: -3.08582323923165 pValue 0.0020677256002284896
Epoch 3275: train_loss 0.021421589 val_loss 0.027110511
ttest: -2.94660534172331 pValue 0.0032634238402505233
Epoch 3276: train_loss 0.021420319 val_loss 0.027112707
ttest: -2.807808685493927 pValue 0.005053951012820588
Epoch 3277: train_loss 0.021419054 val_loss 0.027114902
ttest: -2.6695919625435796 pValue 0.007678158185499029
Epoch 3278: train_loss 0.021417785 val_loss 0.02711709
ttest: -2.5314678992936597 pValue 0.011462439453451342
Epoch 3279: train_loss 0.021416519 val_loss 0.027119251
ttest: -2.3937572148140633 pValue 0.016802055368648844
Epoch 3280: train_loss 0.021415258 val_loss 0.02712147
ttest: -2.256135590490772 pValue 0.024209384781919682
Epoch 3281: train_loss 0.021413997 val_loss 0.027123634
ttest: -2.118922914934655 pValue 0.03426542197205822
Epoch 3282: train_loss 0.021412732 val_loss 0.027125891
ttest: -1.9821151029523298 pValue 0.047653803574888604
Epoch 3283: train_loss 0.021411479 val_loss 0.027128018
ttest: -1.845551276829156 pValue 0.06516020840032091
Epoch 3284: train_loss 0.021410218 val_loss 0.027130255
ttest: -1.7093875643802292 pValue 0.08759241243472579
Epoch 3285: train_loss 0.021408964 val_loss 0.027132427
ttest: -1.5737803806718746 pValue 0.11575588516827919
#################################################################
Target Domain Transfer
Test RMSE: 8.257633

(POI) E:\root\ChenLiyue\traffic_kesdan\Experiments\STMeta_Transfer>